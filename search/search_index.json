{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"JHU CompTox Lab","text":"<p>Welcome to the JHU CompTox Lab! Our goal is to make bio/cheminformatics concepts digestible and the application of techniques clear. We aim to cover topics in programming languages, biostatistics, machine learning, and 'omics level analysis. </p> <p> </p>"},{"location":"#meet-the-team","title":"Meet the Team","text":""},{"location":"#about-us","title":"About Us","text":""},{"location":"biostatistics/binomial-test/","title":"Binomial Test","text":""},{"location":"biostatistics/binomial-test/#estimating-proportions","title":"Estimating Proportions","text":"<p>When we estimate proportions using a sample, this proportion is also subject to sampling error. And just like the mean, we can estimate it's standard error with:</p> \\[SE_p = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] <p>Term Definitions</p> <ul> <li>\\(SE_p\\) standard error of the proportion</li> <li>\\(\\hat{p}\\) sample proportion</li> <li>\\(n\\) number of observations</li> <li>\\(X\\) number of successes out of \\(n\\) observations (down below)</li> </ul> <p>However, there is an issue with this standard error estimate: When either the \\(n\\) is small or our \\(\\hat{p}\\) is extreme (close to 0 or 1) the estimate is not reliable. To remedy this we use the Agresti-Coull interval:</p> \\[p^\\prime = \\frac{X + 2}{n + 4}\\] <p>So that the confidence interval is:</p> \\[1.96 \\pm \\sqrt{\\frac{p^\\prime(1-p^\\prime)}{n + 4}}\\] <p>So let's use this formula to estimate the proportion of males in our sample:</p> <pre><code>library(tidyverse)\nlibrary(binom)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n#take our sample\nsex &lt;- sample(meta$SEX,20)\n\n# define number of successes and number observations\nn = length(sex[!(is.na(sex))])\nmales = length(sex[sex==\"Male\"])\n\n#calculate our confidence interval\nsex.confit &lt;- binom.confint(x = males, n = n, conf.level = 0.95, methods = \"ac\")\n\nsex.confit\n</code></pre> <pre><code>         method  x  n mean     lower     upper\n1 agresti-coull 13 20 0.65 0.4315888 0.8200736\n</code></pre>"},{"location":"biostatistics/binomial-test/#binomial-distribution","title":"Binomial Distribution","text":"<p>What we have just calculated a confidence interval for a binary variable. Above we measured the interval we believe our true population proportion estimate of males to be between. Now let's switch gears and discuss the probability of \\(X\\) number of males in \\(n\\) independent observations would be:</p> \\[P[X] = \\binom{n}{X} p^X (1-p)^{N-X}\\] <p>Where:</p> \\[\\binom{n}{X} = \\frac{n!}{X!(n-X)!}\\] <p>Term Definitions</p> <ul> <li>\\(n\\) number of trials/observations</li> <li>\\(X\\) number of successes</li> <li>\\(p\\) probability of success with each trial/observation</li> </ul> <p>We can see that with different numbers of successes come different probabilities. We can plot these differences in probabilities to get an idea of how probability changes when you change the number of successes:</p> <pre><code>#20 observations/trials\n# with a probabilty of 0.65\nsex.exact.prob &lt;- data.frame(\n  X = 0:20,\n  probs = dbinom(x = 0:20, size = 20, prob = 0.65)\n)\n\nggplot(sex.exact.prob, aes(x=X,y=probs)) + \n  geom_bar(fill=\"lightpink\",stat = \"identity\")+\n  theme_bw()+\n  labs(\n    x=\"X number of Successes\",\n    y=\"Probability of X Successes\",\n    title=\"Probability of X Successes v. X number of Successes\"\n  )\n</code></pre> <p></p> <p>Note</p> <p>You'll note that we use <code>dbinom</code> (for the probability density function) - but there is also the option for <code>pbinom</code> (for the cumulative density function ). What do these options mean? - probability density function probability of exactly X successes - cumulative density function probability of less than or equal to X successes</p> <p>Here we are trying to determine the probability of exactly X successes which is why we use <code>dbinom</code></p>"},{"location":"biostatistics/binomial-test/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>So we've determined the probability of \\(X\\) successes in \\(N\\) trials with a \\(p\\) probability of success per trial. What if we wanted to test a hypothesis that this is or is not the behavior of the underlying population? First let's cover some hypothesis testing terms:</p> <ul> <li>\\(H_0\\) or null hypothesis states that any oberved differences are due to chance</li> <li>\\(H_1\\) or alternative hypohthesis that any oberved differences are not due to chance</li> <li>\\(\\alpha\\) or probability where we reject the null (a.k.a hypothesis where any oberved differences are due to chance)</li> <li>test statistic or numeric summary to help distinguish between the the null and the alternative hypothesis</li> </ul> <p>Here we will ask, is this proportion of males evidence that males are over-represented in glioblastoma patients? So if we wanted to test for this overrepresentation, we could specify the number of males in our samples, the total number of observations, and the probability under the null hypothesis(that there is a 50% probability of male patients and a 50% probability of female patients).</p> <pre><code>binom.res &lt;- binom.test(x = 13, n = 20, p = 0.5, alternative = \"two.sided\")\nbinom.res\n</code></pre> <pre><code>Exact binomial test\n\ndata:  13 and 20\nnumber of successes = 13, number of trials = 20, p-value = 0.2632\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4078115 0.8460908\nsample estimates:\nprobability of success \n                  0.65 \n</code></pre> <p>Explanation</p> <p>So here we see that our hypothesis is that:</p> <ul> <li>we have 13 successes and 20 trials</li> <li>our hypothesis is that our true probability of success is not equal to 0.5</li> <li>our p-value is above 0.05; so we do not have enough evidence to reject the null (a.k.a that our observed probability is due to chance)</li> <li>our sample probability of success is 0.65</li> </ul> <p>Reportable Confidence Interval</p> <p>Remember to use the Agresti-Coull confidence interval, and not the confidence interval reported by <code>binom.test()</code></p>"},{"location":"biostatistics/binomial-test/#references","title":"References","text":"<ol> <li>Wilson score and Agresti-Coull intervals for binomial proportions</li> <li>BIOL - 202: Analyzing a single categorical variable</li> <li>A Guide to dbinom, pbinom, qbinom, and rbinom in R</li> <li>Test Statistic</li> </ol>"},{"location":"biostatistics/biostatistics/","title":"Biostatistics","text":""},{"location":"biostatistics/biostatistics/#setupcheatsheet","title":"Setup/Cheatsheet","text":"<ul> <li>Setup</li> <li>Biostatistics Cheatsheet</li> </ul>"},{"location":"biostatistics/biostatistics/#variables-and-sampling","title":"Variables and Sampling","text":"<ul> <li>Quantitative Variables</li> <li>Qualitative Variables</li> <li>Sampling</li> <li>Confidence Intervals</li> <li>Probability Distributions</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-one-categorial-variable","title":"Analyzing One Categorial Variable","text":"<ul> <li>Binomial Test</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-two-categorical-variables","title":"Analyzing Two Categorical Variables","text":"<ul> <li>Odds</li> <li>Risk/Odds Ratio</li> <li>Fisher's Exact Test</li> <li>Chi-Square Test</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-one-numeric-variable","title":"Analyzing One Numeric Variable","text":"<ul> <li>One Sample T-Test</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-numeric-variable-with-two-groups","title":"Analyzing Numeric Variable With Two Groups","text":"<ul> <li>Paired T-Test</li> <li>Two Sample T-Test</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-two-numeric-variables","title":"Analyzing Two Numeric Variables","text":"<ul> <li>Correlation</li> </ul>"},{"location":"biostatistics/biostatistics/#analyzing-two-or-more-groups","title":"Analyzing Two Or More Groups","text":"<ul> <li>One-Way ANOVA</li> </ul>"},{"location":"biostatistics/biostatistics_cheatsheet/","title":"Statistical Tests","text":""},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-0-independent-variables","title":"1 Dependent Variable/ 0 Independent Variables","text":"statistical test description example in R one sample t-test test whether a sample mean significantly differs from a hypothesized value (assumes your vairable is normally distributed) <code>t.test(data, mu = meanToTest)</code> one sample median test test whether a sample median differs significantly from a hypothesized value (does not assume your variable is normally distributed) <code>wilcox.test(data, mu = medianToTest, alternative = \"two.sided\")</code> binomial test test whether the proportion of successes on a two-level categorical dependent variable significantly differs from a hypothesized value - <code>binom.test(numberOfActualSuccesses, numberOfTrialsYouDo, probabilityOfSuccessToTest)</code> chi-square goodness of fit test whether the observed proportions for a categorical variable differ from hypothesized proportions <code>chisq.test(vectorOfCounts, hypothesizedProportions)</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-1-independent-variables-with-2-levels","title":"1 Dependent Variable/ 1 Independent Variables with 2 Levels","text":"statistical test description example in R two independent samples t-test compare the means of a normally distributed dependent variable for two independent groups <code>t.test(data1, data2)</code> wilcoxon-mann-whitney test non-parametric analog to the independent samples t-test and can be used when the dependent variable is not normally distributed <code>wilcox.test(data1, data2, alternative = \"two.sided\")</code> chi-square test tests for a relationship between two categorical variables (makes the assumption that each cell has at least 5 when you split by table!) <code>chisq.test(table(variable1,variable2))</code> fisher\u2019s exact test tests for a relationship between two categorical variables, but can be used when cells have counts less than 5 <code>fisher.test(table(variable1,variable2))</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-1-independent-variables-with-2-or-more-levels","title":"1 Dependent Variable/ 1 Independent Variables with 2 or More Levels","text":"statistical test description example in R one-way analysis of variance(ANOVA) test for differences in the means of the dependent variable broken down by the levels of the independent variable - assumes dependent variable is normally distributed, variances for each of the groups are the same <code>aov(numericDependentVariable ~ categoricalIndependentVariable, data = dataFrameWithBothVariables)</code> analysis of co-variance(ANCOVA) test for differences in the means of the dependent variable broken down by the levels of two independent variable - assumes dependent variable is normally distributed, variances for each of the groups are the same <code>aov(numericDependentVariable ~ categoricalIndependentVariable1+categoricalIndependentVariable2, data = dataFrameWithBothVariables)</code> shapiro-wilk test tests for normality of a variable - small p-value = not normally distributed / big p-value = is normally distributed <code>shapiro.test(variableToTest)</code> levene test tests for differences in variances among groups - small p-value = there are differences among groups / big p-value = no differences among groups <code>levene.test(numericDependentVariable ~ categoricalIndependentVariable, data = dataFrameWithBothVariables)</code> kruskal wallis test test for differences between a dependent variable broken down by the levels of the independent variable - non-parametric alternative to one way anova, does not assume normality or equal variances <code>kruskal.test(numericDependentVariable ~ categoricalIndependentVariable, data = dataFrameWithBothVariables)</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-1-independent-variables-with-2-paired-levels","title":"1 Dependent Variable/ 1 Independent Variables with 2 Paired Levels","text":"statistical test description example in R paired t-test compare the means of a normally distributed dependent variable for two dependent/related groups <code>t.test(data1, data2, paired = TRUE, alternative = \"two.sided\")</code> wilcoxon signed rank sum test non-parametric version of paired t-test - does not assume normality <code>wilcox.test(data1, data2, paired = TRUE, alternative = \"two.sided\")</code> mcnemar test tests for differences in proportions between paired data - like before and after some event <code>mcnemar.test(table(pairedVariable1,pairedVariable2))</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-1-independent-variables-with-2-or-more-paired-levels","title":"1 Dependent Variable/ 1 Independent Variables with 2 or More Paired Levels","text":"statistical test description example in R one-way repeated measures analysis of variance(ANOVA) tests for differences between the means of three or more groups where the same subjects show up in each group <code>aov(numericDependentVariable~factor(categoricalVariableThatChanges)+Error(factor(subject)), data = dataFrameWithAllVariables)</code> friedman test non-parametric alternative to the one-way repeated measures ANOVA <code>friedman.test(y=numericDependentVariable, groups=categoricalVariableThatChanges, blocks=subjects)</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#1-dependent-variable-1-numeric-independent-variables","title":"1 Dependent Variable/ 1 Numeric Independent Variables","text":"statistical test description example in R correlation tests for a relationship between normally distributed variables <code>cor.test(variable1, variable2, method=\"pearson\")</code> non-parametric correlation tests for a relationship between non-normally distributed variables <code>cor.test(variable1, variable2, method=\"spearman\")</code>"},{"location":"biostatistics/biostatistics_cheatsheet/#2-or-more-dependent-variables-2-independent-variables-with-2-or-more-levels","title":"2 or More Dependent Variables/ 2 Independent Variables with 2 or More Levels","text":"statistical test description example in R one-way multivariate analysis of variance(MANOVA) assess how two or more dependent variables are affected by a categorical variable <code>manova(cbind(numericDependentVariable1, numericDependentVariable2) ~ independentCategoricalVariable, data = dataframeWithAllTheVariables)</code> <p>References</p> <ol> <li>UCLA Statistical Methods and Analysis</li> <li>Statology</li> <li>STHDA</li> </ol>"},{"location":"biostatistics/biostats/","title":"Introduction to Biostatistics","text":"<p>Biostatistics attempts to use statiscal methods to solve biological problems. This involves data so for the purpose of our biostatistics tutorials we will need to do some setup.</p>"},{"location":"biostatistics/biostats/#setup","title":"Setup","text":"<p>For the following machine learning tutorials we will be using glioblastoma data from cBioPortal. When working within R it is useful to set up an R project. R projects will set your working directory relative to the project directory. This can help ensure you are only working with files within this project space. To create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>biostatistics</code>)</li> <li><code>Create Project</code></li> </ol> <p>When analyzing data it is useful to create a folder to house your raw data, scripts and results. We can do this by clicking the <code>New Folder</code> icon to create these folders:</p> <ol> <li>Click <code>New Folder</code> &gt; Enter <code>data</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>scripts</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>results</code> &gt; Click OK</li> </ol> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>download.file(url = \"https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz\",destfile = \"./data/gbm_cptac_2021.tar.gz\" )\nuntar(tarfile = \"./data/gbm_cptac_2021.tar.gz\",exdir = \"./data/\")\n</code></pre>"},{"location":"biostatistics/chi-square/","title":"Chi-Square Test","text":""},{"location":"biostatistics/chi-square/#chi-square-test","title":"Chi-Square Test","text":"<p>When comparing categorical variables, your data won't always have 2x2 dimensions as in the Fisher's Test Topic Note.  For tables with greater dimensions we can use the Chi-square test to test hypotheses of association.  The Chi-Square or \\(\\chi^2\\) test is defined by:</p> \\[\\chi^2 = \\sum_i{\\frac{(O_i-E_i)^2}{E_i}}\\] <p>with:</p> \\[d.f. = (r-1)(c-1)\\] <p>Explanation of Terms</p> <ul> <li>\\(\\chi^2\\) chi-square statistic</li> <li>\\(O_i\\) observed value at cell i</li> <li>\\(E_i\\) expected value at cell i</li> <li>\\(d.f.\\) degrees of freedom</li> <li>\\(r\\) number of rows</li> <li>\\(c\\) number of columns</li> </ul>"},{"location":"biostatistics/chi-square/#chi-square-distribution","title":"Chi-Square Distribution","text":"<p>This \\(\\chi^2\\) value is then compared to a probability distriubtion. In our Binomial Test Topic Note, we described that  probability distributions help us determine the probability of an event given some parameter. In the Binomial Test Topic Note, that parameter was the number of successes, but here it is our degrees of freedom. We won't cover the math of this probability distribution function here, but if your curious check out the wikipedia page on chi-sqare distributions.  Let's use R to examine how probability changes with varying degrees of freedom and \\(\\chi^2\\) values:</p> <pre><code># examine the chi square cdf\nchi.sq.df &lt;- data.frame(\n  probability = 0,\n  chi_square = 0,\n  df = 0\n)\nfor(i in 2:9){\n  chi.sq.df.i &lt;- data.frame(\n    probability = dchisq(1:10,i),\n    chi_square = 1:10,\n    df = rep(as.character(i),10)\n  )\n  chi.sq.df &lt;- rbind(chi.sq.df,\n                     chi.sq.df.i)\n}\nchi.sq.df &lt;- chi.sq.df[-1,]\n\n\nggplot(chi.sq.df,            \n       aes(x = chi_square,\n           y = probability,\n           color = df)) +  \n  geom_line()+\n  theme_bw()\n</code></pre> <p></p> <p>Now, what you might have guessed from the equation above, but we are testing the following hypotheses:</p> <ul> <li>\\(H_0\\) or null hypothesis that there is no association between the two categorical values</li> <li>\\(H_a\\) or alternative hypothesis that there is an association between the two categorical values</li> </ul>"},{"location":"biostatistics/chi-square/#visualizing-data","title":"Visualizing Data","text":"<p>Here we will test the association between losing a patient to follow up and their country of origin. Let's try visualizing this first:</p> <pre><code># load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n\n# generate a table of losing a patient to follow up\n# by their country of origin\n# removing zero count countries\ntable &lt;- as.data.frame.matrix(\n  table(meta$LOST_TO_FOLLOW_UP,meta$COUNTRY_OF_ORIGIN)\n) %&gt;%\n  select(c(China,Poland,Russia, `United States`))\n\ntable\n</code></pre> <pre><code>    China Poland Russia United States\nNo     21     14     16            16\nYes     9      2      3             5\n</code></pre> <p>Now let's see this plotted!</p> <pre><code>#plot our data\ntable %&gt;% \n  t() %&gt;% \n  reshape2::melt() %&gt;%\n  ggplot(.,aes(x=Var1,y=value,fill=Var2))+\n  geom_bar(stat = \"identity\",position=\"fill\") +\n  scale_y_continuous(labels = scales::percent)+\n  theme_bw()+\n  scale_fill_manual(values=c(\"aquamarine3\",\"lightpink3\")) +\n  labs(\n    x=\"\",\n    y=\"Frequency\",\n    fill=\"Lost To Follow Up?\"\n  )\n</code></pre> <p></p>"},{"location":"biostatistics/chi-square/#running-the-chi-square-test","title":"Running the Chi-Square Test","text":"<pre><code># run the chi-square test\nchisq.test(table)\n</code></pre> <pre><code>    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 2.4197, df = 3, p-value = 0.49\n\nWarning message:\nIn chisq.test(table) : Chi-squared approximation may be incorrect\n</code></pre> <p>Explanation of Results</p> <p>Here we note that:</p> <ul> <li>Our chi-square test statistic is <code>2.4197</code></li> <li>The degrees of freedom is <code>3</code></li> <li>Our p-value is <code>0.49</code> and is greater than 0.05:<ul> <li>indicating there is not enough evidence to reject the null hypothesis (a.k.a there is no association between the two categorical values)</li> </ul> </li> <li>Our test assumptions might not be correct!</li> </ul>"},{"location":"biostatistics/chi-square/#assumptions","title":"Assumptions","text":"<p>The Chi-Square test is not free of assumptions:</p> <ul> <li>categories shouldn't have an expected frequency less than one</li> <li>20% of the categories should not have an expected frequency less than 5</li> </ul> <p>In the table above we note that some frequency values are indeed less than 5! Now how does our test change if our values are increased ten-fold?</p> <pre><code>chisq.test(table*10)\n</code></pre> <pre><code>Pearson's Chi-squared test\n\ndata:  table * 10\nX-squared = 24.197, df = 3, p-value = 2.272e-05\n</code></pre> <p>Here we see that not only did the warning disappear but our p-value is below the canonical 0.05! So be aware that the Chi-Square test is very dependent on sample size. </p>"},{"location":"biostatistics/chi-square/#references","title":"References","text":"<ol> <li>BIOL 202</li> <li>Chi-Square Distribution</li> </ol>"},{"location":"biostatistics/confidence-interval/","title":"Confidence Intervals","text":""},{"location":"biostatistics/confidence-interval/#confidence-intervals","title":"Confidence Intervals","text":"<p>Estimating the mean from a sample is going to have some fluctuation defined by the standard error. We can define a range or confidence interval  which we expect to contain the true mean. Often we report a 95% confidence interval. This interval is defined by plus or minus 1.96 times the standard error:</p> \\[ -1.96\\frac{\\sigma}{\\sqrt{N}} \\le \\mu  \\le +1.96\\frac{\\sigma}{\\sqrt{N}}\\] <p>Term Definitions</p> <ul> <li>\\(\\sigma\\) Standard deviation of the sample</li> <li>\\(N\\) Number of observations in the sample</li> <li>\\(\\mu\\) sample mean</li> </ul> <p>Let's try this with our sample:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n## defined some population of ages\nages &lt;- sample(meta$AGE,20)\n\n## data frame of results\nsummary &lt;- data.frame(\n  Sample_Mean=mean(ages,na.rm = T),\n  Standard_Error=sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)])),\n  Lower_Bound_CI = mean(ages,na.rm = T) - 1.96*(sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)]))),\n  Upper_Bound_CI = mean(ages,na.rm = T) + 1.96*(sd(ages,na.rm = T)/sqrt(length(ages[!is.na(ages)])))\n)\n\nsummary\n</code></pre> <pre><code>  Sample_Mean Standard_Error Lower_Bound_CI Upper_Bound_CI\n1       61.35       2.316162       56.81032       65.88968\n</code></pre> <p>Note</p> <p>So we are 95% confident that the true mean lies somewhere between <code>56.81032</code> and <code>65.88968</code></p>"},{"location":"biostatistics/confidence-interval/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"biostatistics/correlation/","title":"Correlation","text":""},{"location":"biostatistics/correlation/#pearson-correlation","title":"Pearson Correlation","text":"<p>So far we have compared two groups within the same numeric variable. But what about comparing two different numeric variables? We often accomplish this  by assessing the correlation between the two numeric variables. First we will discuss Pearson Correlation which can be calculated by:</p> \\[ r = \\frac{\\sum{(x - \\mu_x)(y - \\mu_y)}}{\\sqrt{\\sum{(x - \\mu_x)^2} \\sum{(y - \\mu_y)^2}}}  \\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : correlation coefficient</li> <li>\\(x\\) : variable x</li> <li>\\(y\\) : variable y</li> <li>\\(\\mu_x\\) : mean of variable of x</li> <li>\\(\\mu_y\\) : mean of variable of y</li> </ul> <p>Pearson Correlation is called parametric correlation as it depends on the distribution of your data. This type of correlation is assessing the linear dependence of variables x and y. We can test the signifcance of this association using the t statistic for pearson correlation:</p> \\[t = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n - 2}\\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : correlation coefficient</li> <li>\\(n\\) : number of observations</li> </ul>"},{"location":"biostatistics/correlation/#calculating-pearson-correlation","title":"Calculating Pearson Correlation","text":"<p>Using our glioblastoma data, let's determine the correlation between height and weight in our patients. However, before we do this,  let's plot height  versus weight:</p> <pre><code># let's plot height and weight\nggplot(meta,aes(x=HEIGHT,y=WEIGHT))+\n  geom_point()+\n  theme_bw()\n</code></pre> <p></p> <p>Here we see there is a positive relationship between height and weight. Let's test the significance of this relationship:</p> <pre><code># test the correlation between height and weight\ncor.test(\n  x = meta$HEIGHT,\n  y = meta$WEIGHT,\n  method = \"pearson\"\n)\n</code></pre> <pre><code>    Pearson's product-moment correlation\n\ndata:  meta$HEIGHT and meta$WEIGHT\nt = 9.5779, df = 97, p-value = 1.094e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5795233 0.7863597 \nsample estimates:\n     cor \n0.697175 \n</code></pre> <p>Explanation of Results</p> <ul> <li>our test statistic is <code>9.5779</code></li> <li>our d.f. is <code>97</code></li> <li>our p-value is <code>1.094e-15</code> giving us enough evidence to reject the null hypothesis that the correlation is equal to 0</li> <li>the alternative hypothesis is that the true correlation is not equal to 0 appears supported</li> <li>the 95% confidence interval around our correlation coefficient is <code>0.5795233</code> and <code>0.7863597</code></li> <li>the correlation coefficient is <code>0.697175</code> indicating a positive association</li> </ul> <p>Correlation Coefficient Interpretation</p> <ul> <li>correlation coefficient = 0 : no association</li> <li>correlation coefficient = 1 : positive assocation</li> <li>correlation coefficient. = -1 : negative associtaion</li> </ul>"},{"location":"biostatistics/correlation/#pearson-correlation-assumptions","title":"Pearson Correlation Assumptions","text":"<p>Now that we have conducted our test we must test our assumptions:</p> <ul> <li>the relationship is linear</li> <li>is each variable normally distributed?</li> </ul> <p>We saw from our plot that the relationship does appear linear, meaning that as one variable increases/decreases the other does too. To test the normality of each variable we will use the Shapiro-Wilk Test:</p> <pre><code># test the normality of height and weight\nshapiro.test(meta$HEIGHT)\nshapiro.test(meta$WEIGHT)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$HEIGHT\nW = 0.98735, p-value = 0.4689\n\n    Shapiro-Wilk normality test\n\ndata:  meta$WEIGHT\nW = 0.91388, p-value = 7.436e-06\n</code></pre> <p>Given our p-value for the Shapiro-Wilk Test on height is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. However, we note the opposite case for weight. The p-value is below 0.05 which indicates the data are not  normally distributed.</p>"},{"location":"biostatistics/correlation/#spearman-rank-correlation","title":"Spearman Rank Correlation","text":"<p>Above we saw that the normality assumption was violated in our data. What are we to do! Another option we could pursue is non-parametric correlation  which does not make the assumptions we discussed above. This correlation coefficient is referred to as Spearman Rank Correlation which can be  calculated like so:</p> \\[ r = \\frac{\\sum{(x\\prime - \\mu_{x\\prime} )(y\\prime  - \\mu_{y\\prime} )}}{\\sqrt{\\sum{(x\\prime  - \\mu_{x\\prime} )^2} \\sum{(y\\prime  - \\mu_{y\\prime} )^2}}}  \\] <p>Explanation of Terms</p> <ul> <li>\\(r\\) : spearman rank correlation coefficient</li> <li>\\(x\\) : ranked variable x</li> <li>\\(y\\) : ranked variable y</li> <li>\\(\\mu_x\\) : mean of variable of x</li> <li>\\(\\mu_y\\) : mean of variable of y</li> </ul> <p>Essentially, the only difference is that the values in the variables are ranked and then the correlation coefficient is calculated. Let's try this in R!</p> <pre><code># test the spearman correlation between height and weight\ncor.test(\n  x = meta$HEIGHT,\n  y = meta$WEIGHT,\n  method = \"spearman\",\n  exact = FALSE\n)\n</code></pre> <pre><code>    Spearman's rank correlation rho\n\ndata:  meta$HEIGHT and meta$WEIGHT\nS = 46676, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7113395 \n</code></pre> <p>Explanation of Results</p> <ul> <li>the p-value is less than <code>2.2e-16</code></li> <li>the p-value is below 0.05 giving us enough evidence to reject the null hypothesis, that the true rho is equal to 0</li> <li>the spearman correlation coefficient is <code>0.7113395</code> indicative of a positive association</li> </ul>"},{"location":"biostatistics/correlation/#references","title":"References","text":"<ol> <li>Correlation Test Between Two Variables in R</li> <li>BIOL 202 - Pearson correlation analysis</li> </ol>"},{"location":"biostatistics/distributions/","title":"Probability Distributions","text":""},{"location":"biostatistics/distributions/#the-variable-the-probability-and-the-distriubtion","title":"The Variable, The Probability, And The Distriubtion","text":"<p>Probability can be used to assess the likelihood of getting a value. Multiple values make up a variable, like a set of biomarker values, temperature values, etc.. This variable is related to the probability of getting the value by the probability distribution. Before we get to probability distributions we will talk a bit more about variables, specifically random variables!</p> <p> </p>"},{"location":"biostatistics/distributions/#random-variables","title":"Random Variables","text":"<p>Random variables are some quantity derived from a random process. Think about drawing some random value from a bag of possible values. That set of possible values can change depending on what you are measuring:</p> <ul> <li>If you are measuring a variable that exists on a continuous spectrum it is called a continuous random variable. Think rainfall, where the values are continous (1,1.1, 1.005,2, etc.)</li> <li>If you are measure a variable that is limited to integers, the variable is called a discrete random variable. Think of count data like number of shoes, where you cannot have values in between values (2,4,5, etc.)</li> <li>If you are measuring a variable that is limited to true or false values, the variable is called a boolean random variable. Think of marital status, where the values are limited to yes/no or true/false (True, False)</li> </ul> <p> </p>"},{"location":"biostatistics/distributions/#probability-distributions","title":"Probability Distributions","text":"<p>For each value in a random variable there is some probability of getting that value. A set of these probabilities is known as the probability distribution! These probablities can be calculated using different functions depending on your variable type. Broadly speaking, there are two kinds of probability fucntions:</p> <ul> <li>Probability Mass Function - What is the probabilty of getting that exact value?</li> <li>Cumulative Distribution Function - What is the probability of getting a value less than or equal to that value?</li> </ul> <p> </p>"},{"location":"biostatistics/distributions/#references","title":"References","text":"<ol> <li>A Gentle Introduction to Probability Distributions</li> <li>Connecting the CDF and the PDF</li> </ol>"},{"location":"biostatistics/fisher-test/","title":"Fisher's Exact Test","text":""},{"location":"biostatistics/fisher-test/#fishers-exact-test","title":"Fisher's Exact Test","text":"<p>In the odds ratio topic note we noted that the odds ratio could help us determine whether the odds  were greater in one group versus another. We can test the strength of association of the group and event with Fisher's Exact Test.  Fisher's Exact Test has the following hypotheses:</p> <ul> <li>\\(H_0\\) or null hypothesis: there is no association between the group and event (Odds ratio = 1)</li> <li>\\(H_a\\) or alternative hypothesis: there is an association between the group and event (Odds ratio != 1)</li> </ul> <p>We can calculate the probability given the following contingency table with:</p> group 1 group 2 Event a b No Event c d \\[p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!(a+b+c+d)!}\\] <p>Explanation of Terms</p> <ul> <li>\\(a\\) number of members in group 1 with event</li> <li>\\(b\\) number of members in group 2 with event</li> <li>\\(c\\) number of members in group 1 without event</li> <li>\\(d\\) number of members in group 2 without event</li> </ul> <p>Let's assess the relationship between pathiet sex and losing patients to follow up:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n# What is the frequency distribution of losing males/females to follow up\ntable &lt;- as.data.frame.matrix(\n  table(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n)\n\ntable\n</code></pre> <pre><code>       No Yes\nFemale 33  10\nMale   44  10\n</code></pre> <p>Before we continue, we need to make this table match the contingency table above. With the rows being the event and the columns being the group:</p> <pre><code># Reorder so that we are assessing the odds ratio of losing patients to follow up\ntable &lt;- as.data.frame.matrix(\n  table(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\n  select(c(Yes,No)) %&gt;%\n  t()\n\ntable\n</code></pre> <pre><code>    Female Male\nYes     10   10\nNo      33   44\n</code></pre> <p>Now let's conduct our hypothesis test:</p> <pre><code>#apply our test\nfisher.test(table)\n</code></pre> <pre><code>    Fisher's Exact Test for Count Data\n\ndata:  table\np-value = 0.6191\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.4395952 4.0274603\nsample estimates:\nodds ratio \n   1.32933 \n</code></pre> <p>Explanation of Results</p> <p>Here we note:</p> <ul> <li>our p-value is above 0.05 and thus not strong enough to reject the null (a.k.a the true odds ratio is equal to 1)</li> <li>the 95% confidence interval reveals that our true odds ratio is somewhere between <code>0.4395952</code> and <code>4.0274603</code></li> <li>our odds ratio that patient sex is associated with losing the patient to follow up is about <code>1.3</code></li> </ul>"},{"location":"biostatistics/fisher-test/#references","title":"References","text":"<ol> <li>BIOL 202</li> <li>Fisher's Exact Test</li> </ol>"},{"location":"biostatistics/odds-ratio-risk/","title":"Risk/Odds Ratio","text":""},{"location":"biostatistics/odds-ratio-risk/#risk","title":"Risk","text":"<p>We have defined odds as the probability of that event happening divided by the probability of that event not happening.  In our topic note on odds we asked what were the odds of losing a male patient to follow up. This is different from risk. Risk can be defined as:</p> \\[Risk = \\frac{n_i}{N}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number of times event happened</li> <li>\\(N\\) total number of events</li> </ul> <p>Here we see this is just a proportion or the number of times an event happened divided by the total number of events! Let's assess the risk of  losing a male patient to follow up:</p> <pre><code>#risk\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n# what are the odds of being a female non-smoker in our dataset?\ntable &lt;- as.data.frame.matrix(\n  table(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\n  mutate(row_totals = apply(.,1,sum)) %&gt;%\n  rbind(t(data.frame(column_totals=apply(., 2, sum))))\n\ntable\n\n\nmale &lt;- table %&gt;%\n  filter(rownames(.) == \"Male\") %&gt;%\n  mutate(\n    risk_male_lost = Yes/row_totals,\n    risk_male_not_lost = No/row_totals)\n\nmale\n</code></pre> <pre><code>     No Yes row_totals risk_male_lost risk_male_not_lost\nMale 44  10         54      0.1851852          0.8148148\n</code></pre> <p>We see that the risk of losing a male patient to follow up is about 18%. </p>"},{"location":"biostatistics/odds-ratio-risk/#relative-risk","title":"Relative Risk","text":"<p>You might hear about two risks how relate to each other - also called the relative risk. Relative risk can be calculated by:</p> \\[Relative Risk = \\frac{Risk_i}{Risk_j}\\] <p>Where :</p> \\[Risk_i = \\frac{n_i}{N_i}\\] \\[Risk_j = \\frac{n_j}{N_j}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number of times event happens in group i</li> <li>\\(N_i\\) total number of group i members</li> <li>\\(n_j\\) number of times event happens in group j</li> <li>\\(N_j\\) total number of group j members</li> </ul> <p>We will calculate the relative risk of losing a female patient to follow up versus a male patient. However, before we do so we will visualize  our data:</p> <pre><code># visualize risks:\n\nas.data.frame.matrix(\n  table(meta$SEX,meta$LOST_TO_FOLLOW_UP)) %&gt;%\n  t() %&gt;%\n  reshape2::melt() %&gt;%\n  ggplot(.,aes(x=Var2,y=value,fill=Var1))+\n  geom_bar(stat = \"identity\",position=\"fill\") +\n  scale_y_continuous(labels = scales::percent)+\n  theme_bw()+\n  scale_fill_manual(values=c(\"aquamarine3\",\"lightpink3\")) +\n  labs(\n    x=\"\",\n    y=\"Frequency\",\n    fill=\"Lost To Follow Up?\"\n  )\n</code></pre> <p></p> <p>Here we see that females are slighly more prone to being lost to follow up. How does this translate to relative risk?</p> <pre><code>risks &lt;- table %&gt;%\n  filter(rownames(.) != \"column_totals\") %&gt;%\n  mutate(\n    risk_lost = Yes/row_totals,\n    risk_not_lost = No/row_totals)\n\nrisks\n</code></pre> <pre><code>       No Yes row_totals risk_lost risk_not_lost\nFemale 33  10         43 0.2325581     0.7674419\nMale   44  10         54 0.1851852     0.8148148\n</code></pre> <p>Here we can eyeball that the risk of losing a female patient to follow up is greater than losing a  male patient. Let's see what the relative risk is:</p> <pre><code>relative_risk &lt;- risks$risk_lost[\"Female\"]/risks$risk_lost[\"Male\"]\n\nrelative_risk\n</code></pre> <pre><code>  Female \n1.255814 \n</code></pre> <p>Here we can guage from the relative risk that being a female patient increases the risk of losing the patient to follow up. </p>"},{"location":"biostatistics/odds-ratio-risk/#odds-ratio","title":"Odds Ratio","text":"<p>The odds ratio of two events is also used as a measure to compare events between groups. However, the odds ratio is the ratio of odds of an event in one group versus another and is defined by:</p> \\[Odds\\ Ratio = \\frac{Odds_i}{Odds_j}\\] <p>Where:</p> \\[Odds_i = \\frac{n_i}{n_{not\\ i}}\\] \\[Odds_j = \\frac{n_j}{n_{not\\ j}}\\] <p>Explanation of Terms</p> <ul> <li>\\(Odds_i\\) odds event happens in group i</li> <li>\\(Odds_j\\) odds event happens in group j</li> <li>\\(n_i\\) number of times event happens in group i</li> <li>\\(n_{not\\ i}\\) number of times event doesn't happen in group i</li> <li>\\(n_j\\) number of times event happens in group j</li> <li>\\(n_{not\\ j}\\) number of times event doesn't happen in group j</li> </ul> <p>So let's calculate the odds ratio of losing a female patients to follow up, versus male patients to follow up:</p> <pre><code>odds &lt;- table %&gt;%\n  filter(rownames(.) != \"column_totals\") %&gt;%\n  mutate(\n    odds = Yes/No)\nodds\n</code></pre> <pre><code>       No Yes row_totals odds_lost\nFemale 33  10         43 0.3030303\nMale   44  10         54 0.2272727\n</code></pre> <p>Here we note that the odds of losing a female patient to follow up are higher than the odds of losing a female. The odds ratio would then be:</p> <pre><code>odds_ratio &lt;- odds$odds[rownames(odds)==\"Female\"]/odds$odds[rownames(odds)==\"Male\"]\nodds_ratio\n</code></pre> <pre><code>[1] 1.333333\n</code></pre> <p>Here we note that the odds of losing a female patient versus a male patient to follow up are 1.3 to 1. We can see that the odds ratio is similar to the relative risk, but the values are nonetheless different. </p> <p>Relative Risk/Odds Ratio Interpretation</p> <ul> <li>Odds Ratio/Relative Risk = 1 the factor does not affect the event</li> <li>Odds Ratio/Relative Risk &lt; 1 the factor decreases the risk of the event (protective factor)</li> <li>Odds Ratio/Relative Risk &gt; 1 the factor increases the risk of the event (risk factor)</li> </ul> <ol> <li>Relative Risk</li> <li>Odds Ratio</li> <li>BIOL 202</li> <li>Common pitfalls in statistical analysis: Odds versus risk</li> </ol>"},{"location":"biostatistics/odds/","title":"Odds","text":"<p>We often hear about the odds of something happening, but what does this mean? Well, the odds would be:</p> \\[O = \\frac{p}{1-p}\\] <p>Explanation of Terms</p> <ul> <li>\\(O\\) the odds of something happening</li> <li>\\(p\\) the probability that thing happens</li> </ul> <p>So colloquially, we see that the odds of an event is the probability of that event happening divided by the  probability of that event not happening. We are going to assess something called lost to follow up. In the setting of a clinical  trial this term describes losing patients due to various reasons. Let's try using our data to calculate the odds of losing a male patient to  follow up:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n# what are the odds of being a female non-smoker in our dataset?\ntable = as.data.frame.matrix(\n  table(meta$SEX,meta$LOST_TO_FOLLOW_UP)\n) %&gt;%\n  mutate(row_totals = apply(.,1,sum)) %&gt;%\n  rbind(t(data.frame(column_totals=apply(., 2, sum))))\n\ntable\n</code></pre> <pre><code>              No Yes row_totals\nFemale        33  10         43\nMale          44  10         54\ncolumn_totals 77  20         97\n</code></pre> <p>Here we can have created what is called a contingency table or table that describes the frequency distribution of variables. We see that more patients are not lost to follow up. Let's calculate the odds now!</p> <pre><code>male &lt;- table %&gt;%\n  filter(rownames(.) == \"Male\") %&gt;%\n  mutate(\n    prob_male_lost = Yes/row_totals,\n    prob_male_not_lost = No/row_totals,\n    odds_male_lost = prob_male_lost/(1-prob_male_lost),\n    odds_male_not_lost = prob_male_not_lost/(1-prob_male_not_lost))\n\nmale\n</code></pre> <pre><code>     No Yes row_totals prob_male_lost prob_male_not_lost odds_male_lost odds_male_not_lost\nMale 44  10         54      0.1851852          0.8148148      0.2272727                4.4\n</code></pre> <p>So here we see that the odds of losing a male to follow up are 0.23 to 1. An alternative way of looking at this is that the odds of not losing a male to follow up are 4.4 to 1.</p>"},{"location":"biostatistics/odds/#references","title":"References","text":"<ol> <li>BIOL - 202: Analyzing a single categorical variable</li> <li>Lost To Follow Up</li> <li>Contingency Table</li> </ol>"},{"location":"biostatistics/one-t-test/","title":"One Sample T-Test","text":""},{"location":"biostatistics/one-t-test/#one-sample-t-test","title":"One-Sample T-Test","text":"<p>When we deal with numeric variables we often examine the mean of that variable. The one-sample t-test can help answer the following questions about our  numeric variable:</p> <ul> <li>Does the mean of our sample, \\(\\mu\\), equal the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu = \\mu_0\\)</li> <li>\\(H_a: \\mu \\neq \\mu_0\\)</li> </ul> </li> <li>Is the mean of our sample, \\(\\mu\\), less than the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu \\le \\mu_0\\)</li> <li>\\(H_a: \\mu &gt; \\mu_0\\)</li> </ul> </li> <li>Is the mean of our sample, \\(\\mu\\), greater than the theoretical mean of our population, \\(\\mu_0\\)?<ul> <li>\\(H_0: \\mu \\ge \\mu_0\\)</li> <li>\\(H_a: \\mu &lt; \\mu_0\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if the sample mean is equal to the population mean we are conducting a two-sided test. When we ask if the sample mean is less than  or greater than the population mean, we are conducting a one-sided test.</p>"},{"location":"biostatistics/one-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our one-sample t-test statistic can be calculated by:</p> \\[t = \\frac{\\mu - \\mu_0}{\\sigma / \\sqrt{n}} \\] \\[d.f. = n - 1\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean</li> <li>\\(\\mu_0\\) : theoretical population mean</li> <li>\\(\\sigma\\) : standard deviation of our sample</li> <li>\\(n\\) : sample size</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"biostatistics/one-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Using our glioblastoma data, we are going to ask: Does the mean age of our patients equal the theoretical mean of the U.S. population (Let's say the avearage age is 32)? Before we do so, we need to ask; what probability function are we comparing our test statistic to? For a numeric variable we often compare our test statistic to a Gaussian or normal distribution. The probability density function for the normal distribution has the following formula:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"biostatistics/one-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Just like our proportion tests, we also have a confidence interval around our sample parameter, in this case the sample mean. So for a test statistic, \\(t\\), at an \\(\\alpha\\) level of 0.05, our confidence interval would be:</p> \\[\\mu \\pm t \\frac{\\sigma}{\\sqrt{n}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma\\) : sample standard deviation</li> <li>\\(n\\) :  sample size</li> </ul>"},{"location":"biostatistics/one-t-test/#running-the-one-sample-t-test","title":"Running the One-Sample T-Test","text":"<p>Putting this all together let's test whether or not the mean of our sample is equal to the theoretical mean of our population, 32:</p> <pre><code>#one-sample t-test\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n# run the one-sample t-test to determine if the\n# mean of our sample is equal to 32\nt.test(x = meta$AGE,\n       mu = 32,\n       alternative = \"two.sided\")\n</code></pre> <pre><code>    One Sample t-test\n\ndata:  meta$AGE\nt = 20.621, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 32\n95 percent confidence interval:\n 55.39750 60.38028\nsample estimates:\nmean of x \n 57.88889 \n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>20.621</code></li> <li>our d.f. is <code>98</code></li> <li>The pvalue is below <code>2.2e-16</code></li> <li>our alternative hypothesis is that the true mean is not equal to 32</li> <li>our sample mean is <code>57.88889</code> </li> <li>the 95% confidence interval for our mean is <code>55.39750</code> to <code>60.38028</code></li> <li>So we see that we have enough evidence to reject the null hypothesis that the true mean of our sample is equal to 32</li> </ul>"},{"location":"biostatistics/one-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are independent of one another</li> <li>the numeric variable is a continuous numeric variable</li> <li>there are no signficant outliers</li> <li>the data are normally distributed</li> </ul> <p>Our data are age, which is indeed a continuous variable. The data are also independent of one another (the age of our patients should not be dependent on the age of another patient). We can identify outliers using the rosner test in the <code>EnvStats</code> package. But to use this test we need to identify how many outliers we think there are. Let's estimate this visually by plotting our data:</p> <pre><code># plot our data and see if we notice any outliers\nggplot(meta, aes(x=AGE)) +\n  geom_histogram(fill =\"lightpink\") +\n  theme_bw()\n</code></pre> <p></p> <p>Given we don't see any drastic outliers let's say we might have 5 outliers:</p> <pre><code># run the rosner test and identify whether or not\n# we do have outliers\nlibrary(EnvStats)\nros.test &lt;- rosnerTest(x = meta$AGE, k = 5)\nros.test$all.stats\n</code></pre> <pre><code>  i   Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier\n1 0 57.88889 12.49154    24      86 2.712947   3.380651   FALSE\n2 1 58.23469 12.07007    88      81 2.466042   3.377176   FALSE\n3 2 57.92784 11.74224    30      99 2.378408   3.373658   FALSE\n4 3 58.21875 11.44709    31      92 2.377788   3.370097   FALSE\n5 4 58.50526 11.15641    34      34 2.196519   3.366490   FALSE\n</code></pre> <p>Here we see that the last column in the output above indicates that our 5 possible outliers are probably not outliers! Now let's check if our data are normally distributed using the Shapiro-Wilk Test:</p> <pre><code># run the shapiro-wilk test on our data\nshapiro.test(meta$AGE)\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$AGE\nW = 0.98784, p-value = 0.5038\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed.</p>"},{"location":"biostatistics/one-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the one-sample Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the one-sample\n# t-test the one sample Wilcoxon signed rank test\nwilcox.test(meta$AGE,\n            mu = 32,\n            alternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon signed rank test with continuity correction\n\ndata:  meta$AGE\nV = 4936, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 32\n</code></pre>"},{"location":"biostatistics/one-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - One-Sample T-Test</li> <li>One-Sample T-test in R</li> <li>Normal Distribution</li> <li>One-Sample T-Test using SPSS Statistics</li> <li>Nonparametric statistics</li> <li>One-Sample Wilcoxon Signed Rank Test in R</li> </ol>"},{"location":"biostatistics/one-way-anova/","title":"One-Way ANOVA","text":""},{"location":"biostatistics/one-way-anova/#one-way-anova-hypothesis","title":"One-Way ANOVA Hypothesis","text":"<p>In the paired t-test topic note, and the two-sample t-test topic note we discussed how to compare the means between two groups. What if you'd like to compare the means of two or more groups? This problem can be solved using a one-way ANOVA. The hypothesis of the One-Way ANOVA states:</p> <ul> <li>\\(H_0\\) : there is no difference between the means of each group</li> <li>\\(H_a\\) : One or more of the group means is different from the other group means</li> </ul>"},{"location":"biostatistics/one-way-anova/#between-group-sum-of-squares","title":"Between Group Sum of Squares","text":"<p>To determine this difference we first calculate the between group sum of squares:</p> \\[ SS_{between} =  \\sum_a^z{n_a(x_a - \\overline{X})} \\] <p>Explanation of Terms</p> <ul> <li>\\(SS_between\\) : between group sum of squares</li> <li>\\(a\\) : group a</li> <li>\\(z\\) : last group</li> <li>\\(x_a\\) : mean of group a</li> <li>\\(n_a\\) : sample size of group a</li> <li>\\(\\overline{X}\\) : mean of all groups</li> </ul> <p>So here we see that for each group we will subtract the total mean from the group mean and multiply that value by sample size of that group. We then take  all those values for each group and add them together to get the sum of squares. </p>"},{"location":"biostatistics/one-way-anova/#within-group-sum-of-squares","title":"Within Group Sum of Squares","text":"<p>Now we will need to calculate the within group sum of squares:</p> \\[ SS_{within} = \\sum_a^z{(x_{ia} - \\overline{x_a})}\\] <p>Explanation of Terms</p> <ul> <li>\\(SS_{within}\\) : within group sum of squares</li> <li>\\(a\\) : group a</li> <li>\\(z\\) : last group</li> <li>\\(x_ia\\) : value i in group a</li> <li>\\(\\overline{x_a}\\) : mean of group a</li> </ul> <p>Here we note that for each group we sum the difference from the mean for each value and add them together. Then when we are done adding these differences  for each group we add those values together to get the within group sum of squares.</p>"},{"location":"biostatistics/one-way-anova/#one-way-anova-f-statistic","title":"One-Way ANOVA F-Statistic","text":"<p>Now to get our F-statistic we need to calculate the between group mean square value and the within group mean square value:</p> \\[ MS_{between} = \\frac{SS_{between}}{k - 1}\\] \\[ MS_{within} = \\frac{SS_{within}}{t - k}\\] \\[ F = \\frac{MS_{between}}{MS_{within}}\\] <p>Explanation of Terms</p> <ul> <li>\\(F\\) : F-statistic</li> <li>\\(MS_{between}\\) : between group mean square value</li> <li>\\(MS_{within}\\) : within group mean square value</li> <li>\\(k\\) : number of groups</li> <li>\\(t\\) : total number of observations between all groups</li> <li>\\(SS_{between}\\) : between group sum of squares</li> </ul>"},{"location":"biostatistics/one-way-anova/#visualizing-our-data","title":"Visualizing our data","text":"<p>As you can see this is pretty laborious. Luckily, R can do these calculations for us and we will use R to determine if there is any significant  difference in age between the patient's country of origin. Here we will filter out countries that have less than 5 observations. First let's try a  visual inspection of our data:</p> <pre><code># one-way ANOVA\nlibrary(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n# isolate countries and ages\ncountries_ages &lt;- meta %&gt;%\n  filter(COUNTRY_OF_ORIGIN %in% c(\"China\",\"Poland\",\"Russia\",\"United States\"))\n\n# plot our data\nggplot(countries_ages,aes(x=COUNTRY_OF_ORIGIN,y=AGE,fill=COUNTRY_OF_ORIGIN)) +\n  geom_boxplot()+\n  theme_bw()+\n  labs(\n    x=\"\",\n    y=\"AGE\",\n    fill=\"Country of Origin\"\n  )\n</code></pre> <p></p> <p>Here we can visually see that the mean age of patients from China is lower than the other groups.</p>"},{"location":"biostatistics/one-way-anova/#one-way-anova-in-r","title":"One-Way ANOVA in R","text":"<p>Let's now apply a One-Way ANOVA in R and output the summary of results:</p> <pre><code># calculate our one-way ANOVA\nanova_res &lt;- aov(AGE ~ COUNTRY_OF_ORIGIN, data = countries_ages)\n\n# print out a summary of results\nsummary(anova_res)\n</code></pre> <pre><code>                  Df Sum Sq Mean Sq F value Pr(&gt;F)  \nCOUNTRY_OF_ORIGIN  3   1583   527.6   3.824 0.0128 * \nResiduals         84  11591   138.0                 \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n</code></pre> <p>Explanation of Results</p> <ul> <li>Here we see that our between sum of squares is <code>1583</code> and our within sum of squares is <code>11591</code></li> <li>We also note that our between group mean square value is <code>527.6</code> and our within mean square valueis <code>138.0</code></li> <li>By dividing our between group mean square value by the within mean square value we get an F-statistic of <code>3.824</code></li> <li>We notice a p-value of <code>0.0128</code>, below 0.05, giving us enough evidence to reject the null:<ul> <li>that there is not a difference between group means</li> </ul> </li> </ul>"},{"location":"biostatistics/one-way-anova/#assumptions","title":"Assumptions","text":"<p>Like any statistical test, we make assumptions. The assumptions made by the one-way ANOVA are:</p> <ul> <li>The residuls are normally distributed</li> <li>The variances of each group are equal (Homoscedasticity)</li> </ul> <p>Note</p> <p>The residuals for a One-Way ANOVA are calculated by taking each value and subtracting the mean of the group that value belongs to</p> <p>To test the normality of the residuals we will use the Shapiro-Wilk test:</p> <pre><code># use the shapiro-wilk test to determine the normality of the residuals\nshapiro.test(x = residuals(anova_res))\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  residuals(anova_res)\nW = 0.98732, p-value = 0.5527\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed. To determine the homoscedasticity of our data we will use the Levene test:</p> <pre><code># apply the levene test to our data\nlibrary(car)\nleveneTest(AGE ~ COUNTRY_OF_ORIGIN, data = countries_ages)\n</code></pre> <pre><code>Levene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.1514 0.9285\n      84  \n</code></pre> <p>Here we note that the p-value is above 0.05, indicating we don't have enough evidence to say that there is a significant difference in the variances between groups.</p>"},{"location":"biostatistics/one-way-anova/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Kruskal-Wallis rank sum test:</p> <pre><code># run the non-parametric alternative to the one-way ANOVA\n# the Kruskal-Wallis rank sum test\nkruskal.test(AGE ~ COUNTRY_OF_ORIGIN, \n             data = countries_ages)\n</code></pre> <pre><code>    Kruskal-Wallis rank sum test\n\ndata:  AGE by COUNTRY_OF_ORIGIN\nKruskal-Wallis chi-squared = 9.1676, df = 3, p-value = 0.02714\n</code></pre>"},{"location":"biostatistics/one-way-anova/#references","title":"References","text":"<ol> <li>One-Way ANOVA Test in R</li> <li>How to Perform a One-Way ANOVA by Hand</li> <li>Nonparametric statistics</li> </ol>"},{"location":"biostatistics/paired-t-test/","title":"Paired T-Test","text":""},{"location":"biostatistics/paired-t-test/#paired-t-test","title":"Paired T-Test","text":"<p>We have seen how to compare our sample mean to a theory of the underlying population. Here we will ask how do we compare the means of paired observations? We can use the paired t-test to answer any one of the following questions:</p> <ul> <li>Does the mean difference, \\(m\\), equal to 0?<ul> <li>\\(H_0: m = 0\\)</li> <li>\\(H_a: m \\neq 0\\)</li> </ul> </li> <li>Is the mean difference, \\(m\\), less than 0?<ul> <li>\\(H_0: m \\le 0\\)</li> <li>\\(H_a: m &gt; 0\\)</li> </ul> </li> <li>Is the mean difference, \\(m\\), greater than 0?<ul> <li>\\(H_0: m \\ge 0\\)</li> <li>\\(H_a: m &lt; 0\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if the mean difference is equal to 0 we are conducting a two-sided test. When we ask if the sample mean is less than  or greater than 0, we are conducting a one-sided test.</p>"},{"location":"biostatistics/paired-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our paired t-test statistic can be calculated by:</p> \\[t = \\frac{m}{\\sigma / \\sqrt{n}} \\] \\[d.f. = n - 1\\] <p>Explanation of Terms</p> <ul> <li>\\(m\\) : mean difference</li> <li>\\(\\sigma\\) : standard deviation of our sample</li> <li>\\(n\\) : sample size</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"biostatistics/paired-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Just like in the one-sample t-test topic note we will be comparing our test statistic to the normal distribution with a  probability density function of:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"biostatistics/paired-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Similarily to our one-sample t-test topic note our confidence interval will be defined as:</p> \\[\\mu \\pm t \\frac{\\sigma}{\\sqrt{n}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu\\) : sample mean difference</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma\\) : standard deviation of the differences</li> <li>\\(n\\) :  sample size</li> </ul>"},{"location":"biostatistics/paired-t-test/#running-the-paired-t-test","title":"Running the Paired T-Test","text":"<p>Seeing as our glioblastoma data does not have a paired observation (essentially a before-after comparison), we are going to need to generate some  dummy data to apply our paired-test to:</p> <pre><code># paired t-test\nlibrary(tidyverse)\n# create our dummy data\ndf &lt;- data.frame(\n  cholesterol = c(sample(70:120,20),sample(110:220,20)),\n  group = c(rep(\"before\",20),rep(\"after\",20))\n)\n\n# what is mean of each group\ndf %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n=n(),\n    standard_deviation=sd(cholesterol),\n    mean=mean(cholesterol)\n  )\n</code></pre> <pre><code># A tibble: 2 \u00d7 4\n  group      n standard_deviation  mean\n  &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 after     20               32.4 160. \n2 before    20               14.2  94.8\n</code></pre> <p>Here we note that the before group has much lower cholesterol levels than the after group. Let's now apply the paired t-test to determine the significance of this difference. We will do this by asking is the difference between groups equal 0?</p> <pre><code># run the paired t-test to determine if the \n# difference between groups is equal to 0\nt.test(\n  cholesterol ~ group, \n  data = df, \n  paired = TRUE)\n</code></pre> <pre><code>    Paired t-test\n\ndata:  cholesterol by group\nt = 8.6603, df = 19, p-value = 5.061e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 49.59408 81.20592\nsample estimates:\nmean of the differences \n                   65.4\n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>8.6603</code></li> <li>The pvalue is <code>5.061e-08</code></li> <li>our alternative hypothesis is that the true difference in means is not equal to 0</li> <li>our mean of the differences is <code>65.4</code></li> <li>the 95% confidence interval for our mean of differences is <code>49.59408</code> to <code>81.20592</code></li> <li>So we see that we have enough evidence to reject the null hypothesis that the true difference in means is equal to 0</li> </ul>"},{"location":"biostatistics/paired-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are paired</li> <li>the differences of the pairs are normally distributed</li> </ul> <p>The values in our dummy data are intended to be paired. Meaning that the first observation in the before group is the same patient as the first observation in the after group. Now to check if the differences of the pairs are normally distributed we can use the Shapiro-Wilk test like we did in the  one-sample t-test topic note:</p> <pre><code># differences betweeen the paired observations\ndifference &lt;- df$cholesterol[df$group == \"before\"] - df$cholesterol[df$group == \"after\"]\n\n# Shapiro-Wilk test to determine are they normally distributed\nshapiro.test(difference) \n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  difference\nW = 0.94613, p-value = 0.3121\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed.</p>"},{"location":"biostatistics/paired-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the paired\n# t-test the Wilcoxon signed rank test\nwilcox.test(cholesterol ~ group, \n            data = df, \n            paired = TRUE,\n            alternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon signed rank exact test\n\ndata:  cholesterol by group\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n</code></pre>"},{"location":"biostatistics/paired-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - Paired T-Test</li> <li>Paired Samples T-test in R</li> <li>Normal Distribution</li> <li>Paired Samples Wilcoxon Test in R</li> <li>Nonparametric statistics</li> </ol>"},{"location":"biostatistics/qual/","title":"Qualitative Variables","text":"<p>Qualitative variables can be thought of as categories: so variables like eye color, gender, and race.  When assessing qualitative variables it is useful to consider proportions:</p> \\[\\frac{n_i}{N}\\] <p>Explanation of Terms</p> <ul> <li>\\(n_i\\) number in category of interest</li> <li>\\(N\\) total number of observations</li> </ul> <p>So let's calculate some proportions in R!</p> <pre><code>library(tidyverse)\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\ncountry_sum &lt;- meta %&gt;%\n  count(COUNTRY_OF_ORIGIN, sort = TRUE) %&gt;% \n  mutate(proportion = n / sum(n)) %&gt;%\n  mutate(COUNTRY_OF_ORIGIN = replace_na(COUNTRY_OF_ORIGIN,\"NA\"))\n\ncountry_sum\n</code></pre> <pre><code>  COUNTRY_OF_ORIGIN  n proportion\n1             China 30 0.30303030\n2     United States 21 0.21212121\n3            Russia 19 0.19191919\n4            Poland 18 0.18181818\n5              &lt;NA&gt;  6 0.06060606\n6          Bulgaria  2 0.02020202\n7           Croatia  1 0.01010101\n8            Mexico  1 0.01010101\n9       Phillipines  1 0.01010101\n</code></pre> <p>Note</p> <p>You'll see that we do have an <code>NA</code> value here and that it's proportion in our variable is counted too! Since the <code>NA</code> value in R has special properties we ensure it is a character and not an NA value using the <code>replace_na()</code> function. </p> <p>Qualitative variables can be visualized using a bar plot:</p> <pre><code>ggplot(country_sum, aes(x=proportion,y=reorder(COUNTRY_OF_ORIGIN,+proportion))) + \n  geom_bar(fill=\"lightpink\",stat = \"identity\")+\n  theme_bw()+\n  labs(\n    x=\"Proportion\",\n    y=\"Country of Origin\",\n    title=\"Country of Origin Barplot\"\n  )\n</code></pre> <p></p> <p>Tip</p> <p>Here we ensure that we reorder our countries with the <code>reorder()</code> function as <code>ggplot2</code> will not order our data for us.</p>"},{"location":"biostatistics/qual/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"biostatistics/quant/","title":"Quantitative Variables","text":"<p>Quantitaive variables are numerical data: so variables such as height, weight, age. We can describe these variables with the following terms:</p> <p>Explanation of Terms</p> <ul> <li>Minimum : smallest value in your variable</li> <li>Median : middle value in your variable</li> <li>Mean : average value of your variable</li> <li>Max : largest value in your variable</li> <li>Count : how many values are in your variable</li> <li>Standard deviation : measure of the spread of your variable</li> </ul> <p>Let's see how to do this in our code:</p> <pre><code>library(tidyverse)\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                     header = T,\n                   sep=\"\\t\")\n\nheight.sum &lt;- meta %&gt;%\n  summarise( \n    minimum = min(HEIGHT, na.rm = T),\n    median = median(HEIGHT, na.rm = T),\n    mean = mean(HEIGHT, na.rm = T),\n    max = max(HEIGHT, na.rm = T),\n    count = length(HEIGHT[!is.na(HEIGHT)]),\n    SD = sd(HEIGHT, na.rm = T))\n\nheight.sum\n</code></pre> <pre><code>  minimum median     mean max count       SD\n1     150    170 169.6768 196    99 9.875581\n</code></pre> <p>Note</p> <p>You'll note here that we explicitly remove <code>NA</code> values to calculate these descriptive statistics.</p> <p>Now that we know how to calculate our descriptive statistics, let's try and visualize our numeric data:</p> <pre><code>ggplot(meta, aes(x=HEIGHT)) + \n  geom_histogram(fill=\"lightpink\")+\n  theme_bw()+\n  labs(\n    x=\"Height\",\n    y=\"Frequency\",\n    title=\" Histogram of Height\"\n  )\n</code></pre> <p></p>"},{"location":"biostatistics/quant/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"biostatistics/sampling/","title":"Sampling","text":"<p>When we try to assess an underlying population we often take samples of that population. Let's try and take a sample using the <code>sample()</code> function in R:</p> <pre><code>library(tidyverse)\n# load meta data\nmeta &lt;- read.table(\"./data/gbm_cptac_2021/data_clinical_patient.txt\",\n                   header = T,\n                   sep=\"\\t\")\n\n## defined some population of ages\nages &lt;- sample(meta$AGE,20)\n</code></pre> <p>If we wanted to take the same random sample we could use the <code>set.seed()</code> function:</p> <pre><code>## grab the same sample\nset.seed(123)\nages1 &lt;- sample(meta$AGE,20)\nages2 &lt;- sample(meta$AGE,20)\n</code></pre>"},{"location":"biostatistics/sampling/#sampling-error","title":"Sampling Error","text":"<p>Not every sample is going to be a true approximation of the underline population. This difference is known as the sampling error. What's assess our sample and see how it stacks up against our population:</p> <pre><code>data.frame(\n  Sample_Mean=mean(ages,na.rm = T),\n  Population_Mean=mean(meta$AGE,na.rm = T)\n)\n</code></pre> <pre><code>  Sample_Mean Population_Mean\n1        57.4        57.88889\n</code></pre> <p>Here we note that while similar to our true meta data mean, it is not exact. When we don't know the actual population mean we can get a whole range (or distribution) of means. The standard error of the mean is the measure of that sampling distribution:</p> \\[\\frac{\\sigma}{\\sqrt{N}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) Standard deviation of the sample</li> <li>\\(N\\) Number of observations in the sample</li> </ul> <p>Math Tip</p> <p>We can see that increasing the size of the sample, decreases the standard error of the mean.</p>"},{"location":"biostatistics/sampling/#references","title":"References","text":"<ul> <li>BIOL202 Tutorials</li> </ul>"},{"location":"biostatistics/stats/","title":"Stats","text":""},{"location":"biostatistics/stats/#variables-and-sampling","title":"Variables and Sampling","text":"<ul> <li>Quantitative Variables</li> <li>Qualitative Variables</li> <li>Sampling</li> <li>Confidence Intervals</li> <li>Probability Distributions</li> </ul>"},{"location":"biostatistics/stats/#analyzing-one-categorial-variable","title":"Analyzing One Categorial Variable","text":"<ul> <li>Binomial Test</li> </ul>"},{"location":"biostatistics/stats/#analyzing-two-categorical-variables","title":"Analyzing Two Categorical Variables","text":"<ul> <li>Odds</li> <li>Risk/Odds Ratio</li> <li>Fisher's Exact Test</li> <li>Chi-Square Test</li> </ul>"},{"location":"biostatistics/stats/#analyzing-one-numeric-variable","title":"Analyzing One Numeric Variable","text":"<ul> <li>One Sample T-Test</li> </ul>"},{"location":"biostatistics/stats/#analyzing-numeric-variable-with-two-groups","title":"Analyzing Numeric Variable With Two Groups","text":"<ul> <li>Paired T-Test</li> <li>Two Sample T-Test</li> </ul>"},{"location":"biostatistics/stats/#analyzing-two-numeric-variables","title":"Analyzing Two Numeric Variables","text":"<ul> <li>Correlation</li> </ul>"},{"location":"biostatistics/stats/#analyzing-two-or-more-groups","title":"Analyzing Two Or More Groups","text":"<ul> <li>One-Way ANOVA</li> </ul>"},{"location":"biostatistics/two-t-test/","title":"Two Sample T-Test","text":""},{"location":"biostatistics/two-t-test/#two-sample-t-test","title":"Two-Sample T-Test","text":"<p>In the paired t-test topic note we examined the difference in means between paired data. To compare the difference in means between two unpaired groups we will use the two-sample t-test. The two-sample t-test can be used to ask the following:</p> <ul> <li>Does the mean of group 1, \\(\\mu_1\\), equal the mean of our group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 = \\mu_2\\)</li> <li>\\(H_a: \\mu_1 \\neq \\mu_2\\)</li> </ul> </li> <li>Is the mean of group 1, \\(\\mu_1\\), less than the mean of group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 \\le \\mu_2\\)</li> <li>\\(H_a: \\mu_1 &gt; \\mu_2\\)</li> </ul> </li> <li>Is the mean of group 1, \\(\\mu_1\\), greater than the mean of group 2, \\(\\mu_2\\)?<ul> <li>\\(H_0: \\mu_1 \\ge \\mu_2\\)</li> <li>\\(H_a: \\mu_1 &lt; \\mu_2\\)</li> </ul> </li> </ul> <p>Tip</p> <p>When we ask if mean of group 1 is equal to the mean of group 2 we are conducting a two-sided test. When we ask if the mean of group 1 is less than or greater than the mean of group 2, we are conducting a one-sided test.</p>"},{"location":"biostatistics/two-t-test/#test-statistic","title":"Test Statistic","text":"<p>Our one-sample t-test statistic can be calculated by:</p> \\[t = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\frac{\\sigma_1}{n_1} + \\frac{\\sigma_2}{n_2}}} \\] \\[d.f. = n_1 + n_2 - 2\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu_1\\) : mean of group 1</li> <li>\\(\\mu_2\\) : mean of group 2</li> <li>\\(\\sigma_1\\) : standard deviation of group 1</li> <li>\\(\\sigma_2\\) : standard deviation of group 2</li> <li>\\(n_1\\) : size of group 1</li> <li>\\(n_2\\) : size of group 2</li> <li>\\(d.f.\\) : degrees of freedom</li> </ul>"},{"location":"biostatistics/two-t-test/#normal-distribution","title":"Normal Distribution","text":"<p>Just like in the one-sample t-test topic note we will be comparing our test statistic to the normal distribution with a probability density function of:</p> \\[f(x) = \\frac{1}{(\\sigma\\sqrt{2 \\pi})} e^{-(\\frac{(x - \\mu)^2}{2 \\sigma^2})}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\sigma\\) : standard deviation</li> <li>\\(\\mu\\) : mean</li> </ul>"},{"location":"biostatistics/two-t-test/#confidence-interval","title":"Confidence Interval","text":"<p>Just like our proportion tests, we also have a confidence interval around our sample parameter, in this case the sample mean. So for a test statistic, \\(t\\), at an \\(\\alpha\\) level of 0.05, our confidence interval would be:</p> \\[(\\mu_1 - \\mu_2) \\pm t * \\sqrt{\\frac{\\sigma_1}{n_1} + \\frac{\\sigma_2}{n_2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(\\mu_1\\) : mean of group 1</li> <li>\\(\\mu_2\\) : mean of group 2</li> <li>\\(t\\) : test statistic for an \\(\\alpha\\) of 0.05</li> <li>\\(\\sigma_1\\) : standard deviation of group 1</li> <li>\\(\\sigma_2\\) : standard deviation of group 2</li> <li>\\(n_1\\) : size of group 1</li> <li>\\(n_2\\) : size of group 2</li> </ul>"},{"location":"biostatistics/two-t-test/#running-the-two-sample-t-test","title":"Running the Two-Sample T-Test","text":"<p>Putting this all together let's ask: Is mean age of males equal to the mean age of females in our glioblastoma data? We will start by manually calculating the mean age and standard deviation of males and females:</p> <pre><code># what is mean of each group\nmeta %&gt;%\n  group_by(SEX) %&gt;%\n  summarise(\n    n=n(),\n    standard_deviation=sd(AGE),\n    mean=mean(AGE)\n  )\n</code></pre> <pre><code># A tibble: 2 \u00d7 4\n  SEX        n standard_deviation  mean\n  &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 Female    44               13.7  57.9\n2 Male      55               11.6  57.9\n</code></pre> <p>Remarkably we notice that the mean age of males and the mean age of females are the same! We will now use the two-sample t-test to ask if the mean age of males equals the mean age of females:</p> <pre><code># run the two-sample t-test to determine if the\n# mean age of males is equal to the mean\n# age of females\nt.test(\n  AGE ~ SEX, \n  data = meta, \n  method = \"two.sided\",\n  var.equal = FALSE)\n</code></pre> <pre><code>    Welch Two Sample t-test\n\ndata:  AGE by SEX\nt = 0.014062, df = 84.559, p-value = 0.9888\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.105621  5.178348 \nsample estimates:\nmean in group Female   mean in group Male \n            57.90909             57.87273 \n</code></pre> <p>Explanation</p> <ul> <li>our test statistic is <code>0.014062</code></li> <li>The pvalue is below <code>0.9888</code></li> <li>our alternative hypothesis is that the true difference in means is not equal to 0</li> <li>the mean female is is <code>57.90909</code> and the mean male age is <code>57.87273</code></li> <li>the 95% confidence interval for our difference in means is <code>-5.105621</code> to <code>5.178348</code></li> <li>So we see that we do not have enough evidence to reject the null hypothesis that the true difference in means is equal to 0</li> </ul>"},{"location":"biostatistics/two-t-test/#assumptions","title":"Assumptions","text":"<p>So now that we have conducted our test, we should assess the test's assumptions:</p> <ul> <li>the values are independent of one another</li> <li>the variances in each group are equal</li> <li>the data in each group normally distributed</li> </ul> <p>Here we note that our age values should be independent of one another (the age of one patient should not affect the age of another). Now for the assumption that variances of each group are equal - we avoided this by setting the <code>var.equal</code> argument equal to false. If we were to set it to true, we could check our variances with the F-Test:</p> <pre><code># use the f test to determine if the variance in \n# group 1 is the same as the variance in group 2\nvar.test(\n  AGE ~ SEX, \n  data = meta\n)\n</code></pre> <pre><code>    F test to compare two variances\n\ndata:  AGE by SEX\nF = 1.3849, num df = 43, denom df = 54, p-value = 0.2559\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.7879316 2.4804317\nsample estimates:\nratio of variances \n          1.384904 \n</code></pre> <p>Here we will just note that the p-value is above 0.05 and thus there isn't a significant difference in the variance of group 1 and the variance of group 2. Now to check if the data in each group are normally distributed we will use the Shapiro-Wilk test:</p> <pre><code># check the normality of male ages and female ages\nshapiro.test(meta$AGE[meta$SEX == \"Male\"])\nshapiro.test(meta$AGE[meta$SEX == \"Female\"])\n</code></pre> <pre><code>    Shapiro-Wilk normality test\n\ndata:  meta$AGE[meta$SEX == \"Male\"]\nW = 0.98809, p-value = 0.8604\n\n    Shapiro-Wilk normality test\n\ndata:  meta$AGE[meta$SEX == \"Female\"]\nW = 0.95578, p-value = 0.09048\n</code></pre> <p>Given our p-value is above 0.05 we do not have enough evidence to reject the null hypothesis of the Shapiro-Wilk Test; that the data are normally distributed. In other words, if the p-value is above 0.05 your data are normally distributed. However, it should be noted that the Shapiro-Wilk test p-value for female ages is rather close to 0.05 so there does seem to be some skey in female ages. </p>"},{"location":"biostatistics/two-t-test/#non-parametric-alternative","title":"Non-Parametric Alternative","text":"<p>A non-parametric test is often used when either the assumptions about the distribution are not met. Additionally, these tests do not depend on the parameter they are assessing. Here, if the assumptions above are not met we can use the non-parametric equivalent, the Wilcoxon signed rank test:</p> <pre><code># run the non-parametric alternative to the unpaired\n# t-test the Wilcoxon signed rank test\nwilcox.test(AGE ~ SEX, \n            data = meta, \n            alternative = \"two.sided\")\n</code></pre> <pre><code>    Wilcoxon rank sum test with continuity correction\n\ndata:  AGE by SEX\nW = 1278.5, p-value = 0.6318\nalternative hypothesis: true location shift is not equal to 0\n</code></pre>"},{"location":"biostatistics/two-t-test/#references","title":"References","text":"<ol> <li>BIOL 202 - Two-Sample T-Test</li> <li>Unpaired Two-Samples T-test in R</li> <li>Normal Distribution</li> <li>2 sample test of mean</li> <li>Unpaired Two-Samples Wilcoxon Test in R</li> <li>Nonparametric statistics</li> </ol>"},{"location":"machine_learning/intro-machine/","title":"Intro machine","text":""},{"location":"machine_learning/intro-machine/#introduction-to-machine-learning","title":"Introduction To Machine Learning","text":"<p>Machine learning, broadly speaking are algorithms and statistical models to analyze and draw inferences from patterns in data.  Here we find that we can use machine learning for a few broad tasks:</p> <ul> <li>Dimension Reduction to reduce the number of variables we need to consider</li> <li>Unsupervised Learning identify patterns in data that are unlabelled</li> <li>Classification predict which class label an observation might have</li> <li>Regression assess the relationship between variables </li> </ul> <p>The following cheatsheet is also useful to understand when to use what method:</p> <p></p>"},{"location":"machine_learning/intro-machine/#references","title":"References","text":"<ul> <li> <p>Oxford Languages</p> </li> <li> <p>SAS Blogs</p> </li> </ul>"},{"location":"machine_learning/introduction/","title":"Introduction","text":""},{"location":"machine_learning/introduction/#setup-for-machine-learning-tutorials","title":"Setup For Machine Learning Tutorials","text":"<ul> <li>Setup</li> </ul>"},{"location":"machine_learning/introduction/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Dimension Reduction</li> <li>Clustering</li> <li>K-means Clustering</li> <li>Hierarchical Clustering</li> </ul>"},{"location":"machine_learning/introduction/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Linear Models</li> <li>Logistic Regression</li> <li>Survival Analysis Part 1</li> <li>Survival Analysis Part 2</li> <li>Multivariate Regression</li> <li>Model Performance</li> </ul>"},{"location":"machine_learning/machine_learning/","title":"Machine Learning","text":""},{"location":"machine_learning/machine_learning/#setup-for-machine-learning-tutorials","title":"Setup For Machine Learning Tutorials","text":"<ul> <li>Setup</li> </ul>"},{"location":"machine_learning/machine_learning/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>Dimension Reduction</li> <li>Clustering</li> <li>K-means Clustering</li> <li>Hierarchical Clustering</li> </ul>"},{"location":"machine_learning/machine_learning/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Linear Models</li> <li>Logistic Regression</li> <li>Survival Analysis Part 1</li> <li>Survival Analysis Part 2</li> <li>Multivariate Regression</li> <li>Model Performance</li> </ul>"},{"location":"machine_learning/machine_learning/#deep-learning","title":"Deep Learning","text":"<ul> <li>Deep Learning Setup</li> <li>Basics</li> <li>Linear Models</li> <li>Classifiers/Multilayer Perceptrons</li> </ul>"},{"location":"machine_learning/setup/","title":"Tutorial Setup","text":""},{"location":"machine_learning/setup/#setup","title":"Setup","text":"<p>For the following machine learning tutorials we will be using glioblastoma data from cBioPortal. When working within R it is useful to set up an R project. R projects will set your working directory relative to the project directory. This can help ensure you are only working with files within this project space. To create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>machine_learning</code>)</li> <li><code>Create Project</code></li> </ol> <p>When analyzing data it is useful to create a folder to house your raw data, scripts and results. We can do this by clicking the <code>New Folder</code> icon to create these folders:</p> <ol> <li>Click <code>New Folder</code> &gt; Enter <code>data</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>scripts</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>results</code> &gt; Click OK</li> </ol> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>download.file(url = \"https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz\",destfile = \"./data/gbm_cptac_2021.tar.gz\" )\nuntar(tarfile = \"./data/gbm_cptac_2021.tar.gz\",exdir = \"./data/\")\n</code></pre>"},{"location":"machine_learning/setup/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter a timeout error you can manually set the timeout to longer than the standard 60 seconds:</p> <pre><code>options(timeout = 9000000000000000000)\n</code></pre>"},{"location":"machine_learning/deep_learning/00_setup/","title":"Setup","text":""},{"location":"machine_learning/deep_learning/00_setup/#software","title":"Software","text":"<p>First things first you'll need an integrated development environment or IDE to start coding. We recomend VS code for coding in python and really anything other than R. Use the following link to install VS code:</p> <p>VS Code Installation</p> <p>Additionally, you'll need Anaconda for managing packages, use the following link to install Anaconda:</p> <p>Anaconda Installation</p>"},{"location":"machine_learning/deep_learning/00_setup/#extensions","title":"Extensions","text":"<p>Once both are installed you'll need to install some extensions in VS code. We will be working with Jupyter notebooks so we need to install the Jupyter extension:</p> <ol> <li>Click <code>Extensions</code></li> <li>Type <code>Jupyter</code> in the search bar and download the Jupyter extension</li> </ol>"},{"location":"machine_learning/deep_learning/00_setup/#project-setup","title":"Project Setup","text":"<p>Now let's set up a space for us to work in. We will start by creating a virtual environment:</p> <ol> <li>Click <code>Open Folder</code></li> <li>Create a folder and name it <code>deep_learning</code></li> <li>Go inside the deep learning folder <code>cd deep_learning</code></li> <li>Create 3 folders inside <code>deep_learning</code>:</li> <li><code>mkdir data</code></li> <li><code>mkdir scripts</code></li> <li><code>mkdir results</code></li> </ol> <p>To download the data we will use:</p> <ol> <li>Go into the <code>data</code> folder: <code>cd data</code></li> <li><code>curl --output gbm_cptac_2021.tar.gz https://cbioportal-datahub.s3.amazonaws.com/gbm_cptac_2021.tar.gz</code></li> <li>Extract the contents: <code>tar -zxvf</code></li> </ol> <p>Now to set up the virtual environment:</p> <ol> <li>Go to <code>View</code> &gt; <code>Terminal</code></li> <li>In terminal enter: <code>conda create -n dl python=3.12 pytorch torchvision plotly</code></li> <li>Then enter <code>y</code></li> <li>Then at the top of VS code select \"Select Kernel\"</li> <li>Select \"dl\"</li> </ol> <p>Now Let's Start Coding!</p>"},{"location":"machine_learning/deep_learning/01_basics/","title":"Basics","text":"<p>Prerequisites</p> <ul> <li>Deep Learning Setup : Setup workspace and download python libraries</li> </ul> <p>Learning Objectives</p> <ol> <li>What is a Tensor?</li> <li>Basic Operations</li> <li>Matrix Multiplication</li> <li>Tensor Manipulation</li> <li>Accessing Elements in a Tensor</li> <li>Tensor Statistics</li> <li>Working with GPUs</li> <li>Random Values and Reproducibility</li> <li>Tensor and NumPy Integration</li> </ol>"},{"location":"machine_learning/deep_learning/01_basics/#what-is-a-tensor","title":"What is a Tensor?","text":"<p>Machine learning is all about manipulating numbers and to do that we have different ways of storing those numbers, typically in structures called tensors. Tensors can be a single number (scalar), a list or vector of numbers (1D tensor), a matrix (2D tensor), a list of matrices (3D tensor), so on and so forth:</p> <p>What is a Tensor?</p> <p> </p> <p>Now let's see how we make a tensor in PyTorch!</p> <pre><code>scalar = torch.tensor(7)  # Scalar\nvector = torch.tensor([1, 2, 3])  # Vector\nmatrix = torch.tensor([[1, 2], [3, 4]])  # Matrix\ntensor = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])  # 3D Tensor\n</code></pre> <p>Machine learning papers always include a lot of math jargon, but don't be afraid! Let's go through a few of the symbols for tensors! </p> <ul> <li>Scalar: A single number \\(s \\in \\mathbb{R}\\).</li> <li>Vector: A 1D array or vector is shown as: \\(\\mathbf{v} \\in \\mathbb{R}^n\\).</li> <li>Matrix: A 2D array or matrix with m rows and n columns is shown as: \\(\\mathbf{M} \\in \\mathbb{R}^{m \\times n}\\).</li> <li>3D Tensor: a list of o matrices with m rows and n columns is shown as: \\(\\mathbf{M} \\in \\mathbb{R}^{o \\times m \\times n}\\).</li> </ul> <p>Now in python these tensors are shown using different numbers of brackets:</p> <p>Tips and Tricks</p> <ul> <li>Dimensions as brackets:</li> <li><code>[]</code> \u2192 1D (vector)</li> <li><code>[[]]</code> \u2192 2D (matrix)</li> <li><code>[[[]]]</code> \u2192 3D</li> </ul>"},{"location":"machine_learning/deep_learning/01_basics/#basic-operations","title":"Basic Operations","text":"<p>Again, tensors are just numbers and we can perform basic operations on them, like addition, subtraction, multiplication and division. Let's go through how we would do that!</p> <pre><code>tensor = torch.tensor([1, 2, 3])\nprint(tensor + 10)  # Addition\nprint(tensor * 10)  # Multiplication\n</code></pre> <p>output</p> <pre><code>tensor([11, 12, 13])\ntensor([10, 20, 30])\n</code></pre> <p>Now in math speak we represent the operations above (and others like division and subtraction) like so: </p> <ul> <li>Element-wise addition: \\(\\mathbf{A} + c\\), where \\(c\\) is added to each element in \\(\\mathbf{A}\\).</li> <li>Element-wise multiplication: \\(\\mathbf{A} \\times c\\), where \\(c\\) multiplies each element of \\(\\mathbf{A}\\).</li> </ul> <p>Tips and Tricks</p> <ul> <li>Reassign to modify: Operations don\u2019t change the tensor unless you reassign the result back to some variable.</li> </ul>"},{"location":"machine_learning/deep_learning/01_basics/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>A lot of machine learning relies heavily on matrix operations and often times multiplying matrices together. When we multiple matrices we need to match the inner dimensions and then we end up with a matrix with the outer dimensions:</p> <p>This Works!</p> <p>\\(A(4 \\times 2) \\cdot B(2 \\times 3) \\rightarrow C(4 \\times 3)\\) This works because the inner dimensions are 2 and 2</p> <p>This Doesn't Work!</p> <p>\\(A(4 \\times 3) \\cdot B(2 \\times 3) \\rightarrow X\\) This does not work because the inner dimensions are 3 and 2. They are different!</p> <p>Let's do this in PyTorch!</p> <pre><code># Define A as a 2x3 tensor\nA = torch.tensor([[1, 2, 3], [4, 5, 6]])\n# Define B as a 3x2 tensor\nB = torch.tensor([[7, 8], [9, 10], [11, 12]])\n# Perform matrix multiplication\nresult = torch.matmul(A, B)\nprint(result)\n</code></pre> <p>output</p> <pre><code>tensor([[ 58,  64],\n        [139, 154]])\n</code></pre> <p>Tips and Tricks</p> <ul> <li>Instead of writing out <code>torch.matmul(A,B)</code> you can use the shorthand <code>A @ B</code> for the same result!</li> </ul> <p>Now what would this look like in math speak?</p> <ul> <li>Matrix multiplication:</li> <li>\\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\)</li> <li>\\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\)</li> <li>the matrix product is:</li> <li>\\(\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\)</li> <li>and each value is calculated by</li> <li>\\(c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}\\)</li> </ul> <p>Now that's a lot, let's just look at a visualization of how you multiply matrices:</p>"},{"location":"machine_learning/deep_learning/01_basics/#tensor-manipulation","title":"Tensor Manipulation","text":"<p>Ok so when working with tensors and especially performing matrix operations, we will often be trying to match dimensions. We can do that using a number of different functions in PyTorch. Let's start out by making a vector:</p> <pre><code>x = torch.tensor([1,2,3,4,5,6])\nx\n</code></pre> <p>output</p> <pre><code>tensor([1, 2, 3, 4, 5, 6])\n</code></pre> <p>Great, now let's add an extra dimension so that it no longer a vector but one row of a matrix. We can do this by using the <code>view</code> or <code>reshape</code>function:</p> <pre><code>x.view(1,6),x.reshape(1,6)\n</code></pre> <p>output</p> <pre><code>(tensor([[1, 2, 3, 4, 5, 6]]), tensor([[1, 2, 3, 4, 5, 6]]))\n</code></pre> <p>Now what about making it a column?</p> <pre><code>x.view(6,1),x.reshape(6,1)\n</code></pre> <p>output</p> <pre><code>(tensor([[1],\n         [2],\n         [3],\n         [4],\n         [5],\n         [6]]),\n tensor([[1],\n         [2],\n         [3],\n         [4],\n         [5],\n         [6]]))\n</code></pre> <p>Now what if we wanted to convert it back to just that list of numbers?</p> <pre><code>x.view(1,6).squeeze()\n</code></pre> <p>output</p> <pre><code>tensor([1, 2, 3, 4, 5, 6])\n</code></pre> <p>Hmmm, I want to have my column back, let's reverse with unsqueeze! Here we set dim=1 so that it unsqueezes back to a column, dim=0 would make it a row:</p> <pre><code>x.view(1,6).squeeze().unsqueeze(dim=1)\n</code></pre> <p>output</p> <pre><code>(tensor([[1],\n         [2],\n         [3],\n         [4],\n         [5],\n         [6]])\n</code></pre> <p>Now what if we needed to stack our tensors? Well the <code>stack</code> function will do just that. Here we stack by row:</p> <pre><code>torch.stack([x,x],dim=0)\n</code></pre> <p>output</p> <pre><code>tensor([[1, 2, 3, 4, 5, 6],\n      [1, 2, 3, 4, 5, 6]])\n</code></pre> <p>Let's stack by column as well!   </p> <pre><code>torch.stack([x,x],dim=1)\n</code></pre> <p>output</p> <pre><code>tensor([[1, 1],\n        [2, 2],\n        [3, 3],\n        [4, 4],\n        [5, 5],\n        [6, 6]])\n</code></pre>"},{"location":"machine_learning/deep_learning/01_basics/#accessing-elements-in-a-tensor","title":"Accessing Elements in a Tensor","text":"<p>To get specfic elements in a tensor we should start by taking an example tensor:</p> <pre><code>one_mat = torch.tensor([[[1,2,3],\n                         [4,5,6],\n                         [7,8,9]]])\none_mat\n</code></pre> <p>output</p> <pre><code>tensor([[[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]])\n</code></pre> <p>Let's see it's shape:</p> <pre><code>one_mat.shape\n</code></pre> <p>output</p> <pre><code>torch.Size([1, 3, 3])\n</code></pre> <p>Now let's see what is the first element in the tensor:</p> <pre><code>one_mat[0]\n</code></pre> <p>output</p> <pre><code>tensor([[1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]])\n</code></pre> <p>Hey look at that the first element is the matrix, let's look at the first element of this first element:</p> <pre><code>one_mat[0][0]\n</code></pre> <p>output</p> <pre><code>tensor([1, 2, 3])\n</code></pre> <p>Now how about the first element of this element:</p> <pre><code>one_mat[0][0][0]\n</code></pre> <p>output</p> <pre><code>tensor(1)\n</code></pre>"},{"location":"machine_learning/deep_learning/01_basics/#tensor-statistics","title":"Tensor Statistics","text":"<p>Now often times we may want to summarize elements in our tensors. What is the maximum or minimum value? How about the sum? The mean? We can do this using the following functions:</p> <pre><code># here we specify that the data type is a float so that we can get these summary statistics!\nx = torch.tensor([1, 2, 3, 4], dtype=torch.float)\nprint(x.min(), x.max(), x.mean(), x.sum())\n</code></pre> <p>output</p> <pre><code>tensor(1.) tensor(4.) tensor(2.5000) tensor(10.)\n</code></pre> <ul> <li>Minimum: \\(\\text{min}(x)\\) returns the smallest element.</li> <li>Maximum: \\(\\text{max}(x)\\) returns the largest element.</li> <li>Mean: \\(\\text{mean}(x) = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\), where \\(n\\) is the number of elements.</li> <li>Sum: \\(\\text{sum}(x) = \\sum_{i=1}^{n} x_i\\), where \\(n\\) is the number of elements.</li> </ul> <p>It may be important to grab where in the tensor our minimum or maximum is as well:</p> <pre><code>x = torch.tensor([10, 20, 30])\nprint(x.argmax(), x.argmin())  # Index of max and min\n</code></pre> <p>output</p> <pre><code>tensor(2) tensor(0)\n</code></pre>"},{"location":"machine_learning/deep_learning/01_basics/#working-with-gpus","title":"Working with GPUs","text":"<p>Machine learning can be hard and often needs computer power to make it happen. One way it can make that happen is with graphic processing units or GPUs. These are hardware that are great at handling certain tasks like matrix math really fast. So it can be useful to move our tensors to a GPU so that our AI models can run faster! But first let's check to see if they are available:</p> <pre><code>torch.cuda.is_available()  # True if GPU is available\n</code></pre> <p>output</p> <pre><code>True\n</code></pre> <p>Now to move a tensor to GPU:</p> <p><pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntensor = torch.tensor([1, 2, 3]).to(device)\nprint(tensor)\n</code></pre> To move it back to the central processing unit or CPU, just do the following:</p> <pre><code>tensor_cpu = tensor.to('cpu')\n</code></pre>"},{"location":"machine_learning/deep_learning/01_basics/#random-values-and-reproducibility","title":"Random Values and Reproducibility","text":"<p>Now we often times initialize model values with random numbers to ensure those numbers are consistent, you can set a \"seed\" number to ensure consistency:</p> <pre><code>torch.manual_seed(42)\nrandom_tensor = torch.rand(3, 4)\nprint(random_tensor)\n</code></pre> <p>output</p> <pre><code>tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n</code></pre>"},{"location":"machine_learning/deep_learning/01_basics/#tensor-and-numpy-integration","title":"Tensor and NumPy Integration","text":"<p>Tensors are special objects designed for complex operations. However, we may want to convert them back to numpy objects so that we can do things like plot model results. First we will show you how to take a numpy array and make a tensor:</p> <pre><code># NumPy to PyTorch\nimport numpy as np\nnp_array = np.array([1, 2, 3])\ntensor = torch.from_numpy(np_array)\nprint(tensor)\n</code></pre> <p>output</p> <pre><code>tensor([1, 2, 3], dtype=torch.int32)\n</code></pre> <p>Now let's convert that back to numpy!</p> <pre><code># PyTorch to NumPy\nnumpy_array = tensor.numpy()\nprint(numpy_array)\n</code></pre> <p>output</p> <pre><code>[1, 2, 3]\n</code></pre> <p>Key Points</p> <ul> <li>Tensors store data across different dimensions: scalars (0D), vectors (1D), matrices (2D), and higher.</li> <li>Basic element-wise operations can be used to modify tensors (i.e. addition, subtraction, multiplication, division, etc.).</li> <li>For matrix multiplication, the inner dimensions must match!</li> <li>Tensor shapes can be manipulated which can be necessary for things like matrix multiplication.</li> <li>Tensor elements are accessed by the bracket they are in</li> <li>GPUs can be used for faster computation.</li> <li>Set random seeds with torch.manual_seed() for reproducibility.</li> <li>Converting between PyTorch tensors and NumPy arrays can be useful when plotting model metrics</li> </ul>"},{"location":"machine_learning/deep_learning/02_linear_model/","title":"Linear Models","text":"<p>Prerequisites</p> <ul> <li>Deep Learning Setup : Setup workspace and download python libraries</li> </ul> <p>Learning Objectives</p> <ol> <li>Getting the Data</li> <li>Training and Test Data</li> <li>Training the Model</li> <li>Saving Models</li> </ol> <p>Overview of Linear Regression with PyTorch</p> <p> Generated with Napkin </p>"},{"location":"machine_learning/deep_learning/02_linear_model/#1-getting-the-data","title":"1. Getting the Data","text":"<p>Perhaps the easiest way to get started with deep learning is to predict the values from a simple linear model: \\(y = weight*x + bias\\). This line can be thought of as a set of predicted values and the distance from those values to the real points is the error:</p> <p>Diagram of a Best Fit Line</p> <p> </p> <p>We are going to start with a simple example of a line where we know the slope (i.e. the weight), the bias (i.e. the y intercept) and fill it with values from 0 to 1 that increase by 0.02 each value.</p> <pre><code>import torch\nimport plotly.graph_objects as go\n\n# Create a simple linear dataset\nweight = 0.8\nbias = 0.2\nX = torch.arange(0, 1, 0.02).unsqueeze(1)\ny = weight * X + bias\n</code></pre>"},{"location":"machine_learning/deep_learning/02_linear_model/#2-training-and-test-data","title":"2. Training and Test Data","text":"<p>When we work with models, we often want to train our model to predict some result and then test it to see if it is working. Usually we allocate the majority of the data (in this case 80%) to the training data, and a minority of the data (20%) as our testing data.</p> <pre><code># Split data into training and test sets (80/20 split)\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n</code></pre> <p>When working with data, it is critical that we visually inspect it! You would be suprised how many mistakes or eureka moments can happen just by making a graph. Here we will use plotly to plot our training data, testing data and any predictions we might make.</p> <pre><code># Plot data using Plotly\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    fig = go.Figure()\n\n    # Plot training data\n    fig.add_trace(go.Scatter(x=train_data.squeeze(), y=train_labels.squeeze(), mode='markers', name='Training Data', marker=dict(color='#1c4f96')))\n\n    # Plot testing data\n    fig.add_trace(go.Scatter(x=test_data.squeeze(), y=test_labels.squeeze(), mode='markers', name='Testing Data', marker=dict(color='#f58160')))\n\n    # Plot predictions if available\n    if predictions is not None:\n        fig.add_trace(go.Scatter(x=test_data.squeeze(), y=predictions.squeeze(), mode='markers', name='Predictions', marker=dict(color='#8f3974')))\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title='X',\n        yaxis_title='Y',\n        template='plotly_white'\n    )\n    # Show the plot\n    fig.show()\n\nplot_predictions(X_train, y_train, X_test, y_test)\n</code></pre> <p>Features v. Outcome for Testing and Training Data</p> <p> </p> <p>To create a model we will start by defining a few things. We are goint to make a class, <code>LrModel</code>. A class is essentially a map for creating objects and has methods and attributes: - Attributes are variables that belong to the class - Methods are functions inside that class that help perform some function</p> <p>If we use this class on some data that is an instance of the class. Now here we define one layer, <code>linear</code> with one input and one output. Then we define a method, <code>forward</code> where the output of the single layer is returned. This is essentially describing how inputs move through the model. You'll note that after we initiate the model, we print it's initial random parameters with <code>model.state_dict</code>.</p> <pre><code>import torch.nn as nn\n\n# Define a linear regression model\nclass LrModel(nn.Module): # ensure that nn.Module is inherited\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(in_features=1, out_features=1) # define one layer, with one input and one output\n\n    def forward(self, x): # create a method, that will return the output of the one layer, this describes how inputs move through the model \n        return self.linear(x)\n\n# Initiate the model\nmodel = LrModel() # initiate the model and create an instance!\n\n# Check initial parameters\nprint(model.state_dict()) \n</code></pre>"},{"location":"machine_learning/deep_learning/02_linear_model/#3-training-the-model","title":"3. Training the Model","text":"<p>Ok so we have the shell of a model, now we need to train it on our data so that it can make predictions! To do this we are going to introduce a couple concepts:</p> <ul> <li>Loss Function: measures how far off the predictions are from the real data points. Here we are using the L1 loss AKA Mean Absolute Error (MAE).</li> <li>Optimizer: The optimizer will update the parameters of the model in order to minimize the loss. Here we are using stochastic gradient descent with a learning rate, which controls how big the updates are, of 0.01</li> <li>Epoch: An epoch is just how many times the model is reevaluated, and parameters are updated.</li> <li>Backpropagation: This involves calculating the loss from your input and then using this information you backpropagate that information into the model to update the weights. Essentially, allowing the model to learn and improve it's predictions.</li> <li>Gradient Descent: So we are left with a question - what weights, lead to the smallest loss value? To get to this minimum we need to use gradient descent, where we calculate the derivative of the loss function with respect to the model weights to get to some minimum loss value that has our desired model weight!</li> </ul> <p>Gradient Descent</p> <p>  Adapted from the carpentries tutorial on deep learning</p> <pre><code># Loss function and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 500\ntrain_loss_vals = []\ntest_loss_vals = []\nepoch_vals = []\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    y_pred = model(X_train)  # Forward pass\n    loss = loss_fn(y_pred, y_train)  # Calculate loss\n\n    optimizer.zero_grad()  # Zero gradients so that we clear updates from the updates from the previous step\n    loss.backward()  # Backpropagation to calculate how each parameter affects the error\n    optimizer.step()  # Update parameters or weights to reduce that error\n\n    # Evaluate every 50 epochs\n    if epoch % 50 == 0:\n        model.eval() # set the model in evaluation mode\n        with torch.inference_mode(): # get into inference mode\n            test_pred = model(X_test)\n            test_loss = loss_fn(test_pred, y_test)\n            train_loss_vals.append(loss.detach().numpy())\n            test_loss_vals.append(test_loss.detach().numpy())\n            epoch_vals.append(epoch)\n            print(f\"Epoch {epoch}: Train Loss = {loss.item()}, Test Loss = {test_loss.item()}\")\n</code></pre> <p>We should point out how we are using torch's inference mode, <code>with torch.inference_mode()</code>, to make predictions. This will stop gradient calculations in order to evaluate our model in a memory efficient way. Now let's take a look at the relationship between the number of epochs and the losses for both the test and training data. </p> <pre><code># define a function for plotting losses v. epochs\ndef plot_epoch_losses(epochs, train_losses, test_losses):\n    fig = go.Figure()\n\n    # Plot training losses\n    fig.add_trace(go.Scatter(x=epochs, y=train_losses, mode='lines', name='Training Loss', marker=dict(color='#1c4f96')))\n\n    # Plot testing losses\n    fig.add_trace(go.Scatter(x=epochs, y=test_losses, mode='lines', name='Testing Loss', marker=dict(color='#f58160')))\n\n    # Update layout\n    fig.update_layout(\n        title='Training and Testing Losses vs Epochs',\n        xaxis_title='Epochs',\n        yaxis_title='Loss',\n        legend_title='Legend',\n        template='plotly_white'\n    )\n\n    # Show the plot\n    fig.show()\n\nplot_epoch_losses(epoch_vals, train_loss_vals, test_loss_vals)\n</code></pre> <p>Epochs v. Loss</p> <p> </p> <p>Here we see that the testing error is higher than the training data for the first few hundred epochs until around epoch 300. Now that we have trained our model and see that the loss is quite low, let's compare these predicted values back to the original values.</p> <pre><code># Make predictions after training\nmodel.eval()\nwith torch.inference_mode():\n    y_preds = model(X_test)\n\n# Plot predictions using Plotly\nplot_predictions(X_train, y_train, X_test, y_test, y_preds)\n</code></pre> <p>True and Predicted Data</p> <p> </p> <p>They look great! The predicted values appear to track both tested and traing values well!</p>"},{"location":"machine_learning/deep_learning/02_linear_model/#4-saving-models","title":"4. Saving Models","text":"<p>To save your model, you can use <code>torch.save</code> and save the current state of the model with <code>model.state_dict</code> and then designate where to put your model with a file name (here we call it <code>lr_model.pth</code>).</p> <pre><code># Save model\ntorch.save(model.state_dict(), \"lr_model.pth\")\n</code></pre> <p>Now to load this model we can use <code>torch.load</code> and then the path to our model file name, <code>lr_model.pth</code>.</p> <pre><code># Load model\nloaded_model = LrModel()\nloaded_model.load_state_dict(torch.load(\"lr_model.pth\"))\n</code></pre>"},{"location":"machine_learning/deep_learning/02_linear_model/#5-into-the-neuralverse","title":"5. Into the Neuralverse","text":"<p>We have created a simple situation where there is one input and one output. However, the power in deep learning lies in connecting what we call nodes into complex architectures, which we typically call our input layer, a set of hidden layers and an output layer:</p> <p>Neural Network Architecture Overview </p> <p> </p> <p>What does each node look like though? Well each node is like a neuron, it takes a set of inputs, processes them and creates an output. </p> <p>Neural Network Node Overview </p> <p> </p> <p>Here we see that each node is the sum of the products of the variables and weights, then that sum is fed through an activation function, which decides, hey is that sum enough to trigger an output. We will get more into that later though!</p> <p>Key Points</p> <ul> <li>It is a good idea to break your data into two parts, training and testing, to see how well your model might do with outside data</li> <li>A simple linear model is composed of a \\(y = weight*x + bias\\) </li> <li>The model weights are updated each time the model is trained (an epoch), where the weights are updated as to minimize the loss, or how far off the predicted data points are from the true data points.</li> <li>Save your models to use later or on other data</li> </ul>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/","title":"Classifiers/Multilayer Perceptrons","text":"<p>Prerequisites</p> <ul> <li>Deep Learning Setup : Setup workspace and download python libraries</li> </ul> <p>Learning Objectives</p> <ol> <li>Classifiers</li> <li>Data Preparation and Exploration</li> <li>Build the Model</li> <li>Loss Functions</li> <li>Optimizers</li> <li>Activation Functions</li> <li>Train and Evaluate the Model</li> <li>The Logit, the Probability and the Label</li> <li>Evaluation Metrics</li> </ol> <p>Overview of Classification</p> <p> Generated with Napkin </p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#classifiers","title":"Classifiers","text":"<p>In the previous section, we predicted numeric values using in a regression model. However, deep learning is not limited to predicting numbers but classes as well. So how do we do this? </p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#data-preparation-and-exploration","title":"Data Preparation and Exploration","text":"<p>We are going to start by scoping a problem for our model to solve. In this case we are going to use glioblastoma gene expression data, where we will use a gene's expression (level of RNA that gene has made) to predict a phenotype. To start we will try to predict smoking status. First, let's load the data and take a peak!</p> <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\ngbm = pd.read_csv(\"../data/gbm_data.csv\",on_bad_lines='skip')\ngbm.shape\n</code></pre> <p>output</p> <pre><code>(87, 133)\n</code></pre> <p>Great! We can see that this data frame contains 87 rows and 133 columns. Let's take a peak at the column that would contain smoking status:</p> <pre><code>gbm.loc[:,\"SMOKING_HISTORY\"]\n</code></pre> <p>output</p> <pre><code>0          Current reformed smoker within past 15 years\n1          Current reformed smoker within past 15 years\n2                                                   NaN\n                            ...                        \n86    Lifelong non-smoker: Less than 100 cigarettes ...\nName: SMOKING_HISTORY, Length: 87, dtype: object\n</code></pre> <p>This doesn't look particularly clean, so we will just have to make a new column with the smoking status. We will do this by saying that if we see \"non-smoker\" in the text, then we classify the sample as coming from a non-smoker. But before this we should remove the NA rows, given we don't have any smoking information on them!</p> <pre><code>gbm = gbm[gbm['SMOKING_HISTORY'].notna()]\ngbm['smoking_status'] = np.where(gbm['SMOKING_HISTORY'].str.contains('non-smoker'), 'non-smoker', 'smoker')\ngbm.loc[:,\"smoking_status\"]\n</code></pre> <p>output</p> <pre><code>0         smoker\n1         smoker\n3         smoker\n         ...    \n84        smoker\n85    non-smoker\n86    non-smoker\nName: smoking_status, Length: 75, dtype: object\n</code></pre> <p>Now before going forward, it is critical to take a look at our data. Are there any obvious patterns right off the bat? Let's visualize the first few variables using a pairplot:</p> <pre><code># Create a pair plot\nfig = px.scatter_matrix(gbm, \n            dimensions=['LINC00159','EFTUD1P1','C20orf202','KRT17P8','RPL7L1P9'],\n            color=\"smoking_status\",\n            title=\"Pair Plot of Selected Variables\")\nfig.update_layout(\n    height=700,\n    width=700,\n    template='plotly_white'\n          )\nfig.show()\n</code></pre> <p>Pairplot of Selected Features</p> <p> </p> <p>We can see that a few of our variables can sort of startify smokers and non-smokers. Now that we have the smoking status variable in our data frame and we have a few features to use, we can start getting our data ready for our model! However, to give this to a machine learning model we need to make this into a number (Here we will do 0 and 1).</p> <pre><code>gbm_filt = gbm.loc[:,['LINC00159','EFTUD1P1','C20orf202','KRT17P8','RPL7L1P9','smoking_status']]\ngbm_filt['smoking_status'] = gbm_filt['smoking_status'].astype('category').cat.codes\ngbm_filt['smoking_status']\n</code></pre> <p>output</p> <pre><code>0     1\n1     1\n     ..\n85    0\n86    0\nName: smoking_status, Length: 75, dtype: int8\n</code></pre> <p>Now let's split our data into our features (things used to predict) and outcome (the thing to be predicted):</p> <pre><code># Split the data into features and outcome variable\nX = gbm_filt.drop('smoking_status', axis=1).values\ny = gbm_filt['smoking_status'].values\n</code></pre> <p>When dealing with variables we often need to make sure they are on the same scale. That way, one variable doesn't have way more pull than another just because one is an order of magnitude larger.</p> <pre><code># Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n</code></pre> <p>Now we are going to split our data into training and test sets. Then we will convert our data into tensors so Pytorch can use the data: </p> <pre><code># Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=81)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n</code></pre> <p>We will now convert our training and test data into a <code>TensorDataset</code> object, then we manage that object using <code>DataLoader</code>, which acts to process the data in batches for parallel processing: </p> <pre><code># Create DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n</code></pre>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#build-the-model","title":"Build the Model","text":"<p>With our data prepared, we can make our model! But we have decisions to make. Our model is going to be a little more complex than our previous simple linear model. We are going to make what is called a multilayer perceptron. It sounds complicated but all it is is a neural network where all nodes in one layer are connected to all nodes in the next layer:</p> <p>Multilayer Perceptron Architecture</p> <p> </p> <p>This is great starting point with neural networks as we are not doing anything funky with connecting nodes. Each node is just connected to all nodes in the next layer. But we still need to make decisions about our loss function, the optimizer and now our activation function given we are connecting multiple layers.</p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#loss-functions","title":"Loss Functions","text":"<p>As a review loss functions tell us how far off our predicted value is to our real value and there are a few different ways to define loss:</p> Loss Function Task Equation Description PyTorch Function Binary Cross-Entropy Loss Binary classification \\(\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\) - \\(y\\): true label  - \\(\\hat{y}\\): predicted probability  - \\(N\\): number of samples <code>torch.nn.BCELoss</code> or <code>torch.nn.BCEWithLogitsLoss</code> Cross-Entropy Loss (Multi-Class) Multi-class classification \\(\\text{CE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\\) - \\(C\\): number of classes  - \\(y_i\\): one-hot encoded true label  - \\(\\hat{y}_i\\): predicted probability for class \\(i\\) <code>torch.nn.CrossEntropyLoss</code> Mean Absolute Error (MAE) / L1 Loss Regression \\(\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} \\|y_i - \\hat{y}_i\\|\\) - \\(y\\): true value  - \\(\\hat{y}\\): predicted value <code>torch.nn.L1Loss</code> Mean Squared Error (MSE) / L2 Loss Regression \\(\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\\) - \\(y\\): true value  - \\(\\hat{y}\\): predicted value <code>torch.nn.MSELoss</code> <p>Now you may notice that when using the cross-entropy loss, labels are one-hot encoded. What does that even mean? Well all this means is that in a column with two or more unique values, unique values are made into new columns, and if the original column had that value, you put a 1, if not, you put a 0. Take a look at this visualization:</p> <p>One-Hot Encoding</p> <p> </p> <p>Now how about optimizers?</p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#optimizers","title":"Optimizers","text":"<p>Optimizers tweak our model so that we optimize our model parameters to minimize the loss:</p> Optimizer Task Equation / Update Rule Description PyTorch Function Stochastic Gradient Descent (SGD) General optimization \\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)\\) \\(\\theta_t\\): parameters at step \\(t\\) \\(\\eta\\): learning rate  \\(\\nabla_\\theta J(\\theta_t)\\): gradient of the loss function  Basic optimization algorithm <code>torch.optim.SGD</code> Adam Adaptive momentum and learning rate \\(m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J(\\theta_t)\\) \\(v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_\\theta J(\\theta_t))^2\\) \\(\\hat{m}\\_t = \\frac{m_t}{1 - \\beta_1^t}\\) \\(\\hat{v}\\_t = \\frac{v_t}{1 - \\beta_2^t}\\) \\(\\theta_{t+1} = \\theta_t - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}}\\) \\(\\theta_t\\): parameters at step \\(t\\) \\(m_t\\): first moment (mean) estimate  \\(v_t\\): second moment (variance) estimate  \\(\\hat{m}_t\\), \\(\\hat{v}_t\\): bias-corrected moment estimates  \\(\\eta\\): learning rate  \\(\\beta_1\\), \\(\\beta_2\\): exponential decay rates  \\(\\epsilon\\): small constant for numerical stability <code>torch.optim.Adam</code> <p>Eww that is a lot of math feel free to click the example below to see how a simple gradient descent is calculated using SGD and an MSE loss function:</p> Gradient Descent Example <p>When we train our model, we need to know how changing the parameters\u2014like weights and biases\u2014affects the output. This is where partial derivatives come in.     Think of them as tiny nudges in different directions that tell us how much each part of our function changes when we adjust one variable while keeping the     others fixed. We\u2019re going to look at how to find these nudges step by step, using a simple linear regression model:</p> <p>\\(J(w, b) = \\frac{1}{2} (y - \\hat{y})^2\\)</p> <p>where \\(\\hat{y}\\) is our prediction:</p> <p>\\(\\hat{y} = w \\cdot x + b\\)</p> <p>Step 1: Partial Derivative with Respect to \\(w\\)</p> <p>First up, we need to figure out how changing \\(w\\) affects our loss \\(J\\). Mathematically, that looks like this:</p> <p>\\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2} (y - \\hat{y})^2 \\right]\\)</p> <p>Let\u2019s plug in our prediction:</p> <p>\\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2} (y - (w \\cdot x + b))^2 \\right]\\)</p> <p>We apply the chain rule here:</p> <ul> <li>The outer part, \\((y - \\hat{y})^2\\), becomes \\((y - \\hat{y}) \\cdot (-1)\\) when we differentiate.</li> <li>The inner part, \\(\\hat{y}\\), with respect to \\(w\\) is just \\(x\\).</li> </ul> <p>Putting it together:</p> <p>\\(\\frac{\\partial J}{\\partial w} = -(y - \\hat{y}) \\cdot x\\)</p> <p>Plug in the Values:</p> <p>With our data point \\((x, y) = (2, 3)\\) and initial parameters \\(w_0 = 1.5\\), \\(b_0 = 0.1\\):</p> <p>\\(\\hat{y} = 1.5 \\cdot 2 + 0.1 = 3.1\\)</p> <p>Gradient with respect to \\(w\\):</p> <p>\\(\\frac{\\partial J}{\\partial w} = -(3 - 3.1) \\cdot 2 = 0.2\\)</p> <p>Step 2: Partial Derivative with Respect to \\(b\\)</p> <p>Now let\u2019s see what happens when we change \\(b\\):</p> <p>\\(\\frac{\\partial J}{\\partial b} = -(y - \\hat{y})\\)</p> <p>Plug in the values:</p> <p>\\(\\frac{\\partial J}{\\partial b} = -(3 - 3.1) = 0.1\\)</p> <p>Step 3: Combine into a Gradient</p> <p>We store these partial derivatives in a gradient:</p> <p>\\(\\nabla J(w, b) = \\begin{bmatrix} \\frac{\\partial J}{\\partial w} \\\\ \\frac{\\partial J}{\\partial b} \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix}\\)</p> <p>Step 4: Update the Parameters</p> <p>Finally, we use the gradient descent update rule (with \\(\\eta\\) being our learning rate - here it is 0.01):</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)\\)</p> <p>For \\(w\\) and \\(b\\), this means:</p> <ul> <li>Update \\(w\\):</li> </ul> <p>\\(w_{1} = 1.5 - 0.01 \\cdot 0.2 = 1.498\\)</p> <ul> <li>Update \\(b\\):</li> </ul> <p>\\(b_{1} = 0.1 - 0.01 \\cdot 0.1 = 0.099\\)</p> <p>The Result</p> <p>After one step, our new parameters are:</p> <ul> <li>Updated \\(w\\): 1.498</li> <li>Updated \\(b\\): 0.099</li> </ul> <p>And that update to our model parameter is a step towards improving our model!</p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#activation-functions","title":"Activation Functions","text":"<p>Once we have updated our model parameters to optimize our loss, we need to decide what to do with the output. Depending on our goal we will use different activation functions:</p> Activation Function Equation Description Typical Use PyTorch Function ReLU (Rectified Linear Unit) \\(\\text{ReLU}(x) = \\max(0, x)\\) - Outputs the input directly if positive, otherwise outputs zero  - Helps mitigate the vanishing gradient problem Hidden layers of deep neural networks <code>torch.nn.ReLU()</code> or <code>torch.relu()</code> Sigmoid \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) - Squashes input to a range between 0 and 1  - Useful for probabilities Output layers in binary classification <code>torch.nn.Sigmoid()</code> or <code>torch.sigmoid()</code> Tanh \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\) - Squashes input to a range between -1 and 1  - Centered around zero, leading to better convergence than Sigmoid Hidden layers in neural networks <code>torch.nn.Tanh()</code> or <code>torch.tanh()</code> Softmax \\(\\sigma(\\vec{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\\) - Converts an input vector \\(\\vec{z}\\) into a probability distribution over \\(K\\) classes  - \\(e^{z_i}\\): standard exponential function for input vector  - \\(K\\): number of classes in the multi-class classifier  - Ensures that outputs sum to 1 Output layer for multi-class classification <code>torch.nn.Softmax()</code> or <code>torch.softmax()</code> Leaky ReLU \\(\\text{Leaky ReLU}(x) = \\begin{cases} x, &amp; \\text{if } x &gt; 0 \\\\\\\\ \\alpha x, &amp; \\text{if } x \\leq 0 \\end{cases}\\) - Similar to ReLU but with a small slope \\(\\alpha\\) for negative \\(x\\)  - Prevents the dying ReLU problem Hidden layers in deep networks <code>torch.nn.LeakyReLU()</code> <p>Let's create a model that takes in our 5 genes, creates two hidden layers with 64 nodes, uses a ReLU activation function, and returns one output layer:</p> <pre><code>class SSModel(nn.Module):\n    '''\n    Create a model to predict smoking status\n    which is one column 0 for non-smoker, 1 for smoker\n    '''\n    def __init__(self, input_size,num_nodes=64):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_nodes) # take input_size number of features and make a hidden layer with 64 nodes\n        self.relu = nn.ReLU() # use ReLU as the activation function\n        self.fc2 = nn.Linear(num_nodes, 1) # we are predicting values in one column\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Initialize the model, and specify the number of \n# features going in and how many going out\ninput_size = X_train.shape[1]\nmodel = SSModel(input_size)\n</code></pre> <p>The power of an activation function is that it can introduce non-linearity into the model. This allows for the learning of complex patterns - patterns that a linear model might miss. </p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#train-and-evaluate-the-model","title":"Train and Evaluate the Model","text":"<p>Now let's specify our optimizer (to get to our optimum weights) and our loss function (to specify how far away our model is from the truth):</p> <pre><code># Specify the loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n</code></pre> <p>With all the pieces in place, we can train and evaluate our model!</p> <pre><code># Training loop\nnum_epochs = 1000\nepoch_vals = []\nloss_vals = []\nacc_vals = []\npre_vals = []\nrecall_vals = []\nf1_vals = []\n\nfor epoch in range(num_epochs):\n    # Training the model\n    model.train()  # set model in training mode\n    running_loss = 0.0  # to accumulate loss for the epoch\n\n    for X_batch, y_batch in train_loader:\n        #X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # move to device\n\n        # Forward pass\n        outputs = model(X_batch)  # run model on the batch\n        y_long = y_batch.view(-1, 1).float()  # reshape y for compatibility\n        loss = criterion(outputs, y_long)  # calculate loss\n\n        # Backward pass and optimization\n        optimizer.zero_grad()  # clear gradients\n        loss.backward()  # backpropagation\n        optimizer.step()  # gradient descent\n\n        running_loss += loss.item()  # accumulate batch loss\n\n    # Append the average loss for the epoch\n    loss_vals.append(running_loss / len(train_loader))\n\n    # Evaluation\n    model.eval()  # set model in evaluation mode\n    with torch.inference_mode():\n        all_preds = []  # list for predicted labels\n        all_labels = []  # list for true labels\n\n        # Model predictions on test data\n        for X_batch, y_batch in test_loader:\n            #X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # move to device\n            outputs = model(X_batch)  # get model outputs\n            probabilities = torch.sigmoid(outputs)  # get probabilities from logits\n            predicted = (probabilities &gt; 0.5).float()  # convert probabilities to labels\n\n            all_preds.extend(predicted.cpu().numpy())  # append predictions\n            all_labels.extend(y_batch.cpu().numpy())  # append true labels\n\n        # Calculate metrics\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='binary')\n        f1 = f1_score(all_labels, all_preds, average='binary')\n\n        # Append metrics to lists\n        epoch_vals.append(epoch)\n        acc_vals.append(accuracy)\n        pre_vals.append(precision)\n        recall_vals.append(recall)\n        f1_vals.append(f1)\n</code></pre> <p>Woah that is a lot! Let's break it down piece by piece:</p> <p><pre><code>num_epochs = 1000\nepoch_vals = []\nloss_vals = []\nacc_vals = []\npre_vals = []\nrecall_vals = []\nf1_vals = []\n</code></pre> - Sets the number of epochs for training and lists to store epoch numbers, loss, accuracy, precision, recall, and F1-score throughout training.</p> <pre><code>for epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n</code></pre> <ul> <li>Begins the training loop that runs for each epoch.</li> <li>Sets the model to training mode and initializes a variable to accumulate the total loss for the current epoch.</li> </ul> <pre><code>for X_batch, y_batch in train_loader:\n    outputs = model(X_batch)\n    y_long = y_batch.view(-1, 1).float()\n    loss = criterion(outputs, y_long)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    running_loss += loss.item()\n</code></pre> <ul> <li>Iterates over batches in the training set.</li> <li>Runs a forward pass to get predictions, calculates the loss, clears the previous gradients, performs backpropagation to calculate new gradients, and updates the model parameters using the optimizer.</li> <li>Accumulates the batch loss into running_loss.</li> </ul> <pre><code>loss_vals.append(running_loss / len(train_loader))\n</code></pre> <ul> <li>Calculates and appends the average loss for the epoch to <code>loss_vals</code>.</li> </ul> <pre><code>model.eval()\nwith torch.inference_mode():\n    all_preds = []\n    all_labels = []\n</code></pre> <ul> <li>Switches the model to evaluation mode, disabling certain training-specific behaviors and ensuring no gradients are computed for memory and computation efficiency.</li> <li>Creates lists to store predictions and true labels for evaluation.</li> </ul> <pre><code>for X_batch, y_batch in test_loader:\n    outputs = model(X_batch)\n    probabilities = torch.sigmoid(outputs)\n    predicted = (probabilities &gt; 0.5).float()\n    all_preds.extend(predicted.cpu().numpy())\n    all_labels.extend(y_batch.cpu().numpy())\n</code></pre> <ul> <li>Iterates over batches in the test set, runs model outputs, applies the sigmoid function to convert logits to probabilities, and thresholds probabilities to generate binary labels.</li> <li>Appends the predictions and true labels to their respective lists for later metric calculations</li> </ul>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#the-logit-the-probability-and-the-label","title":"The Logit, the Probability and the Label","text":"<p>We are going to break here before moving on to explain the rest of the code to talk a bit about logits, probabilities and labels. We are predicting smoker or not smoker. But the output we get from our model is not a 1 or 0. To get that label we need to first take the output of our model (or logit) and convert it to a probability using the sigmoid activation function. This will squeeze the values between 0 and 1. Then we can set some threshold, here over 0.5, and if the value is over that threshold we label it 1 and if it is below we label it 0. And that is how we get labels from our model output! Now back to the rest of the code:</p> <p><pre><code>accuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\nrecall = recall_score(all_labels, all_preds, average='binary')\nf1 = f1_score(all_labels, all_preds, average='binary')\n\nepoch_vals.append(epoch)\nacc_vals.append(accuracy)\npre_vals.append(precision)\nrecall_vals.append(recall)\nf1_vals.append(f1)\n</code></pre> - Calculates evaluation metrics: accuracy, precision, recall, and F1-score based on the collected predictions and true labels. - Appends these values to their respective list </p>"},{"location":"machine_learning/deep_learning/03_classifier_mlp/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>You'll notice that at the end of our evaluation loop we calculate some metrics to see how well our model is doing:</p> Metric Formula Explanation Accuracy \\(\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\\) Measures the ratio of correctly predicted labels to the total number of labels. It gives an overall indication of model performance. Precision \\(\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\\) Measures how many of the predicted positive labels are actually positive. It indicates the model\u2019s ability to avoid false positives. Recall \\(\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\\) Measures how many actual positive labels were correctly predicted. It shows the model\u2019s ability to capture all positive cases. F1-score \\(\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\) The harmonic mean of precision and recall. It balances the trade-off between precision and recall and is useful when both are important. <p>Now let's visualize how these change over different numbers of epochs:</p> <pre><code>import plotly.graph_objects as go\n\n# Create a figure\nfig = go.Figure()\n\n# Add traces for each metric\n# add in loss \nfig.add_trace(go.Scatter(x=list(range(1, len(epoch_vals) + 1)), y=loss_vals, mode='lines+markers', name='Loss'))\n# add in accuracy\nfig.add_trace(go.Scatter(x=list(range(1, len(epoch_vals) + 1)), y=acc_vals, mode='lines+markers', name='Accuracy'))\n# add in precision\nfig.add_trace(go.Scatter(x=list(range(1, len(epoch_vals) + 1)), y=pre_vals, mode='lines+markers', name='Precision'))\n# add in recall\nfig.add_trace(go.Scatter(x=list(range(1, len(epoch_vals) + 1)), y=recall_vals, mode='lines+markers', name='Recall'))\n# add in F1 score\nfig.add_trace(go.Scatter(x=list(range(1, len(epoch_vals) + 1)), y=f1_vals, mode='lines+markers', name='F1 Score'))\n\n# Update layout\nfig.update_layout(\n    title='Test Metrics Over Epochs',\n    xaxis_title='Epoch',\n    yaxis_title='Value',\n    legend_title='Metric',\n    template='plotly_white'\n)\n\n# Show the plot\nfig.show()\n</code></pre> <p>Classifier Performance Metrics</p> <p> </p> <p>Great! Here we see that after around epoch 700 these metrics plateau. However, that is not to say that with more epochs we couldn't get better performance!</p> <p>Key Points</p> <ul> <li>Deep learning models can be used for both regression (predicting numbers) and classification (predicting classes).</li> <li>Data preparation usually involves cleaning, creating relevant columns, visualizing data, and normalizing features to ensure consistency in the model.</li> <li>The data is usually split into training and test sets.</li> <li>We can convert this data into PyTorch tensors and TensorDataset objects for efficient batch processing.</li> <li>Multilayer perceptrons are neural networks wehere all nodes in one layer are connected to all nodes in the next layer</li> <li>The training loop includes forward passes, loss calculation, backpropagation, and parameter updates.</li> <li>We can evaluate our model using metrics like accuracy, precision, recall, and F1-score  to gauge model performance.</li> </ul>"},{"location":"machine_learning/deep_learning/04_autoencoder/","title":"Autoencoders","text":"<p>Prerequisites</p> <ul> <li>Deep Learning Setup : Setup workspace and download python libraries</li> </ul> <p>Learning Objectives</p> <ol> <li>Autoencoder Background</li> <li>Building an Autoencoder</li> <li>Training and Evaluating an Autoencoder</li> <li>Variational Autoencoder (VAE) Background</li> <li>Building a VAE</li> <li>Training and Evaluating a VAE</li> </ol>"},{"location":"machine_learning/deep_learning/04_autoencoder/#autoencoders","title":"Autoencoders","text":""},{"location":"machine_learning/deep_learning/04_autoencoder/#background","title":"Background","text":"<p>Neural networks can be great at learning patterns in data. But the trade off is that the model can be too good, meaning it essentially memorizes all the training data and is not generalizable to other data - in other words the model is overfit:</p> <p>Model Overfitting</p> <p> Image Credit: H2O AI </p> <p>So enter autoencoders! Autoencoders take input from a higher dimensional space and encode it in a lower dimensional space, then decode the output of the latent space and reconstructs the data:</p> <p>Autoencoders</p> <p> </p> <p>So, why bother - how does this contribute to prevent overfitting? Well, by encoding and decoding the input data we tend to \"denoise\" our data, allowing the model to learn general patterns without memorizing our data. To assess an autoencoder we use the loss function that makes most sense for our problem. In this tutorial we are going to be predicting smoking status again (a binary variable) so we need the binary cross entropy loss function!</p> \\[ L_{BCE} = - \\frac{1}{N} \\sum_{i=1}^N [y_i log({\\hat{y}_i}) + (1-y_i)log(1- \\hat{y}_i)] \\] <ul> <li>\\(L_{BCE}\\): loss after encoding and decoding</li> <li>\\(N\\): number of observations</li> <li>\\(y_i\\): true value</li> <li>\\(\\hat{y}_i\\): predicted value after endcoding and decoding</li> </ul> <p>With a lower \\(L_{BCE}\\), the model is doing a better job of reconstructing the original data! </p>"},{"location":"machine_learning/deep_learning/04_autoencoder/#building-an-autoencoder","title":"Building an Autoencoder","text":"<p>Let's make ourselves an autoencoder then! We will pull in our glioblastoma data from the classifier example:</p> <pre><code># pandas and numpy for data manipulation and plotly for plotting\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# sklearn for model metrics, normalization and data splitting\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# torch for tensor operations and neural network functions\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\n\n# read in data and filter out na values\ngbm = pd.read_csv(\"../data/gbm_data.csv\",on_bad_lines='skip')\ngbm = gbm[gbm['SMOKING_HISTORY'].notna()]\n\n# make a new column for smoking status\ngbm['smoking_status'] = np.where(gbm['SMOKING_HISTORY'].str.contains('non-smoker'), 'non-smoker', 'smoker')\n\n# filter our data for features of interest\ngbm_filt = gbm.loc[:,['LINC00159','EFTUD1P1','C20orf202','KRT17P8','RPL7L1P9','smoking_status']]\ngbm_filt['smoking_status'] = gbm_filt['smoking_status'].astype('category').cat.codes\n</code></pre> <p>Now we will split our data into training and test datasets and normalize:</p> <pre><code># Split the data into features and outcome variable\nX = gbm_filt.drop('smoking_status', axis=1).values\ny = gbm_filt['smoking_status'].values\n\n# Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=81)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n</code></pre> <p>Great, let's make that autoencoder now!</p> <pre><code>class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super(AutoEncoder, self).__init__()\n        # encoder \n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, latent_dim)\n        )\n        # decoder \n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n\n        )\n    # define the flow through the nodes\n    def forward(self, x):\n        latent = self.encoder(x)\n        reconstructed = self.decoder(latent)\n        return reconstructed\n</code></pre> <p>Here we:</p> <ul> <li>Create an autoencoder where the encoder takes our initial inputs</li> <li>Sends to two hidden layers with 64 nodes and a ReLU activation function</li> <li>Squeezes the input to some latent dimension</li> <li>Next in the decoder, we take the input from the latent space and send it to two more layers with the original dimensions of the hidden layers - 64</li> <li>Then it outputs probabilities for binary classification!</li> <li>Finally, we define how is data going to move through our model - it is encoded by our encoder and then decoded by our decoder.</li> </ul>"},{"location":"machine_learning/deep_learning/04_autoencoder/#train-the-autoencoder","title":"Train the Autoencoder","text":"<p>Let's train our autoencoder and see how well it does!</p> <pre><code># get input dimension\ninput_dim = X_train_tensor.shape[1]\n\nmodel = AutoEncoder(input_dim=input_dim, latent_dim=2)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# convert target tensors to float32\ny_train_tensor = y_train_tensor.float()\ny_test_tensor = y_test_tensor.float()\n\n# set a list for our training and test loss values\nae_train_loss_vals = []\nae_test_loss_vals = []\n\n# training loop\nfor epoch in range(300):\n    model.train()\n\n    # what is our model output and loss\n    output = model(X_train_tensor)\n    loss = criterion(output.squeeze(), y_train_tensor)\n\n    # clear our gradients, backpropagate and \n    # perform gradient descent\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # loss for each training epoch\n    ae_train_loss_vals.append(loss.item())\n\n    # evaluate on test data\n    model.eval()\n    with torch.inference_mode():\n        recon_test = model(X_test_tensor)\n        test_loss = criterion(recon_test.squeeze(), y_test_tensor)\n        ae_test_loss_vals.append(test_loss.item())\n</code></pre> <p>You'll see our familiar flow, where we:</p> <ul> <li>call our model with the our number of input features, and the number of nodes in the latent space</li> <li>define our loss function and optimizer</li> <li>convert our outcome variables to float values</li> <li>loop through our epochs in training mode</li> <li>plug and chug our model</li> <li>get our loss</li> <li>clear our gradients</li> <li>backpropogate to update our model weights</li> <li>perform gradient descent to find the optimal weights</li> </ul> <p>Now let's see how our loss changes as the number of epochs increases!</p> <pre><code># create an empty plot\nfig = go.Figure()\n\n# add training and test loss values\nfig.add_trace(go.Scatter(x=list(range(300)), y=ae_train_loss_vals, mode='lines', name='Training Loss'))\nfig.add_trace(go.Scatter(x=list(range(300)), y=ae_test_loss_vals, mode='lines', name='Test Loss'))\n\n# update labels &amp; layout\nfig.update_layout(\n    title='Autoencoder Loss vs. Epoch',\n    xaxis_title='Epoch',\n    yaxis_title='Loss',\n    template = 'plotly_white'\n)\n\n# show the plot\nfig.show()\n</code></pre> <p>output</p> <p> </p> <p>Great! we see that as the number of epochs increases the loss decreases and that this loss seems to level out after epoch 300.</p>"},{"location":"machine_learning/deep_learning/04_autoencoder/#variational-autoencoders-vae","title":"Variational Autoencoders (VAE)","text":""},{"location":"machine_learning/deep_learning/04_autoencoder/#background_1","title":"Background","text":"<p>A recent improvement on the autoencoder is the variational autoencoder (VAE). So a normal autoencoder is returning a reconstructed value in an effort to avoid overfitting the model. A VAE will turn the latent space into a distribution instead of a vector by returning two things from the latent space:</p> <ul> <li>Mean (\\(\\mu\\)): the central point of the distribution.</li> <li>Log variance \\(log(\\sigma^2)\\): describes how spread out or \u201cwide\u201d the distribution should be around the mean.</li> </ul> <p>Now to get data points they essentially sample from a normal distribution defined by the mean and variance output from the VAE and output another data point \\(z\\): </p> <p>\\(z = \\mu + \\sigma \\odot \\epsilon\\)</p> <p>Where:</p> <ul> <li>\\(\\mu\\) is the mean</li> <li>\\(\\sigma\\) is the standard variation (which you get by converting the log variance with \\(exp(0.5\u22c5log(\\sigma^2))\\) )</li> <li>\\(\\epsilon\\) is the sample from \\(\\mathcal{N}(0,1)\\) (a normal distribution with a mean of 0 and a standard deviation of 1)</li> <li>\\(\\odot\\) is the element wise product</li> </ul> <p>What is cool about this is that we can use variational autoencoders to model the underlying distribution. However, we have an issue, we sampled from a normal distribution \\(\\mathcal{N}(0,1)\\) and our latent space has a distribution of \\(\\mathcal{N}(\\mu,\\sigma^2)\\). To make sure we can sample from \\(\\mathcal{N}(0,1)\\) we should have some way of penalizing the model if it strays too far from that normal distribution \\(\\mathcal{N}(0,1)\\). We do this using Kullback-Leibler (KL) divergence. For a binary loss that is summarized to:</p> <p>\\(L_{KL} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left( 1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2 \\right)\\)</p> How did we get to this? <ul> <li> <p>What is KL Divergence?:</p> </li> <li> <p>The KL divergence between two distributions \\(q(z|x)\\) and \\(p(z)\\) is:</p> <p>\\(L_{KL} = D_{KL}(q(z|x) \\parallel p(z)) = \\int q(z|x) \\log \\frac{q(z|x)}{p(z)} \\, dz\\)</p> </li> <li> <p>For a VAE, \\(q(z|x)\\) has a mean \\(\\mu\\) and a variance \\(\\sigma^2\\) (encoded for each input by the encoder network).</p> </li> <li> <p>Plugging in the Standard Normal Distribution:</p> </li> <li> <p>\\(p(z)\\) is a standard normal distribution \\(N(0, 1)\\), so it has the following formula:</p> <p>\\(p(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\\)</p> </li> <li> <p>\\(q(z|x)\\), has a mean \\(\\mu\\) and variance \\(\\sigma^2\\), and is a normal distribution for each input sample: \\(q(z|x) = N(\\mu, \\sigma^2)\\).</p> </li> <li> <p>Solving the KL Divergence Integral:</p> </li> <li> <p>Substituting the expressions for \\(q(z|x)\\) and \\(p(z)\\) into the KL divergence formula, we get:</p> <p>\\(L_{KL} = \\int N(\\mu, \\sigma^2) \\log \\frac{N(\\mu, \\sigma^2)}{N(0, 1)} \\, dz\\)</p> </li> <li> <p>Now if solve our integral we get our KL divergence!:</p> <p>\\(L_{KL} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left( 1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2 \\right)\\)</p> </li> </ul> <p>We can then add this loss to our reconstruction loss to get the loss of our VAE:</p> <p>\\(L_{VAE} =  L_{BCE} + \\beta L_{KL}\\)</p> <p>Where \\(\\beta\\) is a term for how much we want to penalize the latent space. Now enough math, let's make a VAE!</p>"},{"location":"machine_learning/deep_learning/04_autoencoder/#building-a-vae","title":"Building a VAE","text":"<p><pre><code>class VAEClassifier(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super(VAEClassifier, self).__init__()\n\n        # encoder: compresses input to a smaller space\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64), \n            nn.ReLU() \n        )\n\n        # latent space parameters: calculates the mean (mu) and log variance (logvar)\n        self.fc_mu = nn.Linear(64, latent_dim)  \n        self.fc_logvar = nn.Linear(64, latent_dim) \n\n        # Classifier: Outputs a probability for each binary label (0 or 1)\n        # uses a ReLU activation function and\n        # sigmoid function to get class probabilities\n        self.classifier = nn.Sequential(\n            nn.Linear(latent_dim, 64), \n            nn.ReLU(), \n            nn.Linear(64, 1), \n            nn.Sigmoid()  \n        )\n\n    # Reparameterization: Samples from the latent distribution\n    # using mean (mu) and standard deviation derived from log variance (logvar)\n    def reparameterize(self, mu, logvar):\n        # get standard deviation\n        std = torch.exp(0.5 * logvar)\n        # sample from a normal distribution\n        eps = torch.randn_like(std)\n        # output a sampled point \n        return mu + eps * std  \n\n    # Forward pass: Defines data flow through the network\n    def forward(self, x):\n        # encode the input\n        encoded = self.encoder(x)\n        # calcluate the mean and log variance of the latent space\n        mu = self.fc_mu(encoded)  \n        logvar = self.fc_logvar(encoded)\n        # sample points from latent space\n        z = self.reparameterize(mu, logvar)\n        # classify samples and return the output, mean and log variance\n        output = self.classifier(z)  \n        return output, mu, logvar  \n</code></pre> \u200b Here is what is going on:</p> <ul> <li>Encoder: Compresses the input into latent space.</li> <li>Latent Space Parameters: Calculates the mean (mu) and log variance (logvar) for the latent space.</li> <li>Classifier: Uses a linear layer, ReLU, and sigmoid to produce a probability, predicting the binary class (e.g., 0 or 1).</li> <li>Reparameterization: Samples from the latent distribution, turning logvar into standard deviation and adding random value from normal distribution \\(\\mathcal{N}(0,1)\\).</li> <li>Forward Pass: Passes data through the encoder, computes mean and log variance, and then reparameterizes to get a sample point.</li> <li>Sampling: Combines mean with a standard deviation-scaled data point to produce z, a point in the latent space.</li> <li>Binary Classification: Uses z to predict a binary outcome, outputting a probability with the sigmoid activation.</li> <li>Outputs: Returns the probability, along with mean and log variance (which we will need to calcualte the KL divergence!).</li> </ul>"},{"location":"machine_learning/deep_learning/04_autoencoder/#train-a-vae","title":"Train a VAE","text":"<p>Let's get to training our new VAE!</p> <pre><code># call our model and set the number of dimensions for the input and\n# latent space\nvae = VAEClassifier(input_dim=X_train_tensor.shape[1], latent_dim=2)\noptimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n\n# define our VAE loss using the reconstructed loss value\n# the mean and log variance\ndef vae_loss(recon_x, x, mu, logvar):\n    recon_loss = nn.BCELoss()(recon_x.squeeze(), x)\n    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_divergence\n\nvae_train_loss_vals = []\nvae_test_loss_vals = []\n\n# loop through and train the VAE\nfor epoch in range(300):\n    vae.train()\n\n    # grab out reconstruction loss, mean and log variance\n    recon, mu, logvar = vae(X_train_tensor)\n    # calcluate our loss\n    loss = vae_loss(recon, y_train_tensor, mu, logvar)\n\n    # clear our gradients, backpropagate and \n    # perform gradient descent\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # append to list\n    vae_train_loss_vals.append(loss.item())\n\n    # evaluate on test data\n    vae.eval()\n    with torch.inference_mode():\n        recon_test, mu_test, logvar_test = vae(X_test_tensor)\n        test_loss = vae_loss(recon_test, y_test_tensor, mu_test, logvar_test)\n        vae_test_loss_vals.append(test_loss.item())\n</code></pre> <p>Here we are:</p> <ul> <li>Defining our loss function and optimizer.</li> <li>Defining a function to calculate our VAE loss using our reconstructed loss value, the mean and log variance.</li> <li>Setting a training loop and calculating our loss.</li> <li>Clearing our gradients, backpropogating and running gradient descent</li> <li>Adding that loss to our list of losses</li> <li>Evaluating our model on test data and adding the test loss to our list</li> </ul> <p>Let's see how our VAE did with our test and training data!</p> <pre><code># create an empty plot\nfig = go.Figure()\n\n# add training and test loss values\nfig.add_trace(go.Scatter(x=list(range(300)), y=vae_train_loss_vals, mode='lines', name='Training Loss'))\nfig.add_trace(go.Scatter(x=list(range(300)), y=vae_test_loss_vals, mode='lines', name='Test Loss'))\n\n# update labels &amp; layout\nfig.update_layout(\n    title='VAE Loss vs. Epoch',\n    xaxis_title='Epoch',\n    yaxis_title='Loss',\n    template = 'plotly_white'\n)\n\n# show the plot\nfig.show()\n</code></pre> <p>output</p> <p> </p> <p>Key Points</p> <ul> <li>Neural networks can learn patterns but risk overfitting.</li> <li>Autoencoders solve this by compressing (encoding) input data to a smaller, lower-dimensional latent space, and then reconstructing (decoding) it back.</li> <li>Variational Autoencoders (VAEs) improve on autoencoders by modeling the latent space as a distribution (mean and log variance), better matching the underlying distribution of the data</li> <li>The KL divergence term in the VAE loss penalizes deviations from a standard normal distribution, ensuring the latent space is smooth and structured.</li> </ul>"},{"location":"machine_learning/supervised/linear-model/","title":"Introduction To Linear Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine_learning/supervised/linear-model/#linear-regression","title":"Linear Regression","text":"<p>In a regression model assesses the relationship between two quantitative variables by fitting a line to the data.  Using our glioblastoma data, we will assess the relationship between IDH1 gene expression (a gene commonly mutated in this type of cancer)  and TMB score (a measure of mutational burden). You can use a regression model to determine:</p> <ul> <li>how strong the relationship between these two variables is</li> <li>the value of the dependent variable given the independent variable</li> </ul> <p>A linear model follows the following formula:</p> \\[  y = \\beta_0 + \\beta_1 X + \\epsilon \\] <p>What do these terms mean?</p> <ul> <li>\\(y\\): dependent variable</li> <li>\\(\\beta_0\\): intercept (where \\(y\\) = 0)</li> <li>\\(\\beta_1\\): regression coefficient or slope</li> <li>\\(X\\): independent variable</li> <li>\\(\\epsilon\\): error or our estimate (what is the variation in our regression coefficient)</li> </ul> <p>This formula describes the best fit line for our data that tries to minimizes our error \\(\\epsilon\\):</p> <p></p>"},{"location":"machine_learning/supervised/linear-model/#pre-processing","title":"Pre-Processing","text":"RPython <p>Open an R script by going to <code>File</code> &gt; <code>New File</code> &gt; <code>R Script</code>. Save this script using a descriptive name, like <code>linear-model.R</code>. In the script use the following code:</p> <pre><code>## Loading our packages\n## Load the counts data\n## Load the meta data\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\n\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\") \n\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_sample.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n</code></pre> <p>Open a new notebook by going to <code>File</code> &gt; <code>New</code> &gt; <code>Notebook</code>. Save this script using a descriptive name, like <code>linear-model.ipynb</code>. In a code block enter:</p> <pre><code>## Import our libraries\n## Import our data set\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\ncounts = pd.read_csv(\n    'data/gbm_cptac_2021/data_mrna_seq_fpkm.txt' ,\n    sep = '\\t')\nmeta = pd.read_csv(\n    'data/gbm_cptac_2021/data_clinical_sample.txt' , \n    sep = '\\t',\n    skiprows=4)\n</code></pre> <p>Now we will need to do some data cleaning before we plug this into our model:</p> RPython <pre><code>## Change the patient id column to match \n## column names in the counts df\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression\nidh1 = counts %&gt;%\n  filter(Hugo_Symbol == \"IDH1\") %&gt;%\n  select(-Hugo_Symbol) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(PATIENT_ID = rownames(.))\n\ncolnames(idh1) &lt;- c(\"IDH1\",\"PATIENT_ID\")\n\n## merge meta data and IDH1 \n## gene expression\nmerged &lt;- merge(\n  meta,\n  idh1,\n  by=\"PATIENT_ID\")\n</code></pre> <pre><code>## Grab IDH1 row and transpose \n## ensure that patient id is a column\n## and that IDH1 is the column name\nIDH1= counts.loc[counts[\"Hugo_Symbol\"] == \"IDH1\",]\nIDH1 = IDH1.T\nIDH1[\"PATIENT_ID\"] = IDH1.index\nIDH1.columns = [\"IDH1\",\"PATIENT_ID\"]\n\n## Grab TMB score\n## merge IDH1 gene expression and TMB score\n## merging this way ensures data are organized\n## by patient \nTMB= meta[['PATIENT_ID','TMB_NONSYNONYMOUS']]\nmerged=pd.merge(IDH1,TMB,on=\"PATIENT_ID\")\nmerged = merged.set_index('PATIENT_ID')\nmerged.head()\n</code></pre>"},{"location":"machine_learning/supervised/linear-model/#normalizecreate-the-model","title":"Normalize/Create the Model","text":"<p>These data, IDH1 gene expression and TMB score are on two different scales. To ensure a fair comparison of these variables we will normalize (or bring our data to a common scale) our data:</p> RPython <pre><code>## create a normalization function\n## apply this function to our data \n## to get data on a similar scale\nNormalizeData &lt;- function(data){\n  normalized = (data - min(data)) / (max(data) - min(data))\n  return(normalized)\n}\n\nnorm = as.data.frame(\n  apply(merged %&gt;% select(IDH1,TMB_NONSYNONYMOUS), 2,NormalizeData)\n)\n</code></pre> <pre><code>## You might notice our data is on two \n## drastically different scales\n## We will normalize our data\n\ndef NormalizeData(data):\n    normalized = (data - np.min(data)) / (np.max(data) - np.min(data))\n    df = pd.DataFrame(normalized)\n    return normalized\n\nnorm = NormalizeData(merged)\n</code></pre> <p>Now we can fit our regression model!</p> RPython <pre><code>## fit our linear regression model\nmodel &lt;- lm(TMB_NONSYNONYMOUS ~ IDH1, data = norm)\n\n## let's plot our data\n## with the predicted values\nggplot(norm, aes(x=IDH1, y=TMB_NONSYNONYMOUS)) + \n  geom_point() +\n  theme_bw() +\n  ylab(\"TMB\") +\n  xlab(\"IDH1\") +\n  geom_smooth(method=lm)\n</code></pre> <p></p> <pre><code>## fit our linear regression model\ntmb = norm['TMB_NONSYNONYMOUS']\nidh1 = norm['IDH1'].astype(float)\nmodel = sm.OLS(tmb,idh1).fit()\n\n## Let's plot our data\n## along with the predictions from \n## our model\nplt.figure(figsize=(12, 6))\nplt.plot(norm['IDH1'], norm['TMB_NONSYNONYMOUS'], 'o') \nplt.plot(norm['IDH1'], model.predict(idh1), 'r', linewidth=2)\nplt.xlabel('IDH1')\nplt.ylabel('TMB NONSYNONYMOUS')\n\nplt.show()\n</code></pre> <p></p>"},{"location":"machine_learning/supervised/linear-model/#model-results","title":"Model Results","text":"<p>To assess our model we will generate a summary of some important metrics:</p> RPython <pre><code>summary(model)\n</code></pre> <pre><code>Call:\nlm(formula = TMB_NONSYNONYMOUS ~ IDH1, data = norm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.06145 -0.02361 -0.01212  0.00096  0.94101 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  0.06631    0.02216   2.992  0.00351 **\nIDH1        -0.03437    0.05362  -0.641  0.52305   \n---\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\nResidual standard error: 0.09938 on 97 degrees of freedom\nMultiple R-squared:  0.004218,    Adjusted R-squared:  -0.006048 \nF-statistic: 0.4109 on 1 and 97 DF,  p-value: 0.523\n</code></pre> <pre><code>model.summary()\n</code></pre> <pre><code>OLS Regression Results\nDep. Variable:  TMB_NONSYNONYMOUS   R-squared:   0.004\nModel:  OLS Adj. R-squared: -0.006\nMethod: Least Squares   F-statistic:    0.4109\nDate:   Fri, 02 Sep 2022    Prob (F-statistic): 0.523\nTime:   09:38:56    Log-Likelihood: 89.110\nNo. Observations:   99  AIC:    -174.2\nDf Residuals:   97  BIC:    -169.0\nDf Model:   1       \nCovariance Type:    nonrobust       \n            coef    std err t       P&gt;|t|   [0.025  0.975]\nIntercept   0.0663  0.022   2.992   0.004   0.022   0.110\nIDH1       -0.0344  0.054   -0.641  0.523   -0.141  0.072\nOmnibus:    202.245 Durbin-Watson:  2.072\nProb(Omnibus):  0.000   Jarque-Bera (JB):   28769.651\nSkew:   8.846   Prob(JB):   0.00\nKurtosis:   84.618  Cond. No.   6.12\n</code></pre> <p>Let's cover what a few of these mean:</p> RPython <ul> <li><code>Estimate</code> : the model's effect, so here we see that a one unit increase in IDH1 results in a 0.0344 decrease in </li> <li><code>Std. Error</code> : standard error of our estimate</li> <li><code>t value</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>Pr(&gt;|t|)</code> : pvalue that assesses the effect of IDH1 on TMB score if the null hypothesis of no effect were correct</li> <li><code>Multiple R-squared:</code> : \\(r^2\\) value of model - how much of the TMB variability is explained by IDH1 - here it is 0.4%</li> </ul> <ul> <li><code>coef</code> : the model's effect, so here a </li> <li><code>std</code> : standard error of our estimate</li> <li><code>t</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>P&gt;|t|</code> : pvalue that assesses the effect of IDH1 on TMB score if the null hypothesis of no effect were correct</li> <li><code>R-squared</code> : \\(r^2\\) value of model - how much of the TMB variability is explained by IDH1 - here it is 0.4%</li> </ul>"},{"location":"machine_learning/supervised/linear-model/#recap","title":"Recap","text":"<p>So what have we found? Well the model only explains 0.4% of our outcome variation and increased IDH1 expression seems to be associated with a minimal decrease in TMB. Additionally, our high p-value of <code>0.523</code> indicates that we should not reject the possibility that this effect was due to chance. </p>"},{"location":"machine_learning/supervised/linear-model/#assumptions","title":"Assumptions","text":"<p>So we found that IDH1 gene expression is not a great feature to model an outcome of TMB score. However, what if you do get a low p-value, a good \\(r^2\\) value, and a sizeable effect - are you in the clear? Not so fast, a linear model is limited by assumptions in our data:</p> <ul> <li>The data is normally distributed</li> <li>Homogeneity in the variances (Homoscedacity)</li> <li>Independence of your observations</li> </ul>"},{"location":"machine_learning/supervised/linear-model/#testing-assumptions","title":"Testing Assumptions","text":"<p>Mathematically, we can assess Normality and Homoscedacity with the shapiro test and Breusch-Pagan test, respectively:</p> RPython <pre><code>shapiro.test(norm$IDH1)\n</code></pre> <pre><code>Shapiro-Wilk normality test\n\ndata:  norm$IDH1\nW = 0.95137, p-value = 0.001093\n</code></pre> <pre><code>stats.shapiro(norm['IDH1'])\n</code></pre> <pre><code>(0.9513656497001648, 0.0010935224127024412)\n</code></pre> <p>We note here a pvalue (highlighted) of less than <code>0.05</code> which indicates that our data deviates from a normal distribution. Now how about our homoscedacity?</p> RPython <pre><code>lmtest::bptest(model)\n</code></pre> <pre><code>studentized Breusch-Pagan test\n\ndata:  model\nBP = 0.72081, df = 1, p-value = 0.3959\n</code></pre> <pre><code>names = ['Lagrange multiplier statistic', 'p-value',\n    'f-value', 'f p-value']\ntest = sms.het_breuschpagan(model.resid, model.model.exog)\nlzip(names,test)\n</code></pre> <pre><code>[('Lagrange multiplier statistic', 0.7208106123366453),\n('p-value', 0.39587814229097884),\n('f-value', 0.7114286333891047),\n('f p-value', 0.4010451501525186)]\n</code></pre> <p>Here we see that our pvalue is well above <code>0.05</code> indicating there is not enough evidence to reject the null hypothesis that the variance of the residuals are constant - i.e. we do have Homogeneity in our variances. </p>"},{"location":"machine_learning/supervised/linear-model/#references","title":"References","text":"<ul> <li>scribbr</li> <li>STHDA</li> <li>Medium</li> </ul>"},{"location":"machine_learning/supervised/logistic-regression/","title":"Introduction To Logistic Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine_learning/supervised/logistic-regression/#logistic-regression","title":"Logistic Regression","text":"<p>A logistic regression model attempts to classify, so for example can ALDH3A1 gene expression to predict smoking status? So how do we do this? We could fit a linear model to our data but we would end up with something like this:</p> <p></p> <p>Here we see that if we used a linear model, we'd end up predicting values that are not our two catetories - smoker or non-smoker (and on the graph 0 or 1). So how do we fit our data when it is binary? We use a sigmoid function:</p> \\[ p(X) = \\frac{ e^{\\beta_{0} + \\beta_{1}X} }{1 + e^{\\beta_{0} + \\beta_{1}X} } \\] <p>What do these terms mean?</p> <ul> <li>\\(p(X)\\) : probability of smoking status given ALDH3A1 gene expression</li> <li>\\(X\\) : ALDH3A1 gene expression</li> <li>\\(\\beta_{0}\\) : y intercept</li> <li>\\(\\beta_{1}\\) : slope of our line</li> </ul> <p>This sigmoid function creates our S-shaped curve! However we'd like our probability to be linear relationship with X. So we can manipulate this equation to get:</p> \\[ \\frac{p(X)}{1 - p(X)} = e^{\\beta_{0} + \\beta_{1}X}\\] <p>Where \\(\\frac{p(X)}{1 - p(X)}\\) is known as the odds ratio and this can range from \\(0\\) to \\(\\infty\\). However, again this doesn't match our 0 to 1 scale, so we take the log of both sides to get:</p> \\[ log(\\frac{p(X)}{1 - p(X)}) = \\beta_{0} + \\beta_{1}X \\] <p>Here we get \\(log(\\frac{p(X)}{1 - p(X)})\\) or the logit function - where a one unity increase in \\(X\\) increases \\(p(X)\\) by \\(\\beta_{0}\\). </p>"},{"location":"machine_learning/supervised/logistic-regression/#pre-processing","title":"Pre-processing","text":"<p>First let's do some preprocessing to get our combine ALDH3A1 gene expression with smoking status:</p> RPython <pre><code>## load our libraries via our library path\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggplot2)\n\n## load our counts and meta data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\") \n\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n\n## ensure our patient ID's match between \n## the counts and meta data\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression and \n## patient ID \naldh3a1 = counts %&gt;%\n  filter(Hugo_Symbol == \"ALDH3A1\") %&gt;%\n  select(-Hugo_Symbol) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(PATIENT_ID = rownames(.))\n\ncolnames(aldh3a1) &lt;- c(\"aldh3a1\",\"PATIENT_ID\")\n\n## merge counts and meta data\nmerged &lt;- merge(\n  meta,\n  aldh3a1,\n  by=\"PATIENT_ID\")\n\n## create smoking status variable\n## and normalize ALDH3A1 expression\nmerged &lt;- merged %&gt;%\n  mutate(smoking = ifelse(grepl(\"non-smoker\",SMOKING_HISTORY),0,1)) %&gt;%\n  mutate(aldh3a1 = log2(aldh3a1+1))\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine_learning/supervised/logistic-regression/#visualizing-data","title":"Visualizing Data","text":"<p>Now let's take a look at our data:</p> RPython <pre><code>## let's plot our data\nggplot(merged, aes(x=aldh3a1, y=smoking)) + \n  geom_point() +\n  theme_bw() +\n  ylab(\"Smoking Status\") +\n  xlab(\"ALDH3A1 Gene Expression\") \n</code></pre> <p></p> <pre><code>\n</code></pre> What problem do you see with creating a logistic regression model with this data? <p>There is no clear separation of ALDH3A1 gene expression between smokers and non-smokers.</p>"},{"location":"machine_learning/supervised/logistic-regression/#create-the-model","title":"Create the Model","text":"<p>We see that there isn't a great delineation between the two conditions - smoking and non-smoking. But let's creat our logistic regression model to confirm:</p> RPython <pre><code>## let's make our model\nmodel &lt;- glm( smoking ~ aldh3a1, data = merged, family = binomial)\nsummary(model) \n</code></pre> <pre><code>Call:\nglm(formula = smoking ~ aldh3a1, family = binomial, data = merged)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0299  -0.9776  -0.9364   1.3867   1.4800  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.15978    1.61680   0.099    0.921\naldh3a1     -0.05947    0.13897  -0.428    0.669\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 127.95  on 96  degrees of freedom\nResidual deviance: 127.77  on 95  degrees of freedom\n  (2 observations deleted due to missingness)\nAIC: 131.77\n\nNumber of Fisher Scoring iterations: 4\n</code></pre> <p>Here we note:</p> <ul> <li><code>Estimate</code> : the model's effect, so here we see that a one unit increase in ALDH3A1 results in a 0.05947 decrease in the probability of being a smoker</li> <li><code>Std. Error</code> : standard error of our estimate</li> <li><code>z value</code> : test stastic - the larger the statistic, the less likely this effect occured by chance</li> <li><code>Pr(&gt;|z|)</code> : pvalue that assesses the effect of ALDH3A1 on Smoking status if the null hypothesis of no effect were correct</li> </ul> <p>You will note that there is not an \\(r^2\\) value for this model like we saw in the linear regression tutorial. We will need a special metric known as McFadden's \\(r^2\\):</p> <pre><code>pscl::pR2(model)[\"McFadden\"]\n</code></pre> <pre><code>fitting null model for pseudo-r2\n   McFadden \n0.001440711 \n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine_learning/supervised/logistic-regression/#visualizing-predictions","title":"Visualizing Predictions","text":"<p>We will now plot our logistic regression curve:</p> RPython <pre><code>## let's visualize our predictions\nggplot(merged, aes(x=aldh3a1, y=smoking)) + \n  geom_point() +\n  theme_bw() +\n  ylab(\"Smoking Status\") +\n  xlab(\"ALDH3A1 Gene Expression\") +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) \n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"machine_learning/supervised/logistic-regression/#recap","title":"Recap","text":"<p>So what have we found? Well it seems that ALDH3A1 expression has a minimal effect on the probability of being a smoker. Additionally, our high p-value of 0.669 indicates that we should not reject the possibility that this effect was due to chance. This is also confirmed in our visual of the logistic regression curve - where it is not S shaped. This is due to the absense of some boundry, where some value of ALDH3A1 expression separates smokers and non smokers. We can also see this fit isn't great in McFadden's \\(r^2\\) value of <code>0.001440711</code>. McFadden's \\(r^2\\) values close to 0 are indicative of a poor fit while values over 0.4 are indicative of a good fit. </p>"},{"location":"machine_learning/supervised/logistic-regression/#assumptions","title":"Assumptions","text":"<p>When we create a logistic regression model we need to be aware of the limitations:</p> <ul> <li>the response variable must be binary </li> <li>the observations are independent (i.e. observations can't influence the response of other observations)</li> <li>no multicollinearity (will be covered the multivariate regression tutorial) - if you have multiple dependent variables they cannot be correlated</li> <li>no outliers</li> <li>there must be a linear relationship between the logit function and the dependent variable </li> </ul> <p>While we will cover multicollinearity in the multivariate regression tutorial, we will assess here whether or not there is a linear relationship between the logit function and the dependent variable using the Box-Tidwell test:</p> RPython <pre><code>## run the box tidwell test on\n## our model\ncar::boxTidwell(model$linear.predictors ~ merged$aldh3a1[!is.na(merged$aldh3a1)])\n</code></pre> <pre><code> MLE of lambda Score Statistic (z) Pr(&gt;|z|)\n         1              0.7139     0.4753\n\niterations =  0 \n</code></pre> <pre><code>\n</code></pre> <p>Here we note that the probability is above 0.05, implying that ALDH3A1 expression and the logit function are linearly related. </p>"},{"location":"machine_learning/supervised/logistic-regression/#references","title":"References","text":"<ul> <li>datacamp</li> <li>STHDA</li> <li>Statology</li> </ul>"},{"location":"machine_learning/supervised/model-performance/","title":"Introduction To Model Performance","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine_learning/supervised/model-performance/#training-and-testing-data","title":"Training and Testing Data","text":""},{"location":"machine_learning/supervised/model-performance/#model-error","title":"Model Error","text":""},{"location":"machine_learning/supervised/model-performance/#sensitivity-v-specificity","title":"Sensitivity v. Specificity","text":""},{"location":"machine_learning/supervised/model-performance/#area-under-the-curve","title":"Area Under The Curve","text":""},{"location":"machine_learning/supervised/model-performance/#references","title":"References","text":""},{"location":"machine_learning/supervised/multivariate-regression/","title":"Introduction To Multivariate Regression","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN</li> <li>Please be sure to have followed the instructions on the setup page</li> </ul>"},{"location":"machine_learning/supervised/multivariate-regression/#multivariate-regression","title":"Multivariate Regression","text":"<p>In the linear regression and logistic regression tutorials we cover univariate modelling -  or modelling with one variable. Here we discuss how to create multivariate models, or models with multiple independent variables. </p>"},{"location":"machine_learning/supervised/multivariate-regression/#pre-processing","title":"Pre-Processing","text":"RPython <pre><code>## load our libraries via our library path\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggplot2)\n\n## load our counts and meta data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\") \n\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n\n## ensure our patient ID's match between \n## the counts and meta data\nmeta$PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)\n\n## grab IDH1 gene expression and \n## patient ID \naldh3a1 = counts %&gt;%\n  filter(Hugo_Symbol == \"ALDH3A1\") %&gt;%\n  select(-Hugo_Symbol) %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(PATIENT_ID = rownames(.))\n\ncolnames(aldh3a1) &lt;- c(\"aldh3a1\",\"PATIENT_ID\")\n\n## merge counts and meta data\nmerged &lt;- merge(\n  meta,\n  aldh3a1,\n  by=\"PATIENT_ID\")\n\n## create smoking status variable\n## and normalize ALDH3A1 expression\nmerged &lt;- merged %&gt;%\n  mutate(smoking = ifelse(grepl(\"non-smoker\",SMOKING_HISTORY),0,1)) %&gt;%\n  mutate(aldh3a1 = log2(aldh3a1+1))\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine_learning/supervised/multivariate-regression/#build-a-multivariate-model","title":"Build a Multivariate Model","text":"RPython <pre><code>## build several multivariate models\nmodel1 &lt;- glm( smoking ~ AGE, data = merged, family = binomial)\nmodel2 &lt;- glm( smoking ~  AGE + WEIGHT , data = merged, family = binomial)\nmodel3 &lt;- glm( smoking ~  AGE + WEIGHT + BMI , data = merged, family = binomial)\nAIC(model1,\n    model2,\n    model3)\nBIC(model1,\n    model2,\n    model3)\n</code></pre> <pre><code>       df      AIC\nmodel1  2 130.1894\nmodel2  3 121.4100\nmodel3  4 123.2545\n\n       df      BIC\nmodel1  2 135.3797\nmodel2  3 129.1953\nmodel3  4 133.6350\n</code></pre> <pre><code>\n</code></pre>"},{"location":"machine_learning/supervised/multivariate-regression/#aic-bic","title":"AIC &amp; BIC","text":"<p>Above, we created several logistic regression models with different numbers of variables (separated by a <code>+</code> sign). We also display the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) values for the different models. So what are these? These are values are a way of summarizing our models and seeing at what point adding too many variables interferes rather that helps. The lower the AIC/BIC, the better the model. Above we note that model 2 has the lowest AIC/BIC and when we add the third term, <code>BMI</code>, we raise our criterion values. </p>"},{"location":"machine_learning/supervised/multivariate-regression/#overfitting","title":"Overfitting","text":"<p>So why might adding more variables hurt our model if it can help improve our model metrics? Well, while adding more variables can help improve our model's metrics, it doesn't necessarily mean it can help improve our model's ability to predict. Here is a visual:</p> <p></p> <p>Here we see that when a model is under fit, it doesn't necessary follow the direction of the data. On the other hand, if we are too good at predicting our data set, we trade off with the ability to predict new data. </p>"},{"location":"machine_learning/supervised/multivariate-regression/#multicollinearity","title":"Multicollinearity","text":"<p>Additionally, we need to think carefully about the variables we are plugging into our model. Each variable should add new information and should not correlate with one another. Let's try to determine this visually:</p> RPython <pre><code>## grab our correlations\n## plot these correlations\ncors &lt;- cor(merged %&gt;% select(AGE,WEIGHT,BMI))\ncorrplot::corrplot(cors)\n</code></pre> <p></p> <pre><code>\n</code></pre> <p>We can also use the variance inflation factor (VIF) to assess multicollinearity - with values between 5 and 10 indicating multicollinearity:</p> RPython <pre><code>## use the vif function to\n## assess multicollinearity\ncar::vif(model2)\ncar::vif(model3)\n</code></pre> <pre><code>     AGE   WEIGHT \n1.106764 1.106764 \n     AGE   WEIGHT      BMI \n1.108918 3.265643 3.123681 \n</code></pre> <pre><code>\n</code></pre> <p>Here we see that while the VIF values of <code>WEIGHT</code> and <code>BMI</code> do not go over 5, we see in the correlation plot that these variables are indeed highly correlated. Also, when <code>BMI</code> is added to <code>model3</code> we see that the VIF values for <code>WEIGHT</code> and <code>BMI</code> are bumped up closer to 5. </p> Why do you think that BMI has a lower VIF than WEIGHT? <p>Weight is used to calculate BMI. However, BMI also captures information about height which isn't captured by our WEIGHT variable.</p>"},{"location":"machine_learning/supervised/multivariate-regression/#references","title":"References","text":"<ul> <li>STHDA</li> <li>Towards Data Science - overfitting</li> <li>Towards Data Science - logistic</li> </ul>"},{"location":"machine_learning/supervised/surv-part1/","title":"Survival Analysis Part 1","text":""},{"location":"machine_learning/supervised/surv-part1/#survival-analysis-part-1","title":"Survival Analysis Part 1","text":"<p>Survival Analysis refers to a statistical framework to assess the time it takes for some event of interest to occur. In medical research this framework is often used to ask questions about:</p> <ul> <li>a patient's probability of survival given some time period</li> <li>the characteristics that might impact survival</li> <li>the differences in survival between groups</li> </ul>"},{"location":"machine_learning/supervised/surv-part1/#events-and-censoring","title":"Events and Censoring","text":"<p>The type of event can include:</p> <ul> <li>Relapse</li> <li>Death </li> <li>Progression</li> </ul> <p>Now not all patients in a study will experience the same type of event described above and as such these observations need to be censored as to not  influence our results. Censoring can be caused by:</p> <ul> <li>a patient not yet experiencing an event of interest (Relapse, Death, Progression, etc.)</li> <li>a patient being lost to follow up during the study (meaning they drop out for one reason or another)</li> <li>a patient expreiences an alternative event to the event of interest</li> </ul> <p>Given that these are assessed at the end of the study, this is referred to as right censoring.</p>"},{"location":"machine_learning/supervised/surv-part1/#kaplan-meier-survival-probability","title":"Kaplan-Meier Survival Probability","text":"<p>The probability that the event of interest will not happen in a given time period is referred to as the survival probability. We can calculate this using the non-parametric Kaplan-Meier method:</p> \\[S(t_i) = S({t_i} - 1) (1 - \\frac{d_i}{n_i})\\] \\[S(0) = 1\\] \\[t_0 = 0\\] <p>Explanation of Terms</p> <ul> <li>\\(S(t_i)\\) Survival probability at time \\(t_i\\)</li> <li>\\(S({t_i} - 1)\\) Survival probability at time \\({t_i} - 1\\)</li> <li>\\(n_i\\) number of patients that have not experienced event of interest right before time \\(t_i\\)</li> <li>\\(d_i\\) number of events of interest at \\(t_i\\)</li> <li>\\(t_0 = 0\\) your starting time must be 0</li> <li>\\(S(0) = 1\\) your probability of survival at time 0 is 1 </li> </ul>"},{"location":"machine_learning/supervised/surv-part1/#pre-processing","title":"Pre-Processing","text":"<p>Let's start by loading our meta data and ensuring that we have a censored column:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\n# load in patient meta data\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n\n# create a censored status column\n# 1 being they are censored\n# 2 being they are not censored\nmeta$status &lt;- ifelse(\n  (meta$LOST_TO_FOLLOW_UP == \"Yes\" &amp; meta$VITAL_STATUS == \"Living\"),\n  1,\n  2\n)\n</code></pre>"},{"location":"machine_learning/supervised/surv-part1/#fitting-a-survival-curve","title":"Fitting a Survival Curve","text":"<p>To create our survival curve we will fit a model to determine: What is the survival probability of male and female patients and how do these curves compare?</p> <pre><code># fit our survival curve to our data\n# time is days\n# status is our censor status\n# and SEX is our variable of interest\nfit &lt;- survfit(\n  Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX,\n  data = meta)\n\n# examine our fit\nsummary(fit)$table\n</code></pre> <pre><code>           records n.max n.start events   *rmean *se(rmean) median 0.95LCL 0.95UCL\nSEX=Female      27    27      27     27 308.0741   47.10662    274     147     401\nSEX=Male        35    35      35     35 399.2286   39.34208    381     294     511\n</code></pre> <p>What does this table tell us?</p> <ul> <li>First we start off with 27 females and 35 males</li> <li>We then see that we observe the event (death) 27 times in females and 35 times in males</li> <li>the mean survival time in days and it's standard error (<code>*rmean</code> and <code>*se(rmean)</code>)</li> <li>the median survival time in days (<code>median</code>)</li> <li>the confidence interval around our parameter</li> </ul>"},{"location":"machine_learning/supervised/surv-part1/#visualizing-survival-curves","title":"Visualizing Survival Curves","text":"<p>Now if we want to visualize these survival curves we can use the following code:</p> <pre><code># plot the survival curves\nggsurvplot(fit,\n           pval = TRUE, # add in a p-value\n           conf.int = TRUE, # add in a confidence interval\n           risk.table = TRUE, # add in a risk table to our plot\n           risk.table.col = \"strata\", # color the risk table by group\n           linetype = \"strata\", # ensure lines are made per group\n           surv.median.line = \"hv\", # add in median survival time line\n           ggtheme = theme_bw(), # set theme to black and white\n           palette = c(\"#C06C84\",\"#355C7D\")) # change colors of lines\n</code></pre> <p></p> <p>What does this graph tell us?</p> <ul> <li>Here we see time against survival probability</li> <li>We also note a risk table down below that lists the number of individuals at risk for the event</li> <li>Given a p-value above 0.05 we do not have enough evidence to rule out the possibility that any difference between the male and female survival curve is due to chance. </li> </ul>"},{"location":"machine_learning/supervised/surv-part1/#log-rank-test","title":"Log-Rank Test","text":"<p>We can see above that we asked is there any significant difference between these two curves. The test conducted to determine this is called the log rank-test. This test is non-parametric, meaning it does not make any assumptions about the distribution of survival times. If we were solely interested in testing the difference between these two curves we could use the following code:</p> <pre><code>survdiff(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX, data = meta)\n</code></pre> <pre><code>Call:\nsurvdiff(formula = Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX, \n    data = meta)\n\nn=62, 37 observations deleted due to missingness.\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nSEX=Female 27       27     22.6     0.876      1.41\nSEX=Male   35       35     39.4     0.501      1.41\n\n Chisq= 1.4  on 1 degrees of freedom, p= 0.2 \n</code></pre> <p>What do these results mean?</p> <ul> <li>Here we see that we started with 99 observations but ended up with 37 after removing our censored data points</li> <li>Our chi-square test statistic was 1.4</li> <li>Our p-value is 0.2, again indicating we do not have enough evidence to rule out the possibility that any difference between the male and female survival curve is due to chance. </li> </ul>"},{"location":"machine_learning/supervised/surv-part1/#references","title":"References","text":"<ol> <li>Survival Analysis Basics</li> <li>Survival Analysis Part I: Basic concepts and first analyses</li> <li>Nonparametric Estimation from Incomplete Observations</li> <li>Survival plots of time-to-event outcomes in clinical trials: good practice and pitfalls</li> </ol>"},{"location":"machine_learning/supervised/surv-part2/","title":"Survival Analysis Part 2","text":""},{"location":"machine_learning/supervised/surv-part2/#survival-analysis-part-2","title":"Survival Analysis Part 2","text":"<p>In the first Survival Analysis topic note we assessed survival probability curves and if two different survival curves were significantly different. Instead of survival probability, here we will discuss hazard probability, or the probability that an individual has an event at some time \\(t\\). Survival probability on the other hand, refers to the opposite - that the individual will survive to time \\(t\\). To define the hazard probability we will use the following function:</p> \\[h(t)=h_0(t)\u00d7exp(b_{1}x_{1}+b_{2}x_{2}+...+b_{n}x_{n})\\] <p>Explanation of Terms</p> <ul> <li>\\(t\\) the survival time</li> <li>\\(h(t)\\) the hazard probability</li> <li>\\(n\\) different number of covariates \\(x\\) (so variables like age, sex, etc.)</li> <li>\\(b\\) are the coefficients of the impact of each of these covariates </li> <li>\\(h_0(t)\\) would be the baseline hazard</li> </ul> <p>These \\(b\\) coefficients can also be referred to as hazard ratios. And they can be interpreted like so:</p> <p>Hazard Ratio Interpretation</p> <ul> <li>Hazard Ratio &lt; 1: there is a decrease in the hazard</li> <li>HHazard RatioR &gt; 1: there is a increase in the hazard</li> <li>Hazard Ratio = 1: there is no effect on the hazard</li> </ul>"},{"location":"machine_learning/supervised/surv-part2/#pre-processing","title":"Pre-Processing","text":"<p>We will start as we did in the first Survival Analysis topic note and load our libraries, data, and add a censor status column:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\n# load in patient meta data\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n\n# create a censored status column\n# 1 being they are censored\n# 2 being they are not censored\nmeta$status &lt;- ifelse(\n  (meta$LOST_TO_FOLLOW_UP == \"Yes\" &amp; meta$VITAL_STATUS == \"Living\"),\n  1,\n  2\n)\n</code></pre>"},{"location":"machine_learning/supervised/surv-part2/#cox-proportional-hazards-model","title":"Cox Proportional-Hazards Model","text":"<pre><code># run the Cox Proportional-Hazards Model on our data \n# with a few covariates and get a summary\ncox &lt;- coxph(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX + AGE + BMI,\n             data = meta)\nsummary(cox)\n</code></pre> <pre><code>Call:\ncoxph(formula = Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ SEX + \n    AGE + BMI, data = meta)\n\n  n= 62, number of events= 62 \n   (37 observations deleted due to missingness)\n\n             coef exp(coef)  se(coef)      z Pr(&gt;|z|)\nSEXMale -0.300913  0.740143  0.266021 -1.131    0.258\nAGE     -0.003842  0.996166  0.011765 -0.327    0.744\nBMI     -0.006180  0.993839  0.025223 -0.245    0.806\n\n        exp(coef) exp(-coef) lower .95 upper .95\nSEXMale    0.7401      1.351    0.4394     1.247\nAGE        0.9962      1.004    0.9735     1.019\nBMI        0.9938      1.006    0.9459     1.044\n\nConcordance= 0.564  (se = 0.041 )\nLikelihood ratio test= 1.52  on 3 df,   p=0.7\nWald test            = 1.55  on 3 df,   p=0.7\nScore (logrank) test = 1.56  on 3 df,   p=0.7\n</code></pre> <p>What Does this mean?</p> <ul> <li>Here we note that all the coefficients for SEX(being male), AGE, and BMI have slight negative coefficients or hazard ratios</li> <li>We also note that no p-value is below 0.05 indicating that any observed effect on our hazard (death) could be due to chance</li> <li>We also note that sex has the most significant p-value and has the largest magnitude of all the coefficients</li> </ul>"},{"location":"machine_learning/supervised/surv-part2/#assumptions","title":"Assumptions","text":"<p>Before accepting the results of this model we should discuss the assumptions of the Cox Proportional-Hazards Model</p> <p>Assumptions of the Cox Proportional-Hazards Model</p> <ul> <li>the hazard curves different groups should be proportional and not cross each other</li> <li>There are no outliers in our data</li> <li>The relationship between the log hazard and the covariates must be linear</li> </ul> <p>Testing Proportionality in Hazards</p> <pre><code># test proportional hazards assumption\ncox_test &lt;- cox.zph(cox)\nggcoxzph(cox_test)\n</code></pre> <p></p> <p>What does this mean?</p> <ul> <li>Here we see that the global test is above 0.05, indicating we have not violated the proportional hazards assumption</li> </ul> <p>Testing for Outliers</p> <pre><code># test the outlier assumption\nggcoxdiagnostics(cox,\n                 type = \"dfbeta\",\n                 linear.predictions = FALSE, \n                 ggtheme = theme_bw())\n</code></pre> <p></p> <p>What does this mean?</p> <ul> <li>Here we see that there are a few outliers as the patterns of the residuals, especially for BMI, are not spread equally around 0. </li> </ul> <p>Testing for Linearity</p> <p>NEEDS MORE EXPLANATION HERE</p> <pre><code># test for non-linearity for AGE\nggcoxfunctional(Surv(PATH_DIAG_TO_DEATH_DAYS, status) ~ AGE + log(AGE) + sqrt(AGE),\n                data = meta)\n</code></pre> <p></p>"},{"location":"machine_learning/supervised/surv-part2/#references","title":"References","text":"<ol> <li>Cox Proportional-Hazards Model</li> <li>Regression Models and Life-Tables</li> <li>Survival Analysis Part II: Multivariate data analysis \u2013 an introduction to concepts and methods</li> </ol>"},{"location":"machine_learning/unsupervised/clustering/","title":"Distance Metrics","text":""},{"location":"machine_learning/unsupervised/clustering/#clustering","title":"Clustering","text":"<p>Clustering, like the name implies, is a way to group data points to find patterns. Clustering is type of unsupervised learning - where patterns are discovered without labeled data. </p>"},{"location":"machine_learning/unsupervised/clustering/#distance-metrics","title":"Distance Metrics","text":"<p>Before generating clusters we need to calculate the distance between observations:</p> <p></p> <p>To do so we have a few different options for distance metrics:</p> <p>Euclidean Distance</p> \\[d_{euc}(x,y) = \\sqrt{\\sum_{i=1}^n{(x_i - y_i)^2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Manhattan Distance</p> \\[d_{man}(x,y) = \\sum_{i=1}^n{|(x_i - y_i)|}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Eisen Cosine Correlation Distance</p> \\[d_{eis}(x,y) = 1 - \\frac{|\\sum_{i=1}^n{x_iy_i}|}{\\sqrt{\\sum_{i=1}^n {x_i^2}\\sum_{i=1}^n {y_i^2}}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Pearson Correlation Distance </p> \\[d_{pearson}(x,y) = 1 - \\frac{\\sum_{i=1}^n{(x - \\mu_x)(y - \\mu_y)}}{\\sqrt{\\sum_{i=1}^n{(x - \\mu_x)^2} \\sum_{i=1}^n{(y - \\mu_y)^2}}} \\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(\\mu_x\\) mean of variable x</li> <li>\\(\\mu_y\\) mean of variable y</li> <li>\\(n\\) number of observations</li> </ul> <p>Spearman Correlation Distance </p> \\[d_{spearman}(x,y) = 1 - \\frac{\\sum{(x\\prime - \\mu_{x\\prime} )(y\\prime  - \\mu_{y\\prime} )}}{\\sqrt{\\sum{(x\\prime  - \\mu_{x\\prime} )^2} \\sum{(y\\prime  - \\mu_{y\\prime} )^2}}}\\] <p>Explanation of Terms</p> <ul> <li>\\(x\\) variable x</li> <li>\\(y\\) variable y</li> <li>\\(\\mu_x\\) mean of variable x</li> <li>\\(\\mu_y\\) mean of variable y</li> <li>\\(n\\) number of observations</li> </ul>"},{"location":"machine_learning/unsupervised/clustering/#distance-metrics-in-r","title":"Distance Metrics In R","text":"<p>Let's try creating a distance matrix!</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\n  select(-c(Hugo_Symbol)) \n\n# log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\n  theme(axis.text = element_text(size = 3)) +\n  labs(\n    title = \"Pearson Correlation Distances Between Samples\",\n    fill = \"Pearson Correlation\"\n  )\n</code></pre> <p></p>"},{"location":"machine_learning/unsupervised/clustering/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> <li>Agglomerative Hierarchical Clustering</li> <li>Distance Method Formulas</li> </ol>"},{"location":"machine_learning/unsupervised/dimension-reduction/","title":"Introduction To Dimension Reduction","text":"<p>Prerequisites</p> <ul> <li>Tutorial Setup - Create R project, setup workspace, and download data</li> </ul> <ul> <li>Process of reducing the number of variables to a set of principal values where variation in your data becomes apparent. Here is an example with three dimensions:</li> </ul> <p> </p> Dimension Reduction Example <ul> <li>Here we see that most of the variation is visible along the x-y axes</li> <li> <p>So what are the advantages:</p> <ul> <li>simplification</li> <li>denoising</li> <li>variable selection</li> <li>visualization</li> </ul> </li> </ul>"},{"location":"machine_learning/unsupervised/dimension-reduction/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<ul> <li>PCA works by summarizing a set of continuous (quantitative) multivariate (multiple variable) data into a set of linearly uncorrelated variables called principal components.</li> </ul>"},{"location":"machine_learning/unsupervised/dimension-reduction/#pros","title":"Pros","text":"<ul> <li>can be used on large data</li> <li>can be used with sparse data</li> <li>preserves the structure (reproducible)</li> </ul>"},{"location":"machine_learning/unsupervised/dimension-reduction/#cons","title":"Cons","text":"<ul> <li>if one variable is on a different scale (like kg instead of g) it can bias the results. So ensure data is on one scale!</li> <li>points can be crowded with large numbers of observations and reveal no pattern</li> <li>susceptible to outliers</li> </ul>"},{"location":"machine_learning/unsupervised/dimension-reduction/#pre-processing","title":"Pre-Processing","text":"<p>Let's try this in code! First we will need to do some preprocessing:</p> RPython <pre><code>## load our libraries\nlibrary(tidyverse)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(ggplot2)\nlibrary(missMDA)\nlibrary(patchwork)\n\n## load counts/meta data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\")\n\nmeta &lt;- read.csv(\n  file = \"data/gbm_cptac_2021/data_clinical_patient.txt\",\n  skip=4,\n  header = T,\n  sep = \"\\t\"\n)\n\n## ensure patient IDs match \n## patient IDs in counts data\nmeta &lt;- meta %&gt;%\n  mutate(PATIENT_ID = gsub(\"-\",\".\",meta$PATIENT_ID)) %&gt;%\n  column_to_rownames(\"PATIENT_ID\")\n</code></pre> <pre><code># still in development - sorry!\n</code></pre>"},{"location":"machine_learning/unsupervised/dimension-reduction/#normalization","title":"Normalization","text":"<p>Now we will ensure our data are on a common scale by log2 transforming it. This will ensure that we don't bias our PCA in the direction of higher magnitude variables. We will also select the top 50 genes with the highest variance as usually high variance genes are more biologically interesting. </p> RPython <pre><code>## log2 normalize our data\nnorm = log2(counts %&gt;% select(-Hugo_Symbol)+1)\n\n## extract variances\nvars = apply(\n  counts %&gt;% select(-Hugo_Symbol),\n  1, \n  function(x){return(var(x,na.rm = T))})\n\n## select the genes with the\n## top 50 variances\nselected &lt;- norm %&gt;%\n  filter(rank(-vars)&lt;=50) %&gt;%\n  mutate(gene = counts$Hugo_Symbol[rank(-vars)&lt;=50]) %&gt;%\n  column_to_rownames(\"gene\") %&gt;%\n  t() %&gt;%\n  merge(.,meta,by=\"row.names\",all=TRUE) %&gt;%\n  column_to_rownames(\"Row.names\")\n</code></pre> <pre><code># still in development - sorry!\n</code></pre>"},{"location":"machine_learning/unsupervised/dimension-reduction/#pca-plot-interpretation","title":"PCA Plot Interpretation","text":"RPython <pre><code>## run PCA and extract eigenvalues\npca &lt;- PCA(selected[,1:50],graph = FALSE)\nhead(get_eig(pca))\n\n## visualize our eigenvalues/principal components\nfviz_screeplot(pca,addlabels = TRUE)\n</code></pre> <pre><code>      eigenvalue variance.percent cumulative.variance.percent\nDim.1   9.395012        18.790023                    18.79002\nDim.2   6.177962        12.355925                    31.14595\nDim.3   3.924839         7.849677                    38.99563\nDim.4   3.734111         7.468223                    46.46385\nDim.5   2.917162         5.834324                    52.29817\nDim.6   2.352690         4.705380                    57.00355\n</code></pre> <pre><code># still in development - sorry!\n</code></pre> <p>Here we display our our principal components (<code>Dim.1</code>,<code>Dim.2</code>, etc.) and their eigenvalues - or amount of variation that this principal component captures. For example, the first principal component has an eigenvalue of ~ 9.395 and captures about 18.8% of the variance in the data. We can also visualize which variables are manipulating our data the most and our samples themselves:</p> RPython <pre><code>## which variables are contributing\n## to principal components\nfviz_contrib(pca, choice = \"var\", axes = 1, top = 10) |\n  fviz_contrib(pca, choice = \"var\", axes = 2, top = 10) \n\n## let's visualize our samples in \n## principal component space\nfviz_pca_ind(pca,\n             label = \"none\", # hide individual labels\n             habillage = as.factor(selected$SEX), # color by groups\n             addEllipses = FALSE # Concentration ellipses\n)\n</code></pre> <p></p> <p></p> <pre><code># still in development - sorry!\n</code></pre> <p>Here we note that the gene TIMP1, contributes the most to variance of principal component 1. Interestingly, this gene has been implicated in immune infiltration in glioblastoma. We also can see in the variance contribution plot for the second principal component that MT-ATP6 contributes the most to the variance of this dimension. In the plot below the variable contribution plot, we visualize our samples along the first two principal components and color by sex. Here we do not see a discernable pattern - but this kind of coloring is useful say if you want to ensure two conditions are distributed the way you'd expect.</p> <p>Example</p> <p>An example of using PCA to see if conditions are distributed as expected could be a case-control study. Do your case patients cluster together and do your control patients cluster together.</p>"},{"location":"machine_learning/unsupervised/dimension-reduction/#references","title":"References","text":"<ul> <li>RPubs</li> <li>STHDA</li> </ul>"},{"location":"machine_learning/unsupervised/hierarchical/","title":"Hierarchical Clustering","text":""},{"location":"machine_learning/unsupervised/hierarchical/#agglomerative-hierarchical-clustering","title":"Agglomerative Hierarchical Clustering","text":"<p>Agglomerative Hierarchical Clustering is a bottom up approach wherein observations are their own cluster and then merged into larger and larger clusters until their is one root cluster:</p> <p></p> <p>This \"hierarchical\" view of these clusters is called a dendrogram. Here we will discuss Ward's Method for merging these clusters as it is one of the most popular:</p> \\[D_{12} = \\frac{||\\overline{x_1} - \\overline{x_2}||^2}{\\frac{1}{N_1}+\\frac{1}{N_2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(D_{12}\\) distance between clusters 1 and 2</li> <li>\\(N_1\\) number of points in cluster 1</li> <li>\\(N_2\\) number of points in cluster 2</li> <li>\\(\\overline{x_1}\\) mean of cluster 1</li> <li>\\(\\overline{x_2}\\) mean of cluster 2</li> </ul>"},{"location":"machine_learning/unsupervised/hierarchical/#pre-processing","title":"Pre-Processing","text":"<p>Before we apply k-means we will need to create our distance matrix:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\n  select(-c(Hugo_Symbol)) \n\n# log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\n  theme(axis.text = element_text(size = 3)) +\n  labs(\n    title = \"Pearson Correlation Distances Between Samples\",\n    fill = \"Pearson Correlation\"\n  )\n</code></pre> <p></p>"},{"location":"machine_learning/unsupervised/hierarchical/#clustering-with-wards-method","title":"Clustering with Ward's method","text":"<p>Let's apply this in R!</p> <pre><code># apply ward's clustering\nhc &lt;- hclust(d = dist, method = \"ward.D2\")\n\n# visualizing the dendrogram\n# and color by k number of clusters\nfviz_dend(hc,\n          k = 4, \n          k_colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\"))\n</code></pre> <p></p> <p>Info</p> <ul> <li>here we see each sample starts as its own cluster and is gradually merged into larger clusters</li> <li>we choose to visualize 4 clusters by this is really up to your discretion</li> </ul>"},{"location":"machine_learning/unsupervised/hierarchical/#hierarchical-clustering-shortcomings","title":"Hierarchical Clustering Shortcomings","text":"<p>Hierarchical clustering does come with a few issues:</p> <p>Hierarchical Clustering Shortcomings</p> <ul> <li>Hierarchical clustering is computationally expensive and is much slower than the k-means algorithm</li> <li>While this method is less sensitive to the shape of the data, given it starts generating clusters from individual data points; The dendrogram can be difficult to interpret and where to draw the line with cluster membership is not necessarily defined.</li> </ul>"},{"location":"machine_learning/unsupervised/hierarchical/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> <li>Agglomerative Hierarchical Clustering</li> <li>Distance Method Formulas</li> <li>Hierarchical Clustering \u2014 Explained</li> </ol>"},{"location":"machine_learning/unsupervised/k-means/","title":"K-Means Clustering","text":""},{"location":"machine_learning/unsupervised/k-means/#k-means-clustering","title":"K-means Clustering","text":"<p>K-means clustering seeks to derive \\(k\\) number of clusters so that the variation within the cluster is minimized. Additionally, the number of clusters, \\(k\\), is specified by the user. The standard k-means algorithm is the Hartigan-Wong algorithm, which starts by determining the sum of squares for each cluster:</p> \\[W(C_k) = \\sum_{x_i\\in{C_k}}{(x_i - \\mu_k)^2}\\] <p>Then the total within cluster variation is calculated by:</p> \\[total\\ within\\ cluster\\ variation = \\sum_{k=1}^k{\\sum_{x_i\\in{C_k}}{(x_i - \\mu_k)^2}}\\] <p>Explanation of Terms</p> <ul> <li>\\(C_k\\) cluster number \\(k\\)</li> <li>\\(x_i\\) point in cluster \\(C_k\\)</li> <li>\\(\\mu_k\\) mean of points in cluster \\(C_k\\)</li> <li>\\(k\\) number of clusters</li> </ul> <p>Info</p> <p>This total within cluster variation is then minimized to best assign data points to the \\(k\\) number of clusters</p>"},{"location":"machine_learning/unsupervised/k-means/#pre-processing","title":"Pre-Processing","text":"<p>Before we apply k-means we will need to create our distance matrix:</p> <pre><code># load the libraries\n.libPaths(c(\"/cluster/tufts/hpc/tools/R/4.0.0\"))\nlibrary(tidyverse)\nlibrary(factoextra)\n\n# load our counts data\ncounts &lt;- read.csv(\n  file=\"data/gbm_cptac_2021/data_mrna_seq_fpkm.txt\",\n  header = T,\n  sep = \"\\t\")\n\n# make the genes our rownames\nrownames(counts) &lt;- make.names(counts$Hugo_Symbol,unique = TRUE)\n\n# remove the gene symbol column\ncounts &lt;- counts %&gt;%\n  select(-c(Hugo_Symbol)) \n\n# log2 transform our data \n# transpose our data so that our patients are rows\ncounts &lt;- t(log2(counts + 1))\n\n# Change NA counts to 0\ncounts[!is.finite(counts)] &lt;- 0\n\n# generate correlation distance matrix\ndist &lt;- get_dist(counts,method = \"pearson\")\n\n# plot correlation distance matrix\nfviz_dist(dist) +\n  theme(axis.text = element_text(size = 3)) +\n  labs(\n    title = \"Pearson Correlation Distances Between Samples\",\n    fill = \"Pearson Correlation\"\n  )\n</code></pre> <p></p>"},{"location":"machine_learning/unsupervised/k-means/#choosing-k-number-of-clusters","title":"Choosing K Number of Clusters","text":"<p>In R we can use the <code>fviz_nbclust()</code> to determine the optimal number of clusters. This will generate a plot and where the plot dips dramatically is our optimal number of \\(k\\)!</p> <pre><code># k-means clustering\n# choosing k\nfviz_nbclust(counts, kmeans, method = \"wss\") +\n  geom_point(color=\"midnightblue\")+\n  geom_line(color=\"midnightblue\")+\n  geom_vline(xintercept = 3,color=\"firebrick\")\n</code></pre> <p></p> <p>Here we do not see a drastic dip in our plot so we will choose 3 clusters here.</p>"},{"location":"machine_learning/unsupervised/k-means/#applying-k-means","title":"Applying K-means","text":"<p>We will now perform k-means clustering and plot the results!</p> <pre><code># applying the k-means function\nkm &lt;- kmeans(counts, 3, nstart = 25)\nfviz_cluster(km, \n             counts,\n             geom = \"point\",\n             ellipse.type = \"norm\")+\n  theme_bw()+\n  labs(\n    title = \"K-means Cluster Plot with 3 Clusters\"\n  )\n</code></pre> <p></p> <p>You will note here that the clusters overlap to a great degree and there isn't great separation between them.</p>"},{"location":"machine_learning/unsupervised/k-means/#k-means-shortcomings","title":"K-means Shortcomings","text":"<p>Given the lackluster cluster plot above it is worth discussing the shortcomings of k-means clustering:</p> <p>K-means Shortcomings</p> <ul> <li>you are going to need to determine the optimal number of clusters ahead of time</li> <li>the initial center point is chosen at random! And as such your cluster can change depending on that random center point</li> <li>this approach can rely heavily on the mean of the cluster points and the mean is sensitive to outliers in the data!</li> <li>k-means clustering can be affected by data order as well</li> </ul>"},{"location":"machine_learning/unsupervised/k-means/#references","title":"References","text":"<ol> <li>Clustering Distance Measures</li> <li>K-Means Clustering in R: Algorithm and Practical Examples</li> </ol>"},{"location":"omics/omics/","title":"Omics Tutorials","text":""},{"location":"omics/omics/#genomics","title":"Genomics","text":"<ul> <li>Tumor Variant Calling</li> </ul>"},{"location":"omics/omics/#transcriptomics","title":"Transcriptomics","text":"<ul> <li>Bulk RNA-Seq</li> <li>Single-Cell RNA-Seq</li> </ul>"},{"location":"omics/omics/#epigenetics","title":"Epigenetics","text":"<ul> <li>ChIP-Seq Analayis</li> </ul>"},{"location":"omics/omics/#proteomics","title":"Proteomics","text":"<ul> <li>Labelled Proteomics Data Analysis</li> </ul>"},{"location":"omics/omics/#metagenomics","title":"Metagenomics","text":"<ul> <li>16S Amplicon Sequencing Data Analysis</li> </ul>"},{"location":"omics/omics/#epidemiology","title":"Epidemiology","text":""},{"location":"omics/omics/#multiomics","title":"Multiomics","text":""},{"location":"omics/epigentics/chip_seq/01_introduction/","title":"01 introduction","text":""},{"location":"omics/epigentics/chip_seq/01_introduction/#chip-seq-analysis-pipeline","title":"ChIP-Seq Analysis Pipeline","text":"<pre><code># Step 1: Quality Control with FastQC\nfor file in *.fastq; do\n    fastqc $file\ndone\n\n# Step 2: Trim Paired End Data with Trim Galore\nfor file in *_R1.fastq; do\n    base=$(basename $file _R1.fastq)\n    trim_galore --paired $base_R1.fastq $base_R2.fastq\ndone\n\n# Step 3: Align to Reference Genome with Bowtie2\nfor file in *_val_1.fq; do\n    base=$(basename $file _val_1.fq)\n    bowtie2 -x reference_genome -1 $base_val_1.fq -2 $base_val_2.fq -S $base.sam\ndone\n\n# Step 4: Filter BAM Files with Samtools and Sambamba\nfor file in *.sam; do\n    base=$(basename $file .sam)\n    samtools view -bS $base.sam | sambamba view -f bam -F not unmapped -S -o $base.bam /dev/stdin\ndone\n\n# Step 5: Peak Calling with MACS2\nfor file in *.bam; do\n    base=$(basename $file .bam)\n    macs2 callpeak -t $base.bam -f BAM -g genome_size -n $base\ndone\n\n# Step 6: Quality Assessment with Phantompeakqualtools and ChIPQC\nfor file in *.bam; do\n    base=$(basename $file .bam)\n    Rscript phantompeakqualtools.R -s $base.bam -out $base\n    Rscript chipqc.R -s $base.bam -out $base\ndone\n\n# Step 7: Compare Peak Calls with Bedtools\nfor file in *_peaks.narrowPeak; do\n    base=$(basename $file _peaks.narrowPeak)\n    bedtools intersect -a $base_peaks.narrowPeak -b reference_peaks.bed -wa -u &gt; $base_filtered_peaks.bed\ndone\n\n# Step 8: Combine Peak Calls with IDR and Bedtools\nidr --samples $sample1_peaks.narrowPeak $sample2_peaks.narrowPeak --input-file-type narrowPeak --output-file $sample1_vs_sample2_idr_peaks.narrowPeak\nbedtools merge -i $sample1_vs_sample2_idr_peaks.narrowPeak &gt; $sample1_vs_sample2_idr_peaks_merged.bed\n\n# Step 9: Annotate with GREAT\nfor file in *_idr_peaks_merged.bed; do\n    base=$(basename $file _idr_peaks_merged.bed)\n    great -r $file -n $base\ndone\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/01_alt/","title":"01 alt","text":""},{"location":"omics/genomics/tumor_variant_calling/01_alt/#variant-calling-in-tumor-samples","title":"Variant Calling In Tumor Samples","text":"<p>Tools Used</p> <p>FastQC, Trimmomatic, BWA, Samtools, Picard, GATK, and Funcotator</p> <pre><code>#!/bin/bash\n\n# Set input and output variables\nfastq1=\"path/to/tumor_sample_R1.fastq.gz\"\nfastq2=\"path/to/tumor_sample_R2.fastq.gz\"\nref_genome=\"path/to/reference_genome.fasta\"\noutput_dir=\"path/to/output_directory\"\n\n\n# Step 1: Quality Control with FastQC\nfastqc -o $output_dir $fastq1 $fastq2\n\necho \"Quality Control Completed\"\n\n# Step 2: Trimming with Trimmomatic\ntrimmomatic PE -threads 4 $fastq1 $fastq2 \\\n    $output_dir/trimmed_tumor_sample_R1.fastq.gz \\\n    $output_dir/unpaired_tumor_sample_R1.fastq.gz \\\n    $output_dir/trimmed_tumor_sample_R2.fastq.gz \\\n    $output_dir/unpaired_tumor_sample_R2.fastq.gz \\\n    ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\necho \"Trimming Completed\"\n\n# Step 3: Indexing Reference Genome with BWA\nbwa index $ref_genome\n\n# Step 4: Alignment with BWA\nbwa mem -t 4 $ref_genome \\\n    $output_dir/trimmed_tumor_sample_R1.fastq.gz \\\n    $output_dir/trimmed_tumor_sample_R2.fastq.gz \\\n    &gt; $output_dir/aligned_tumor_sample.sam\n\necho \"Alignment Completed\"\n\n# Step 5: Sorting Alignment Files with Samtools\nsamtools sort -@ 4 -o $output_dir/sorted_tumor_sample.bam $output_dir/sorted_tumor_sample.bam\n\necho \"Alignment Files Sorted\"\n\n# Step 6: Mark Duplicates with Picard\njava -jar picard.jar MarkDuplicates \\\n    I=$output_dir/merged_tumor_sample.bam \\\n    O=$output_dir/markduplicates_tumor_sample.bam \\\n    M=$output_dir/markduplicates_metrics.txt\n\n\necho \"Duplicates Marked\"\n\n\n# Step 7: Base Recalibration with GATK\ngatk BaseRecalibrator \\\n    -R $ref_genome \\\n    -I $output_dir/markduplicates_tumor_sample.bam \\\n    --known-sites known_sites.vcf \\\n    -O $output_dir/recalibration_report.table\n\n# Step 8: Apply Base Recalibration with GATK\ngatk ApplyBQSR \\\n    -R $ref_genome \\\n    -I $output_dir/markduplicates_tumor_sample.bam \\\n    --bqsr-recal-file $output_dir/recalibration_report.table \\\n    -O $output_dir/recalibrated_tumor_sample.bam\n\necho \"Bases Have Been Recalibrated\"\n\n# Step 9: Variant Calling with GATK\ngatk Mutect2 \\\n    -R $ref_genome \\\n    -I $output_dir/recalibrated_tumor_sample.bam \\\n    -tumor tumor_sample \\\n    -O $output_dir/tumor_sample_variants.vcf\n\necho \"Variant Calling Complete\"\n\n# Step 10: Variant Annotation with Funcotator\ngatk Funcotator \\\n    -R $ref_genome \\\n    -V $output_dir/tumor_sample_variants.vcf \\\n    -O $output_dir/annotated_tumor_sample_variants.vcf \\\n    --data-sources-path funcotator_data_sources\n\necho \"Variant Annotation Complete\"\n\n# Step 11: Removing Intermediate Files\nrm $output_dir/aligned_tumor_sample.sam\nrm $output_dir/sorted_tumor_sample.bam\nrm $output_dir/merged_tumor_sample.bam\nrm $output_dir/markduplicates_tumor_sample.bam\nrm $output_dir/recalibration_report.table\n\necho \"Tumor Variant Pipeline Complete\"\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/01_introduction/","title":"01 introduction","text":""},{"location":"omics/genomics/tumor_variant_calling/01_introduction/#tumor-variant-calling","title":"Tumor Variant Calling","text":"<pre><code>#!/bin/bash\n\n# Step 1: Define input and output variables\nfastq_dir=path/to/fastq/files\nreference_genome=path/to/reference_genome.fa\noutput_dir=path/to/output_directory\n\n# Step 2: Perform quality control using FastQC\nfastqc -o $output_dir/fastqc_reports $fastq_dir/*.fastq\n\n# Step 3: Trim paired-end data using Trim Galore\nmkdir $output_dir/trimmed_data\nfor file in $fastq_dir/*_R1.fastq; do\n    base=$(basename $file _R1.fastq)\n    trim_galore --paired \\\n    --output_dir $output_dir/trimmed_data \\\n    $fastq_dir/${base}_R1.fastq \\\n    $fastq_dir/${base}_R2.fastq\ndone\n\n# Step 4: Index the reference genome\nbwa index $reference_genome\n\n# Step 5: Align paired-end data with BWA\nmkdir $output_dir/aligned_data\nfor file in $output_dir/trimmed_data/*_val_1.fq; do\n    base=$(basename $file _val_1.fq)\n    bwa mem $reference_genome \\\n    $output_dir/trimmed_data/${base}_val_1.fq \\\n    $output_dir/trimmed_data/${base}_val_2.fq &gt; $output_dir/aligned_data/${base}.sam\ndone\n\n# Step 6: Sort and merge paired-end files\nmkdir $output_dir/sorted_data\nfor file in $output_dir/aligned_data/*.sam; do\n    base=$(basename $file .sam)\n    samtools sort -o \\\n    $output_dir/sorted_data/${base}.bam \\\n    $output_dir/aligned_data/${base}.sam\n\n    samtools index $output_dir/sorted_data/${base}.bam\ndone\n\n# Step 7: Mark paired-end data duplicates\nmkdir $output_dir/deduplicated_data\nfor file in $output_dir/sorted_data/*.bam; do\n    base=$(basename $file .bam)\n    java -jar picard.jar MarkDuplicates \\\n    I=$output_dir/sorted_data/${base}.bam \\\n    O=$output_dir/deduplicated_data/${base}_dedup.bam \\\n    M=$output_dir/deduplicated_data/${base}_dedup_metrics.txt\n\n    samtools index $output_dir/deduplicated_data/${base}_dedup.bam\ndone\n\n# Step 8: Recalibrate bases\nmkdir $output_dir/recalibrated_data\nfor file in $output_dir/deduplicated_data/*_dedup.bam; do\n    base=$(basename $file _dedup.bam)\n    gatk BaseRecalibrator \\\n    -R $reference_genome \\\n    -I $output_dir/deduplicated_data/${base}_dedup.bam \\\n    -O $output_dir/recalibrated_data/${base}_recal_data.table \\\n    --known-sites known_sites.vcf\ndone\n\n# Step 9: Apply recalibration\nmkdir $output_dir/final_data\nfor file in $output_dir/recalibrated_data/*_recal_data.table; do\n    base=$(basename $file _recal_data.table)\n    gatk ApplyBQSR \\\n    -R $reference_genome \\\n    -I $output_dir/deduplicated_data/${base}_dedup.bam \\\n    -O $output_dir/final_data/${base}_recal.bam \\\n    --bqsr-recal-file $output_dir/recalibrated_data/${base}_recal_data.table\ndone\n\n# Step 10: Call somatic variants\nmkdir $output_dir/variant_calls\nfor file in $output_dir/final_data/*_recal.bam; do\n    base=$(basename $file _recal.bam)\n    gatk Mutect2 \\\n    -R $reference_genome \\\n    -I $output_dir/final_data/${base}_recal.bam \\\n    -O $output_dir/variant_calls/${base}_variants.vcf\ndone\n\n# Step 10: Variant annotation with Funcotator\nmkdir $output_dir/annotated_variants\nfor file in $output_dir/variant_calls/*.vcf; do\n    base=$(basename $file _variants.vcf)\n    gatk Funcotator \\\n    -R $reference_genome \\\n    -V $output_dir/variant_calls/${base}_variants.vcf \\\n    -O $output_dir/annotated_variants/${base}_annotated.vcf\ndone\n\n# Remove intermediate files\nrm -rf $output_dir/fastqc_reports\nrm -rf $output_dir/trimmed_data\nrm -rf $output_dir/aligned_data\nrm -rf $output_dir/sorted_data\nrm -rf $output_dir/deduplicated_data\nrm -rf $output_dir/recalibrated_data\nrm -rf $output_dir/final_data\nrm -rf $output_dir/variant_calls\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/","title":"Supplement","text":""},{"location":"omics/genomics/tumor_variant_calling/supplement/#background","title":"Background","text":""},{"location":"omics/genomics/tumor_variant_calling/supplement/#copy-over-data","title":"Copy Over Data","text":"<pre><code># make a directory for the Analysis\nmkdir tn_variant_analysis\n\n# enter directory and copy over data\ncd tn_variant_analysis\nwget https://zenodo.org/record/2582555/files/SLGFSK-N_231335_r1_chr5_12_17.fastq.gz\nwget https://zenodo.org/record/2582555/files/SLGFSK-N_231335_r2_chr5_12_17.fastq.gz\nwget https://zenodo.org/record/2582555/files/SLGFSK-T_231336_r1_chr5_12_17.fastq.gz\nwget https://zenodo.org/record/2582555/files/SLGFSK-T_231336_r2_chr5_12_17.fastq.gz\ncd ..\n\n# make a directory for the reference\nmkdir ref_data\ncd ref_data\nwget https://zenodo.org/record/2582555/files/hg19.chr5_12_17.fa.gz\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#initial-data-qc","title":"Initial Data QC","text":"<pre><code># make the raw data qc folder \nmkdir raw_data_qc\n\n# run fastqc\nfastqc ./raw_data/ -o raw_data_qc\n\n# run multiqc on data \nmultiqc ./raw_data_qc/ -o raw_data_qc/ --title \"Raw Data QC Report\"\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#trimmed-data","title":"Trimmed Data","text":"<pre><code># make an output directory\nmkdir trimmed_data\n\n# run trim galore\nparallel --xapply trim_galore --paired -o ./trimmed_data/ ::: ./raw_data/*_R1.fastq.gz ::: ./raw_data/*_R2.fastq.gz\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#trimmed-data-qc","title":"Trimmed Data QC","text":"<pre><code># make the raw data qc folder \nmkdir trimmed_data_qc\n\n# run fastqc\nfastqc ./trimmed_data/*.fq.gz -o trimmed_data_qc\n\n# run multiqc\nmultiqc ./trimmed_data_qc/ -o trimmed_data_qc/ --title \"Trimmed Data QC Report\"\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#index-reference","title":"Index Reference","text":"<pre><code># index the reference fasta\ncd ref_data\nbwa index hg19.chr5_12_17.fa.gz\ncd ..\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#read-alignment","title":"Read Alignment","text":"<pre><code># make alignment directory\nmkdir aligned_data\ncd aligned_data\n\nfor fname in ../trimmed_data/*_R1.fq.gz\ndo\n    base=${fname%_R1*}\n    bwa mem -t 4 -T 0 -R \"@RG SOMETHING\"../ref_data/hg19.chr5_12_17.fa.gz \"../trimmed_data/${base}_R1.fq.gz\"  \"../trimmed_data/${base}_R2.fq.gz\"  | samtools view -Shb -o ./${base}.bam \ndone\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#sort-sam-files","title":"Sort Sam Files","text":"<pre><code>cd aligned_data\n\n# for each bam file sor the bam file by coordinate\nfor fbam in ./*.bam\ndo\n    java -jar picard.jar SortSam \\\n    CREATE_INDEX=true \\\n    INPUT=fbam \\\n    OUTPUT=fbam.sorted.bam \\\n    SORT_ORDER=coordinate \\\n    VALIDATION_STRINGENCY=STRICT\ndone\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#mark-duplicates","title":"Mark Duplicates","text":"<pre><code>cd aligned_data\n\n# mark duplicates\nfor fsortbam in ./*.sorted.bam\ndo\n    picard -Xmx7g MarkDuplicates \\\n        I=./fsortbam \\\n        O=./fsortbam.dup.bam \\\n        METRICS_FILE=./fsortbam_dup_metrics.txt\ndone\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#base-recalibration","title":"Base Recalibration","text":"<pre><code>for fdupbam in ./*.dup.bam\ndo\n    # step 1  - Build the model\n    gatk --java-options \"-Xmx7g\" BaseRecalibrator \\\n    -I ./fdupbam \\\n    -R ../ref_data/hg19.chr5_12_17.fa.gz \\\n    --known-sites ../ref_data/dbsnp_146.hg38.vcf.gz \\\n    -O ./fdupbam_recal_data.table\n\n    # step 2: Apply the model to adjust the base quality scores\n    gatk --java-options \"-Xmx7g\" ApplyBQSR \\\n    -I ./fdupbam \\\n    -R ../ref_data/hg19.chr5_12_17.fa.gz \\\n    --bqsr-recal-file ./fdupbam_recal_data.table \\\n    -O ./fdupbam.sort.dup.bqsr.bam    \ndone\n</code></pre>"},{"location":"omics/genomics/tumor_variant_calling/supplement/#running-the-haplotypecaller","title":"Running the HaplotypeCaller","text":"<pre><code>for fbqsr in ./*.sort.dup.bqsr.bam \ndo\n\ndone\n</code></pre>"},{"location":"omics/metagenomics/amplicon_seq/01_introduction/","title":"01 introduction","text":""},{"location":"omics/metagenomics/amplicon_seq/01_introduction/#16s-amplicon-sequencing-data-analysis","title":"16S Amplicon Sequencing Data Analysis","text":"<pre><code># --- Amplicon Sequencing Analyis ----------------------------------------------\n\n# --- Load Libraries -----------------------------------------------------------\n\n# load our libraries\n.libPaths(c('/cluster/tufts/hpc/tools/R/4.0.0',.libPaths()))\nlibrary(dada2)\nlibrary(phyloseq)\nlibrary(ggplot2)\nlibrary(DESeq2)\nlibrary(tidyverse)\nlibrary(phangorn)\nlibrary(msa)\n\n# --- Quality Control ----------------------------------------------------------\n# path to files\npath &lt;- \"../data/raw_fastq\"\n\n# sort our files by forward and reverse strands \n# so that the sample names for each strand matches\n# our data has the pattern \"_pass_1.fastq.gz\" \n# and \"_pass_2.fastq.gz\"\npath2Forward &lt;- sort(\n  list.files(\n    path,\n    pattern=\"_pass_1.fastq.gz\",\n    full.names = TRUE)\n)\npath2Reverse &lt;- sort(\n  list.files(\n    path,\n    pattern=\"_pass_2.fastq.gz\",\n    full.names = TRUE)\n)\n\n# now let's grab our sample names\nsampleNames &lt;- sapply(\n  strsplit(\n    basename(path2Forward), \"_\"), `[`, 1)\n\n# plot the forward strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Forward[1:2])+\n  guides(scale = \"none\")\n\n# plot the reverse strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Reverse[1:2])+\n  guides(scale = \"none\")\n\n# --- Trimming -----------------------------------------------------------------\n\n# create new file names for filtered forward/reverse fastq files\n# name each file name in the vector with the sample name\n# this way we can compare the forward and reverse files \n# when we filter and trim\nfiltForward &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltReverse &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_R_filt.fastq.gz\"))\nnames(filtForward) &lt;- sampleNames\nnames(filtReverse) &lt;- sampleNames\n\n# Now we will filter and trim our sequences\nout &lt;- filterAndTrim(\n  path2Forward,\n  filtForward,\n  path2Reverse, \n  filtReverse,\n  truncLen = c(200,150),\n  maxN=0, \n  maxEE=c(2,2), \n  truncQ=2, \n  rm.phix=TRUE,\n  compress=TRUE)\n\n# --- DADA2 Error Model --------------------------------------------------------\n# Learn Error Rates\n\n# dada2 uses a parametric model to learn the error rates\n# for each sequence\nerrForward &lt;- learnErrors(filtForward)\nerrReverse &lt;- learnErrors(filtReverse)\n\n# plot the error rate against theoretical error rates\nplotErrors(errForward,nominalQ=TRUE)\n\n# --- Inferring Sequence Variants ----------------------------------------------\n# Infer Sequnce Variants\n\n# we will now run the dada2 algorithm \n# this algorithm delivers \"true\" sequence variants\n# with information gathered from the error model \n# generated above\ndadaForward &lt;- dada(filtForward, err=errForward)\ndadaReverse &lt;- dada(filtReverse, err=errReverse)\n\n# let's get a summary of our first sample\ndadaForward[[1]]\n\n# --- Merging Reads ------------------------------------------------------------\n\n# Merge Read Pairs\n\n# so far we have \"denoised\", so to speak, \n# these sequence variants. We now need to merge the\n# forward and reverse strands\nmergers &lt;- mergePairs(\n  dadaForward,\n  filtForward,\n  dadaReverse, \n  filtReverse, \n  verbose=TRUE)\n\n# --- ASV Table ----------------------------------------------------------------\n# Making a Sequence Table\n\n# now that we have merged sequences we can construct\n# an Amplicon Sequence Variant (ASV) table\nseqtab &lt;- makeSequenceTable(mergers)\n\n# --- Chimera Removal ----------------------------------------------------------\n\n# Removing Chimeras\n\n# Chimeric sequences occur as errors during PCR \n# when two unrelated templates for a hybrid sequence\n# we will need to remove them before going forward\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=\"consensus\", verbose=TRUE)\n## check to see if the dimensions are different\n## between the chimera filtered and unfiltered\n## ASV tables\n\ndim(seqtab)\ndim(seqtab.nochim)\n\n# --- Pipeline Quality Control -------------------------------------------------\n# Final QC\n\n## we have performed quite a few steps \n## and it would be nice to get a final qc check \n## before assigning taxonomy\ngetN &lt;- function(x) sum(getUniques(x))\nfinalQC &lt;- cbind(\n  out, \n  sapply(dadaForward, getN),\n  sapply(dadaReverse, getN),\n  sapply(mergers, getN),\n  rowSums(seqtab.nochim))\ncolnames(finalQC) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\nrownames(finalQC) &lt;- sampleNames\nfinalQC\n\n# --- Assigning Taxonomy -------------------------------------------------------\n# Assigning Taxonomy\n\n# dada2 uses a naive Bayes classifier when\n# assigning taxonomy. This means we need a training\n# set of sequences with known taxonomy information.\n# here we use the silva database\n\ntaxa &lt;- assignTaxonomy(seqtab.nochim, \"../data/silva_nr99_v138.1_train_set.fa.gz\")\n\n# --- Constructing the Phylogenetic Tree ---------------------------------------\n# extract sequences\n# name the sequences with their sequence so \n# that the ends of the phylogenetic tree are labeled\n# align these sequences\nseqs &lt;- getSequences(seqtab)\nnames(seqs) &lt;- seqs \nmult &lt;- msa(seqs, method=\"ClustalW\", type=\"dna\", order=\"input\")\n\n# convert multiple sequence alignment to a phyDat object\n# calculate the nucleotide distances between ASVs\n# use a neighbor joining algorithm to generate the tree\n# finally calculate the likelihood of the tree given the sequence alignment\nphang.align &lt;- as.phyDat(mult, type=\"DNA\", names=getSequence(seqtab))\ndm &lt;- dist.ml(phang.align)\ntreeNJ &lt;- NJ(dm)\nfit = pml(treeNJ, data=phang.align)\n\n# --- Making a PhyloSeq Object -------------------------------------------------\n\n# Create phyloseq object\n\n# upload meta data for study\n# ensure the rownames of our meta data are our sample name\nmeta &lt;- read.csv(\"../data/metaData.txt\")\nrownames(meta) &lt;- meta$Run\n\n# combine the ASV table, the meta data, and taxonomic data\n# to create the phyloseq object\nps &lt;- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), \n               sample_data(meta), \n               tax_table(taxa),\n               phy_tree(fit$tree)\n)\n\n# Update ASV names to be shorter\n\n# The full ASV DNA sequence can be hard to look at\n# for this reason we move the sequence information to \n# the refseq slot of the phyloseq object\ndna &lt;- Biostrings::DNAStringSet(taxa_names(ps))\nnames(dna) &lt;- taxa_names(ps)\nps &lt;- merge_phyloseq(ps, dna)\ntaxa_names(ps) &lt;- paste0(\"ASV\", seq(ntaxa(ps)))\n\n\n# --- Diversity Analysis -------------------------------------------------------\n\n# Plotting Alpha Diversity Metrics\nplot_richness(ps, x=\"Host\", measures=c(\"Shannon\", \"Simpson\"), color=\"Host\")+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle=65,hjust=1))\n\n# calculate the unifrac distance between samples \n# plot unifrac distances\nordu = ordinate(ps, \"PCoA\", \"unifrac\", weighted=TRUE)\nplot_ordination(ps, ordu, color=\"Host\")+\n  theme_bw()+\n  labs(title = \"Unifrac Distances\")\n\n# --- Phylum Present -----------------------------------------------------------\n\n# transform the sample counts to proportions\n# separate out our proportions\n# separate our our tax info\nps.prop &lt;- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))\notu = data.frame(t(data.frame(ps.prop@otu_table)))\ntax = data.frame(ps.prop@tax_table) \n\n# merge the otu table and phylum column\n# reshape our data to be accepted by ggplot\n# merge taxa data with sample meta data\nmerged &lt;- merge(otu,\n                tax %&gt;% select(Phylum),\n                by=\"row.names\") %&gt;%\n  select(-Row.names) %&gt;%\n  reshape2::melt() %&gt;%\n  merge(.,\n        data.frame(ps.prop@sam_data) %&gt;%\n          select(Run,Host),\n        by.x=\"variable\",\n        by.y=\"Run\")\n\n# plot our taxa \ntaxa_plot &lt;- ggplot(merged,aes(x=variable,y=value,fill=Phylum)) +\n  geom_bar(stat='identity') +\n  theme_bw()+\n  theme(axis.text.x = element_text(angle=45,hjust=1))+\n  labs(\n    x=\"\",\n    y=\"Abundance\",\n    title = \"Barplot of Phylum Abundance\"\n  )+\n  facet_wrap(Host ~ ., scales = \"free_x\")\ntaxa_plot\n\n# --- Differential Abundance ---------------------------------------------------\n\n# Differential Abundance\n\n## convert phyloseq object to DESeq object this dataset was downsampled and \n## as such contains zeros for each ASV, we will need to\n## add a pseudocount of 1 to continue and ensure the data are still integers\n## run DESeq2 against Host status, and ensure wild type is control,\n## filter for significant changes and add in phylogenetic info\ndds = phyloseq_to_deseq2(ps, ~ Host)\ndds@assays@data@listData$counts = apply((dds@assays@data@listData$counts +1),2,as.integer)\ndds = DESeq(dds, test=\"Wald\", fitType=\"parametric\")\nres = data.frame(\n  results(dds,\n          cooksCutoff = FALSE, \n          contrast = c(\"Host\",\"C57BL/6NTac\",\"Mus musculus domesticus\")))\nsigtab = res %&gt;%\n  cbind(tax_table(ps)[rownames(res), ]) %&gt;%\n  dplyr::filter(padj &lt; 0.05) \n\n## order sigtab in direction of fold change\nsigtab &lt;- sigtab %&gt;%\n  mutate(Phylum = factor(as.character(Phylum), \n                         levels=names(sort(tapply(\n                           sigtab$log2FoldChange, \n                           sigtab$Phylum, \n                           function(x) max(x)))))\n  )\n\n# as a reminder let's plot our abundance data again\ntaxa_plot\n\n## plot differential abundance\nggplot(sigtab , aes(x=Phylum, y=log2FoldChange, color=padj)) + \n  geom_point(size=6) + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  ggtitle(\"Mus musculus domesticus v. C57BL/6NTac\")\n</code></pre>"},{"location":"omics/proteomics/labelled_data/01_introduction/","title":"01 introduction","text":""},{"location":"omics/proteomics/labelled_data/01_introduction/#labelled-proteomics-analysis","title":"Labelled Proteomics Analysis","text":"<pre><code># Step 1: Convert TMT (.raw) files to mzML using OpenMS\nmkdir mzml_files\nfor file in *.raw; do\n    OpenSwathConverter -in $file -out mzml_files/${file%.raw}.mzML\ndone\n\n\n# Step 2: Identify peptides using MSGFPlus\nmkdir results\nfor mzml_file in mzml_files/*.mzML; do\n    base_name=$(basename $mzml_file .mzML)\n    MSGFPlus -s $mzml_file -o results/${base_name}_identifications.mzid -d database.fasta -t 20ppm -m 1\ndone\n</code></pre> <pre><code># Step 3: Load Id files as a table\nlibrary(\"mzID\")\nmzids &lt;- list.files(system.file('extdata', package = 'mzID'),\n                    pattern = '*.mzid', full.names = TRUE)\n# load in data\nids &lt;- mzID(mzids)\n\n# flatten into a table\nfid &lt;- flatten(id)\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/","title":"Introduction","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#background","title":"Background","text":"<p>Sequencing data analysis typically focuses on either assessing DNA or RNA. As a reminder here is the interplay between DNA, RNA, and protein:</p> <p>Translation: DNA to mRNA to Protein</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#dna-sequencing","title":"DNA Sequencing","text":"<ul> <li>Fixed copy of a gene per cell </li> <li>Analysis goal: Variant calling and interpretation</li> </ul>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#rna-sequencing","title":"RNA Sequencing","text":"<ul> <li>Copy of a transcript per cell depends on gene expression</li> <li>Analysis goal: Differential expression and interpretation</li> </ul> <p>Note</p> <p>Here we are working with RNA sequencing</p>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#next-generation-sequencing","title":"Next Generation Sequencing","text":"<p>Here we will analyze a DNA sequence using next generation sequencing data. Here are the steps to get that data:</p> <ul> <li>Library Preparation: DNA is fragmented and adapters are added to these fragments</li> </ul> <p>Library Preparation</p> <p></p> <ul> <li>Cluster Amplification: This library is loaded onto a flow cell, where the adapters help hybridize the fragments to the flow cell. Each fragment is then amplified to form a clonal cluster</li> </ul> <p>Cluster Amplification</p> <p></p> <ul> <li>Sequencing: Fluorescently labelled nucleotides are added to this flow cell and each time a base in the fragment bonds a light signal is emmitted telling the sequencer which base is which in the sequence.</li> </ul> <p>Sequencing</p> <p></p> <ul> <li>Alignment &amp; Data Analysis: These sequenced fragments, or reads, can then be aligned to a reference sequence to determine differences.</li> </ul> <p>Read Alignment</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#singe-end-v-paired-end-data","title":"Singe End v. Paired End Data","text":"<ul> <li>single-end sequence each DNA fragement from one end only</li> <li>paired-end sequence each DNA fragement from both sides. Paired-end data is useful when sequencing highly repetitive sequences.</li> </ul> <p>Single-End V. Paired-End Read Data</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_introduction/#references","title":"References","text":"<ol> <li>An introduction to Next-Generation Sequencing Technology</li> <li>Translation: DNA to mRNA to Protein</li> </ol>"},{"location":"omics/transcriptomics/bulk_rna_seq/01_pipeline/","title":"01 pipeline","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/01_pipeline/#bulk-rna-seq-pipeline","title":"Bulk RNA-seq Pipeline","text":"<pre><code>#!/bin/bash\n\n# Step 1: Perform quality control using FastQC\nfastqc -o qc_output/ *.fastq.gz\n\n# Step 2: Aggregate quality control results using MultiQC\nmultiqc qc_output/ -o multiqc_output/\n\necho \"Initial Quality Control Comlete!\"\n\n# Step 3: Trim data using Trim Galore\nmkdir trimmed_output/\nfor file in *.fastq.gz; do\n    trim_galore --paired --output_dir trimmed_output/ $file\ndone\n\necho \"Input Data Has Been Trimmed!\"\n\n# Step 4: Index the reference genome using STAR\nmkdir star_index/\nstar --runMode genomeGenerate \\\n--genomeDir star_index/ \\\n--genomeFastaFiles reference.fasta\n\necho \"Genome has been indexed!\"\n\n# Step 5: Align paired Fastq files to the reference genome using STAR in a loop\nmkdir alignment_output/\nfor file in trimmed_output/*_val_1.fq.gz; do\n    base=$(basename $file _val_1.fq.gz)\n    star --genomeDir star_index/ \\\n    --readFilesIn $file trimmed_output/$base_val_2.fq.gz \\\n    --outFileNamePrefix alignment_output/\"$base\"_ \\\n    --outSAMtype BAM SortedByCoordinate \\\n    --outSAMunmapped Within \\\n    --outSAMattributes Standard \ndone\n\necho \"Read Alignment Complete!\"\n\n# Step 6: Count features using featureCounts\nmkdir featurecounts_output/\n\nfeatureCounts -a annotation.gtf \\\n-o featurecounts_output/counts.txt \\\nalignment_output/*.sam\n\necho \"Feature Counting Complete!\"\n\n# Step 7: Remove intermediate files\nrm -r qc_output/ trimmed_output/ star_index/ alignment_output/\n\necho \"Pipeline Completed!\"\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/","title":"Setup","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/#setting-up-the-analysis-directory","title":"Setting Up The Analysis Directory","text":"<p>To begin we will need a space to work, let's create a directory to house all of our input data, scripts and results:</p> <pre><code>mkdir rna_seq_pipeline\ncd rna_seq_pipeline\n</code></pre> <p>Now let's make subfolders for our data, scripts and results:</p> <pre><code>mkdir data\nmkdir tools\nmkdir qc_output\nmkdir trimmed_output\nmkdir star_index\nmkdir alignment_output\nmkdir featurecounts_output\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/#creating-a-conda-environment","title":"Creating A Conda Environment","text":"<p>For reproducible research it is advisable to keep the software versions you use consistent. An easy way of ensuring this is by creating a Conda environment. For more information on how to build conda environments check out:</p> <p>Conda Environments</p> <p>Here, we will enter our tools directory and create a conda environment from the following yml file:</p> <pre><code>cd tools\nwget https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/omics/transcriptomics/bulk_rna_seq/data/rnaseq_environment.yml\n</code></pre> <p>Now, let's create the environment and activate it!</p> <pre><code>conda env create -f rnaseq_environment.yml    # create conda environment\nsource activate rnaseq                        # activate conda environment\ncd ..                                         # leave tools directory\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/#downloading-fastq-read-data","title":"Downloading Fastq Read Data","text":"<p>Today we will be working with data from Srinivasan et al. 2020 where they assessed transcriptional changes in patients with and without Alzheimer's disease. Let's create an accession list to download a few files from this study:</p> <pre><code>cd data\nnano accList.txt\n</code></pre> <p>accList.txt</p> <pre><code>SRR8440545\nSRR8440550\nSRR8440537\nSRR8440481\n</code></pre> <p>Now we will need meta data for these samples. The following data was taken from the SRA Run Selector. SRA, or sequence read archive, is a public repository for sequence data which we are pulling from for this analysis.</p> <pre><code>nano meta.txt\n</code></pre> <p>meta.txt</p> <pre><code>ID  Diagnosis   Age Sex\nSRR8440545  Control 53  male\nSRR8440550  Control 81  male\nSRR8440537  AD  74  female\nSRR8440481  AD  79  male\n</code></pre> <p>Before we can download our data we will need to configure the sra-toolkit with the following command:</p> <pre><code>vdb-config -i\n</code></pre> <p>Click <code>X</code> and you are finished configuring! To download the sequence data we will use the following command:</p> <pre><code>fastq-dump -N 100000 -X 200000  --skip-technical --split-3 --clip --gzip  $(&lt;./accList.txt)\n</code></pre> <p>Explanation of Terms</p> <ul> <li><code>-N</code> start at read 100000</li> <li><code>-X</code> end at read 200000</li> <li><code>--skip-technical</code> skip technical reads and only download biological reads</li> <li><code>--split-3</code> split paired end data</li> <li><code>--clip</code> remove adapters</li> <li><code>--gzip</code> compress the output</li> </ul>"},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/#download-reference-data","title":"Download Reference Data","text":"<p>Now to actually figure out what genes are expressed we need some sort of reference to map our reads too. We will be using the following reference:</p> <p>Homo sapiens chromosome 17, GRCh38.p14 Primary Assembly</p> <p>To download the fasta file complete the following steps:</p> <ol> <li>Go to Homo sapiens chromosome 17, GRCh38.p14 Primary Assembly</li> <li>Click <code>Send To</code></li> <li>Click <code>File</code></li> <li>Change the format to <code>FASTA</code></li> <li>Click <code>Create File</code></li> <li>Move the downloaded file to the <code>rna_seq_pipeline/data</code> folder</li> </ol> <p>Downloading the Reference FASTA File</p> <p></p> <p>To download the gff3 file complete the following steps:</p> <ol> <li>Go to Homo sapiens chromosome 17, GRCh38.p14 Primary Assembly</li> <li>Click <code>Send To</code></li> <li>Click <code>File</code></li> <li>Change the format to <code>GFF3</code></li> <li>Click <code>Create File</code></li> <li>Move the downloaded file to the <code>rna_seq_pipeline/data</code> folder</li> </ol> <p>Downloading the Reference GFF3 File</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/02_setup/#references","title":"References","text":"<ol> <li>Alzheimer\u2019s Patient Microglia Exhibit Enhanced Aging and Unique Transcriptional Activation</li> <li>Homo sapiens chromosome 17, GRCh38.p14 Primary Assembly</li> </ol>"},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/","title":"Quality Control","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/#fastq-format","title":"FASTQ Format","text":"<p>The sequencing data we will be looking at is composed of reads, or short nucleotide sequences. Read information is stored in FASTQ files:</p> <p>FASTQ Format</p> <p></p> <p>Here we see that for each read there are four lines:</p> <ul> <li>sequence label</li> <li>nucleotide sequence</li> <li>a separator</li> <li>quality score string</li> </ul>"},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/#quality-scores","title":"Quality Scores","text":"<p>Quality scores are a score representing the probability that a base was called in error. A common score used is the Phred Score:</p> Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% <p>Typically, we would like the bases in our sequence to have a quality score higher than 20, meaning that there was a 1 in 100 chance the base was called in error.</p>"},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/#quality-control","title":"Quality Control","text":"<p>To check quality scores (and a few other metrics), we can use a tool called FastQC. Let's make a script:</p> <pre><code>nano initial_qc.sh\n</code></pre> <p>inital_qc.sh</p> <pre><code>#!/bin/bash\n\n# Step 1: Perform quality control using FastQC\nmkdir qc_output/initial_qc\nfastqc -o qc_output/initial_qc/ data/*.fastq.gz\n\n# Step 2: Aggregate quality control results using MultiQC\nmultiqc qc_output/initial_qc -o qc_output/initial_qc --title \"Raw Data QC Report\"\n\necho \"Initial Quality Control Comlete!\"\n</code></pre> <p>To check out the intial quality control report go to <code>qc_output/initial_qc/</code> and open the file <code>Raw-Data-QC-Report_multiqc_report.html</code>. Here you will see a number of plots:</p> <p>General Statistics: columns for samples, the percent of duplicated sequences, gc content, number of sequences (in million) per sample</p> <p></p> <p>Sequence Counts: Sequence counts for each sample. Duplicate read counts are an estimate only.</p> <p></p> <p>Sequence Quality Histograms: The mean quality value across each base position in the read.</p> <p></p> <p>Per Sequence Quality Scores: The number of reads with average quality scores. Shows if a subset of reads has poor quality.</p> <p></p> <p>Per Sequence GC Content: The average GC content of reads. Normal random library typically have a roughly normal distribution of GC content.</p> <p></p> <p>Per Base N Content: The percentage of base calls at each position for which an N was called.</p> <p></p> <p>Sequence Duplication Levels: The relative level of duplication found for every sequence. </p> <p></p> <p>Overrepresented sequences: The total amount of overrepresented sequences found in each library.</p> <p></p> <p>Adapter Content: The cumulative percentage count of the proportion of your library which has seen each of the adapter sequences at each position.</p> <p></p> <p>In the above plots we note that we have a number of duplicated/overrepresented reads. This is to be expected with RNA-seq data as there could be more than one copy of a transcript present. We can also see that the quality of the reads is quite good, with most bases having an average quality score of at least 30. In the Per Sequence GC Content plot, we expect to see that sequences in a sample have a roughly normal distribution of GC content. GC content can be a way to assess the presenece of more than one organism given that different organisms have different GC content percentages. However this is to be expeceted, since this is RNA-seq data, and different transcripts will have different levels of GC content. We can also see slight spikes in the Per Base N Content plot. N's are inserted during base calling when the sequencer can determine a base is present, but cannot determine which base is present. Finally, we note that there are adapters present in our data, specifically the Illumina Universal Adapter. These need to be removed as they do not represent biological sequences. </p>"},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/#read-trimming","title":"Read Trimming","text":"<p>To remove poor quality sequences or adapter sequences we need a tool that can trim these sequences to only keep the highly quality ones. We can use the tool Trim-Galore to do just that. Let's create a script to trim our fastq sequences and perform QC on them:</p> <p>trimming.sh</p> <pre><code>#!/bin/bash\n\n# Step 3: Trim data using Trim Galore\nmkdir qc_output/trimmed_qc\nfor file in ./data/*.fastq.gz; do\n    trim_galore --output_dir ./trimmed_output $file --fastqc --fastqc_args \"-o ./qc_output/trimmed_qc\"\ndone\n\n# quality control on trimmed\nmultiqc qc_output/trimmed_qc -o qc_output/trimmed_qc/ --title \"Trimmed Data QC Report\" \n\necho \"Input Data Has Been Trimmed!\"\n</code></pre> <p>To confirm that the adapters are gone, check the file <code>Trimmed-Data-QC-Report_multiqc_report.html</code>! There you should note that the no samples were found with any adapter contamination above 0.1%.</p>"},{"location":"omics/transcriptomics/bulk_rna_seq/03_quality_control/#references","title":"References","text":"<ol> <li>Base-Calling of Automated Sequencer Traces Using Phred. II. Error Probabilities</li> <li>Quality Control</li> <li>FastQC</li> <li>MultiQC</li> <li>Trim-Galore</li> </ol>"},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/","title":"Read Alignment","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/#read-alignment-with-star","title":"Read Alignment with STAR","text":"<p>Now that our reads are trimmed and we are left with high quality biological sequences we can try to map them back to the genome of the organism we sampled. A popular choice of aligner is STAR or Spliced Transcripts Alignment to a Reference. STAR works by finding something called a Maximal Mappable Prefix (MMP) or the longest part of a read that matches a reference sequence. STAR is splice aware in that when it cannot be mapped contiguously to a genome (like in that case where the MMP maps to an exon and and intron!) the next MMP will be identified and will be mapped to only an acceptor splice site! This MMP, when failing to reach the end of a read, will be extended till it can find regions that do match - allowing for mismatches and indels:</p> <p>STAR: ultrafast universal RNA-seq aligner</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/#building-an-index","title":"Building an Index","text":"<p>Before we can map our reads we need to index our genome, in order to determine order sequence information in such a way to speed up alignment:</p> <pre><code>nano index.sh\n</code></pre> <p>index.sh</p> <pre><code>#!/bin/bash\n\n# Step 4: Index the reference genome using STAR\nSTAR --runMode genomeGenerate \\\n--genomeDir star_index/ \\\n--genomeFastaFiles data/sequence.fasta\n</code></pre> <p>Explanation of Terms</p> <ul> <li><code>--runMode genomeGenerate</code> generate genome index</li> <li><code>--genomeDir star_index/</code> path to the STAR index</li> <li><code>--genomeFastaFiles data/sequence.fasta</code> path to the reference FASTA file</li> </ul>"},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/#gene-annotation","title":"Gene Annotation","text":"<p>The choice of annotation is very important when aligning sequencing! Different gene annotation sources will have different coordinates for genes (also called features):</p> <ul> <li>GENECODE: advisable for well annotated species (e.g. human/mouse) as this annotation source aims to classify all gene features</li> <li>UCSC or ENSEMBL: advisable for non-model organisms as this annotation source aims to classify well classified gene features</li> </ul> <p>Here you will note we are using RefSeq, another annotation option that only contains experimentally validated genes/features. We are using RefSeq here because we can download information for just chromosome 17 which we chose to speed up this tutorial. However, in when mapping RNA-seq data to the whole genome we recommend GENECODE as this will contain genes/features that may not be studied as well.</p>"},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/#read-alignment","title":"Read Alignment","text":"<p>align.sh</p> <pre><code>#!/bin/bash\n\n# Step 5: Align Fastq Files to a Reference Genome\n\n# unzip fastq files\ngzip -d ./trimmed_output/*.fq.gz\n\nfor file in trimmed_output/*.fq; \ndo\n    base=$(basename $file .fq)\n    STAR --genomeDir star_index/ \\\n    --readFilesIn $file \\\n    --outFileNamePrefix alignment_output/${base}_ \\\n    --outSAMtype BAM SortedByCoordinate \\\n    --outSAMunmapped Within \\\n    --outSAMattributes Standard \\\n    --sjdbGTFfile ./data/sequence.gff3 \\\n    --outFilterScoreMinOverLread 0 \\\n    --outFilterMatchNminOverLread 0 \\\n    --outFilterMatchNmin 0 \\\n    --sjdbOverhang 49\ndone\n\n# perform quality control on alignment\nmkdir qc_output/star_qc\nmultiqc -o qc_output/star_qc/ --title \"STAR Alignment QC Report\" alignment_output/*Log.final.out\n</code></pre> <p>Explanation of Terms</p> <ul> <li><code>--genomeDir star_index/</code> path to the STAR index</li> <li><code>--outFileNamePrefix alignment_output/${base}_</code> prefix for output bam files</li> <li><code>--outSAMtype BAM SortedByCoordinate</code> sort bam files</li> <li><code>--outSAMunmapped Within</code> output unmapped reads within the main SAM file</li> <li><code>--outSAMattributes Standard</code> output in standard format</li> <li><code>--sjdbGTFfile ./data/sequence.gff3</code> path to the annotation file</li> <li><code>--sjdbOverhang 49</code> maximum read length minus 1 (Helps to determine splice junctions)</li> <li><code>outFilterScoreMinOverLread/outFilterMatchNminOverLread/outFilterMatchNmin</code> set to zero as our reads are very short and will be discarded otherwise. For reads longer than 50 bp feel free to remove these options</li> </ul> <p>You will also note that we ran MultiQC on our STAR output whic delivers the following:</p> <p>STAR QC General Statistics: percent of sequences aligned to reference and the number of sequences aligned</p> <p></p> <p>STAR QC Alignment Scores: How many reads mapped uniquely, to multiple loci, to too many loci, or were unmapped for being too short</p> <p></p> <p>We typically are aiming for 75% or more of our reads to map to our reference. Here we note that only 45-60% of our reads map to our reference. This may be due to the fact that we are only aligning to the chromosome 17 to speed up this tutorial instead of aligning to the whole genome. Additionally, we set <code>outFilterScoreMinOverLread/outFilterMatchNminOverLread/outFilterMatchNmin</code> to 0 as our reads are very short. For reads longer than 50 bp feel free to remove these options from the STAR command. </p>"},{"location":"omics/transcriptomics/bulk_rna_seq/04_read_alignment/#references","title":"References","text":"<ol> <li>STAR: ultrafast universal RNA-seq aligner</li> <li>GENECODE</li> <li>ENSEMBL</li> <li>UCSC</li> <li>Building a genome index</li> </ol>"},{"location":"omics/transcriptomics/bulk_rna_seq/05_gene_quantification/","title":"Gene Quantification","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/05_gene_quantification/#gene-quantification-with-featurecounts","title":"Gene Quantification with featureCounts","text":"<p>Assigning which reads are expressed in which genes essentially gives each gene a \"count\" per sample - or how many reads actually map to that that gene. If a read maps to a gene by as little as one base pair it is counted towards that gene. However, genes can overlap with each other, meaning a read can map to more than one gene (multi-mapping). These reads are discarded as ambiguous reads given that we can't be sure of which gene they come from:</p> <p>featureCounts Overview</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/05_gene_quantification/#gene-quantification","title":"Gene Quantification","text":"<p>Let's create a script to convert our gff3 file to a gft file (necessary for featureCounts), remove any lines that don't have a gene or transcript id, run featureCounts on our alignment data, and perform qc with MultiQC:</p> <p>featurecounts.sh</p> <pre><code>#!/bin/bash\n\n# Step 6: Count features using featureCounts\n\n# convert gff3 to gtf\ngffread ./data/sequence.gff3 -T -o ./data/sequence.gtf\n\n# ensure there are no empty gene ids or transcript ids\ngrep -v 'gene_id \"\"' ./data/sequence.gtf | grep -v 'transcript_id \"gene' &gt; ./data/sequence_fixed.gtf \n\n# run featurecounts\nfeatureCounts \\\n-a ./data/sequence_fixed.gtf \\\n-o featurecounts_output/featurecounts_results.txt \\\nalignment_output/*bam\n\n# run qc on featurecounts run\nmkdir ./qc_output/featurecounts_qc \nmultiqc -o ./qc_output/featurecounts_qc --title \"featureCounts QC Report\" ./featurecounts_output/*.summary\n</code></pre> <p>Explanation of Terms</p> <p>gffread:</p> <ul> <li><code>-T -o ./data/sequence.gtf</code> make a gtf file instead of a gff3 file</li> </ul> <p>grep:</p> <ul> <li><code>grep -v</code> match the opposite of this pattern</li> </ul> <p>featureCounts:</p> <ul> <li><code>-a ./data/sequence_fixed.gtf</code> path to annotation file</li> <li><code>-o featurecounts_output/featurecounts_results.txt</code> path to output file</li> <li><code>alignment_output/*bam</code> path to our alignment input data</li> </ul> <p>multiqc:</p> <ul> <li><code>-o ./qc_output/featurecounts_qc</code> output path</li> <li><code>--title \"featureCounts QC Report\"</code> report title</li> <li><code>./featurecounts_output/*.summary</code> file to use to make report</li> </ul>"},{"location":"omics/transcriptomics/bulk_rna_seq/05_gene_quantification/#featurecounts-quality-control","title":"featureCounts Quality Control","text":"<p>Now let's examine our featureCounts QC:</p> <p>featureCounts General Statistics: percent and number of sequences assigned</p> <p></p> <p>featureCounts General Statistics: percent and number of sequences assigned, unmapped, multi-mapped, unassigned due to no features present, and unassiged due to ambiguity in mapping</p> <p></p> <p>Here we note that very few reads actually mapped to genes due to multi-mapping, where one read matches multiple genes/features. Given that this is subsampled data mapped to only one chromosome, we aren't going to take any steps to fix this. However, in your own RNA-seq analysis, most reads should be assigned to features.</p>"},{"location":"omics/transcriptomics/bulk_rna_seq/05_gene_quantification/#references","title":"References","text":"<ol> <li>featureCounts Overview</li> <li>featureCounts: an efficient general purpose program for assigning sequence reads to genomic features</li> </ol>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/","title":"Differential Expression","text":""},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#differential-expression","title":"Differential Expression","text":"<p>So far we have taken sequencing reads, removed poor quality sequences/adapters, aligned these reads to a reference genome, and counted the aligned reads based on whether or not they mapped to a gene/feature. What we are now left with is a counts table:</p> <p>Reads To Counts - Adapted from HBC DGE Workshop</p> <p></p> <p>We will now take these counts and use a differential expression tool, DESeq2, to normalize our data to ensure that our counts are not biased by things like gene length, sequencing depth, etc. DESeq2 normalizes gene experssion data using the following procedure: </p> <p>DESeq2 Normalization</p> <p></p> <p>DESeq2 also builds a regression model to assess if the slope between two conditions is different. This essentially will tell us, is the gene expression in one group (Alzheimer's patients) higher/lower than another group of patients (Control patients):</p> <p>DESeq2 Regression Modelling</p> <p></p> <p>DESeq2 then will test if this slope is 0 (meaning no difference between Control and AD) or not 0 (meaning there is a difference between Control and AD) with the Wald statistic. This statistic will then be compared to a normal distribution to get a p-value!</p> <p>DESeq2 Wald Statistic</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#loadingcleaning-data","title":"Loading/Cleaning Data","text":"<p>We will now be working with R to analyze our data! So open RStudio and navigate to the folder <code>rna_seq_pipeline</code>. In that folder create a script called <code>differential_expresson.R</code>. We will start by adding the following to the script:</p> <pre><code># --- Load Libraries -----------------------------------------------------------\nlibrary(tidyverse)         # data manipulation/plotting\nlibrary(DESeq2)            # differential expression\nlibrary(EnhancedVolcano)   # creating volcano plots\nlibrary(ggpubr)            # publication ready plotting\nlibrary(clusterProfiler)   # enrichment analysis \n</code></pre> <p>Now we need to some data cleaning. Our featureCounts table has gene information from the gtf file as the first few columns. This will need to be moved to a separate data frame. Additionally, our column names are currently the full name of the bam file. We will be removing the extra characters from these column names. Finally we will make sure our counts data and meta data have a matching sample order:</p> <pre><code># --- Load Data  ---------------------------------------------------------------\n\n# load meta data \nmeta &lt;- read.csv(\n  file = \"./path/to/rna_seq_pipeline/data/meta.txt\",\n  sep=\"\\t\"\n)\n\n# load featureCounts output\nfc_counts &lt;- read.csv(\n  file = \"./path/to/rna_seq_pipeline/featurecounts_output/featurecounts_results.txt\",\n  sep = \"\\t\",\n  skip = 1)\n\n# isolate the feature data and clean up gene name column\nfeatures &lt;- fc_counts %&gt;%\n  dplyr::select(names(fc_counts)[!grepl(\".bam\",names(fc_counts))]) %&gt;%\n  mutate(Gene=gsub(\"gene-\",\"\",Geneid))\n\n# isolate the counts data \ncounts &lt;- fc_counts %&gt;%\n  dplyr::select(names(fc_counts)[grepl(\".bam\",names(fc_counts))])\n\n# clean up names to remove file name info\nnames(counts) &lt;- gsub(\n  \"alignment_output.|_trimmed_Aligned.sortedByCoord.out.bam\",\n  \"\",\n  names(counts))\n\n# order counts columns to match meta data\ncounts &lt;- counts[,meta$ID]\n\n# make the genes the rownames:\ncounts &lt;- cbind(data.frame(Gene=features$Gene),counts)\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#pre-processing","title":"Pre-Processing","text":"<p>Before we get started with DESeq2 we are going to filter out genes with all zero counts and factor our condition variable (Control v. AD). By factoring we specify there is an order to our condition variable with Control being the base:</p> <pre><code># --- Pre-Processing -----------------------------------------------------------\n\n# remove genes with all zero counts:\ncounts &lt;- counts %&gt;%\n  filter(apply(counts[,-1],1,sum) != 0)\n\n# factor our column of interest\nmeta$Diagnosis &lt;- factor(\n  meta$Diagnosis,\n  levels = c(\"Control\",\"AD\")\n)\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#differential-expression_1","title":"Differential Expression","text":"<p>We will be applying a fairly standard DESeq2 pipeline, where we:</p> <ul> <li>create a DESeqDataSet from the counts and meta data</li> <li>specify the model formula Control v. AD</li> <li>run DESeq2</li> <li>shrink the log2FoldChanges to account for differences in standard error</li> <li>convert our results to a data frame</li> </ul> <pre><code># --- Differential Expression --------------------------------------------------\n# create DESeq2 object\ndds &lt;- DESeqDataSetFromMatrix(countData=counts, \n                              colData=meta, \n                              design=~Diagnosis,\n                              tidy = TRUE)\n\n# run DESeq2 on data\ndds &lt;- DESeq(dds)\n\nmessage(\"Differential Expression Analysis Complete\")\n\n# shrink the log2 fold changes\nres &lt;- lfcShrink(dds=dds, coef=2, type=\"apeglm\")\n\n# convert to a data frame\nres &lt;- data.frame(res)\n</code></pre>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#volcano-plot","title":"Volcano Plot","text":"<p>To plot our differentially expressed genes, or DEGs, we will use a volcano plot or a plot of log2 fold change versus the -log10 of the p-value where genes towards the top are more significant:</p> <pre><code># --- Volcano Plot -------------------------------------------------------------\n\n# use the EnhancedVolcano library to plot our differentially expressed genes\nEnhancedVolcano::EnhancedVolcano(\n  toptable = res,                                          # name of results df\n  x = \"log2FoldChange\",                                    # name of log2FC column\n  y=\"pvalue\",                                              # name of p-value column\n  pCutoff = .1,                                            # p-value cutoff\n  FCcutoff = 1e-06,                                        # fold change cutoff\n  xlim = c(                                                # x axis limits\n    min(res$log2FoldChange)*1.25,\n    max(res$log2FoldChange)*1.25),\n  ylim = c(0,                                              # y axis limits\n           -log10(min(res$pvalue))*1.25),                 \n  lab=rownames(res),                                       # gene names                     \n  title = \"Volcano Plot\",                                  # title\n  subtitle = \"Differential Expression of AD RNA-Seq Data\") # subtitle\n</code></pre> <p>Volcano Plot</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#pca-plot","title":"PCA Plot","text":"<p>PCA or Principal Component Analysis is a way of visualizing the variation in high dimensional data in just a few dimensions. For more information, check out our tutorial on PCA. Let's examine the variation in our gene expression data and color by disease status:</p> <pre><code># --- PCA Plot -----------------------------------------------------------------\n\n# perform variance stabilization for pca\nrld &lt;- rlog(dds, blind=FALSE)\n\n# plot pca plot \nplotPCA(object = rld,               # variance stabilized counts data\n        intgroup=c(\"Diagnosis\")) +  # variable to color by\n  ggpubr::theme_pubr(               # ggplot theme to use\n    legend = \"right\"                # where to put the legend\n  )+\n  labs(\n    color=\"Diagnosis\"               # legend color title\n  )\n</code></pre> <p>PCA Plot</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#top-deg-expression","title":"Top DEG Expression","text":"<p>Often times we may want to visualize the expression of our top differentially expressed genes. We can do this using a violin plot!</p> <pre><code># --- Top DEG Expression -------------------------------------------------------\n\n# grab the top DEG\ntop_deg &lt;- res %&gt;%\n  arrange(pvalue) %&gt;%\n  slice_head(n=1) %&gt;%\n  rownames()\n\n# plot this gene's expression split by your condition variable\nggpubr::ggviolin(\n  data=meta %&gt;%\n    mutate(top_deg=assay(rld)[top_deg,]),\n  x=\"Diagnosis\",\n  y=\"top_deg\",\n  fill=\"Diagnosis\",\n  color=\"Diagnosis\",\n  palette = \"npg\",\n  xlab=\"\",\n  ylab = top_deg\n)\n</code></pre> <p>Top DEG Plot</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#enrichment-analysis","title":"Enrichment Analysis","text":"<p>Our most significant DEGs often represent some biological change. We can better understand the biology behind our gene list by first filtering by some significance threshold (usually an adjusted p-value of 0.05, however given how subsampled our data is we are setting a p-value cutoff of .2) and then running an overrepresentation test on genes that are associated with some gene ontology term. Here we use clusterProfiler to perform gene ontology enrichment!</p> <pre><code># --- ClusterProfiler Enrichment -----------------------------------------------\n\n# enrich significant degs\nenriched &lt;- enrichGO(\n  gene = res %&gt;%                   # significant degs that pass some threshold\n    dplyr::filter(pvalue &lt; .2) %&gt;%\n    rownames(),\n  OrgDb = 'org.Hs.eg.db',          # organism your samples belong to\n  keyType = \"SYMBOL\",              # type of gene name\n  ont = \"ALL\",                     # enrichment ontology\n  pvalueCutoff = 0.1,              # pvalue cutoff\n  pAdjustMethod = \"none\",          # pvalue adjustment method\n  universe = rownames(dds),        # what other genes did you test?\n  qvalueCutoff = 0.2,              # qvalue cutoff\n  minGSSize = 2,                   # minimum number of genes per term\n  maxGSSize = 800                  # maxiumum number of genes per term\n)\n\n# plot enrichment\nenrichplot::dotplot(\n  object = enriched,           # enrichment object\n  x='Count',                   # x-axis \n  showCategory = 10,           # how many terms to sho\n  size='Count',                # what to size by\n  color =\"p.adjust\")+          # what to color by\n  scale_color_gradient(\n    low=\"midnightblue\",        # change colors of dotplot\n    high=\"thistle\"\n  )+\n  labs(                         \n    size=\"Count\",              # change size legend title\n    color=\"Adjusted P-Value\"   # change color legend title\n  )\n</code></pre> <p>Enrichment DotPlot</p> <p></p>"},{"location":"omics/transcriptomics/bulk_rna_seq/06_differential_expression/#references","title":"References","text":"<ol> <li>HBC DGE Workshop</li> <li>Differential expression analysis for sequence count data</li> </ol>"},{"location":"omics/transcriptomics/pcr_validation/pcr/","title":"PCR Validation","text":""},{"location":"omics/transcriptomics/pcr_validation/pcr/#selecting-pcr-candidates-from-rna-seq-results","title":"Selecting PCR Candidates from RNA-Seq Results","text":""},{"location":"omics/transcriptomics/pcr_validation/pcr/#overview","title":"Overview","text":"<p>Here we will describe how to select qPCR validation targets from RNA\u2011seq differential expression analysis. </p> <p>We will cover:</p> <ol> <li>Loading RNA\u2011seq data  </li> <li>Differential expression  </li> <li>Enrichment analysis  </li> <li>A step\u2011by\u2011step PCR\u2011target selection workflow:</li> <li>Identify significant DEGs  </li> <li>Ensure genes are well\u2011expressed  </li> <li>Prioritize genes in enriched pathways  </li> <li>Filter to protein coding genes</li> <li>Check for adequate number exons and transcript length</li> <li>Remove outlier\u2011driven changes  </li> </ol> <p>Here we look to provide a biologically informed method to validate RNA\u2011seq findings.</p>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#1-loading-cleaning-data","title":"1. Loading &amp; Cleaning Data","text":""},{"location":"omics/transcriptomics/pcr_validation/pcr/#installing-packages","title":"Installing Packages","text":"<p>If you ever need to install packages I recommend <code>pacman</code>, which will handle a lot of the complexity of installing tools for you:</p> <pre><code>install.packages(\"pacman\")\n\npacman::p_load(\n  tidyverse,\n  tidybulk,\n  DESeq2,\n  ggpubr,\n  clusterProfiler,\n  org.Hs.eg.db,\n  biomaRt\n)\n</code></pre> <pre><code>library(tidyverse)         \nlibrary(tidybulk)          \nlibrary(DESeq2)             \nlibrary(ggpubr)            \nlibrary(clusterProfiler)   \nlibrary(org.Hs.eg.db)      \nlibrary(biomaRt)\n</code></pre> <p>Download and load the SummarizedExperiment object:</p> <pre><code>download.file(\"https://github.com/comp-tox-jhu/CompToxLab/raw/refs/heads/main/docs/omics/transcriptomics/pcr_validation/data/se.rds\",\n              destfile = \"../data/se.rds\")\n\nse &lt;- readRDS(\"../data/se.rds\")\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#what-is-a-summarizedexperiment","title":"What is a SummarizedExperiment","text":"<p>A SummarizedExperiment is a way of storing omics data:</p> <p></p>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#filtering-and-stabilizing-expression","title":"Filtering and Stabilizing Expression","text":"<p>Before assessing our data for differentially expressed genes we should filter out genes that are lowly expressed to avoid noise.</p> <pre><code>se &lt;- se |&gt; \n  identify_abundant() |&gt; \n  keep_abundant()\n</code></pre> <p>Now to prepare for when we plot our data, let's add another assay, or counts matrix, that is variance stabilized. In this way we not only put our data in log scale but address issues with high variance genes tending to have higher means which can distort the expression trends we vizualize later.</p> <pre><code>assay(se,\"vst\") &lt;- vst(assay(se))\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#2-differential-expression-analysis","title":"2. Differential Expression Analysis","text":"<p>We analyze females separately to detect sex\u2011specific transcriptional patterns. We then join the results with our <code>rowData</code>, which contains metadata about our genes, like the HGNC gene symbol.</p> <pre><code>female_res &lt;- se[,se$Sex==\"F\"] |&gt; \n    test_differential_abundance(~condition, \n                                method = \"deseq2\",\n                                fitType = \"local\",\n                                action = \"get\") |&gt; \n    as.data.frame() |&gt; \n    rownames_to_column(\"GeneID\") |&gt; \n    inner_join(rowData(se) |&gt; \n                 as.data.frame() |&gt; \n                 mutate(GeneID=as.character(GeneID)), \n               by = \"GeneID\") |&gt; \n    mutate(direction = ifelse(log2FoldChange &gt; 0, \"Up\", \"Down\"))\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#significant-degs","title":"Significant DEGs","text":"<p>We first apply a significance filter (Adjusted p-value &lt; 0.01, |log2FC| &gt; log2(1)).</p> <pre><code>sig_df &lt;- female_res |&gt; \n  filter(padj&lt;0.01 &amp; abs(log2FoldChange) &gt; log2(1) ) |&gt; \n  filter(!is.na(Symbol))\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#3-enrichment-analysis","title":"3. Enrichment Analysis","text":"<p>Genes in enriched pathways make strong PCR validation targets because they support biological interpretation.</p> <pre><code># enrich significant degs\nenrich &lt;- compareCluster(\n  Symbol~direction,                # formula for clustering\n  data = sig_df,                   # deg data frame\n  fun = \"enrichGO\",                # enrichment function\n  OrgDb = 'org.Hs.eg.db',          # organism your samples belong to\n  keyType = \"SYMBOL\",              # type of gene name\n  ont = \"ALL\",                     # enrichment ontology\n  pvalueCutoff = 0.1,              # p-value cutoff\n  pAdjustMethod = \"fdr\",           # p-value adjustment method\n  universe = female_res$Symbol,    # what other genes did you test?\n  qvalueCutoff = 0.1,              # q-value cutoff\n  minGSSize = 2,                   # minimum number of genes per term\n  maxGSSize = 1000                 # maximum number of genes per term\n)\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#4-pcr-validation-target-selection-workflow","title":"4. PCR Validation Target Selection Workflow","text":"<p>Selecting qPCR targets is not simply choosing the most significant DEGs. It is a multi\u2011step evaluation process balancing statistics, biology, and assay design.</p>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#41-step-1-start-with-significant-robust-degs","title":"4.1 Step 1 - Start With Significant, Robust DEGs","text":"<p>Criteria:</p> <ul> <li>padj &lt; 0.01  </li> <li>|log2FC| &gt; 1.5  </li> <li>baseMean &gt; 20  </li> </ul> <p>PCR requires consistent, strong, highly-expressed differences.</p> <pre><code>pcr_candidates &lt;- sig_df |&gt;\n  filter(\n    padj &lt; 0.01,\n    abs(log2FoldChange) &gt; log2(1.5),\n    baseMean &gt; 20\n  )\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#42-step-2-prioritize-genes-in-enriched-pathways","title":"4.2 Step 2 - Prioritize Genes in Enriched Pathways","text":"<p>Genes belonging to enriched biological processes or pathways are more likely to reflect true biology and provide meaningful validation.</p> <pre><code>enriched_genes &lt;- enrich@compareClusterResult |&gt;\n               separate_rows(geneID,sep = \"/\") |&gt;\n               group_by(geneID) |&gt; \n               reframe(pathway_hit=n()) |&gt; \n               arrange(desc(pathway_hit))\n\npcr_candidates &lt;- pcr_candidates |&gt;\n  inner_join(enriched_genes,\n             by=c(\"Symbol\"=\"geneID\"))\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#43-step-3-filtering-by-gene-metadata","title":"4.3 Step 3 - Filtering by Gene Metadata","text":"<p>Now we will pull additional gene meta data from bioMart to understand:</p> <ul> <li> <p>Is the gene protein coding?</p> </li> <li> <p>Does the gene have more than 2 exons?</p> </li> <li> <p>Is the minimum transcript length over 100 bp?</p> </li> </ul> <pre><code>mart &lt;- useMart(\"ensembl\", dataset = \"hsapiens_gene_ensembl\")\n\ngene_meta &lt;- getBM(\n  attributes = c(\n    \"hgnc_symbol\",\n    \"gene_biotype\",\n    \"ensembl_exon_id\",\n    \"transcript_length\",\n    \"gene_biotype\"\n  ),\n  filters = \"hgnc_symbol\",\n  values = unique(pcr_candidates$Symbol),\n  mart = mart\n) |&gt; \n  group_by(hgnc_symbol) |&gt; \n  reframe(count_exon=length(unique(ensembl_exon_id)),\n          protein_coding=\"protein_coding\" %in% gene_biotype,\n          transcript_length=min(transcript_length)) |&gt; \n  distinct() |&gt; \n  filter(count_exon&gt;2,\n         protein_coding==TRUE,\n         transcript_length&gt;100)\n</code></pre> <p>Now we will filter by these metrics.</p> <pre><code>pcr_candidates &lt;- pcr_candidates |&gt; \n  filter(Symbol %in% gene_meta$hgnc_symbol)\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#44-step-4-rank-genes-by-biological-and-statistical-strength","title":"4.4 Step 4 - Rank Genes by Biological and Statistical Strength","text":"<p>We sort the genes so the most qPCR\u2011ready candidates rise to the top.</p> <pre><code>ranked &lt;- pcr_candidates |&gt;\n  arrange(\n    desc(abs(log2FoldChange)),  # then effect size\n    desc(pathway_hit),          # biological relevance first\n    desc(baseMean),             # then expression level\n    desc(-log10(padj))          # then the p-value\n  )\n</code></pre> <p>Now let's take a look at the top 10 upregulated genes!</p> <pre><code>ranked |&gt; \n  filter(direction == \"Up\") |&gt; \n  dplyr::select(Symbol,log2FoldChange,baseMean,pathway_hit,padj) |&gt; \n  slice_head(n=10)\n</code></pre> <pre><code>Symbol log2FoldChange baseMean pathway_hit padj\n&lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\nHSPA1A  2.455791    35096.8118  23  2.064107e-04\nHSPA1B  2.443789    33144.9815  18  1.303694e-04\nHSPA1L  1.887385    758.4465    1   3.737354e-05\nATF3    1.837993    632.1215    14  6.419790e-08\nDNAJB1  1.830695    9915.0189   6   3.032908e-04\nCXCR4   1.717091    516.1339    14  2.460203e-04\nFOXJ1   1.643526    391.6230    19  3.994640e-03\nSERPINH1    1.642161    1144.9492   1   4.483712e-04\nRGS1    1.616057    905.4349    1   6.102832e-03\nBCL6B   1.548744    207.3333    5   9.924136e-05\n</code></pre> <p>And the top 10 downregulated genes!</p> <pre><code>ranked |&gt; \n  filter(direction == \"Down\") |&gt; \n  dplyr::select(Symbol,log2FoldChange,baseMean,pathway_hit,padj) |&gt; \n  slice_head(n=10)\n</code></pre> <pre><code>Symbol log2FoldChange baseMean pathway_hit padj\n&lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\nPCSK1   -2.205614   258.76353   21  5.346873e-04\nPCDH8   -2.163541   190.43384   19  4.758336e-05\nADCYAP1 -2.158613   158.49512   18  8.329149e-05\nVGF -2.104217   562.34778   28  6.733200e-06\nBDNF    -2.073503   156.44470   34  1.000697e-06\nCARTPT  -2.065815   129.88813   26  2.415623e-04\nCHGB    -2.035318   1504.91848  1   4.556266e-04\nGABRA4  -1.963389   295.29849   67  5.375256e-04\nPPEF1   -1.935684   64.45886    1   1.592429e-05\nOLFM3   -1.929110   308.58967   3   1.136850e-03\n</code></pre>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#45-step-5-check-for-outlierdriven-effects","title":"4.5 Step 5 - Check for Outlier\u2011Driven Effects","text":"<p>RNA\u2011seq log2FC values can be inflated by outlier samples. Before selecting a PCR target, visualize gene counts:</p> <pre><code>#| fig-width: 5\n#| fig-height: 5\n\nse[rowData(se)$Symbol %in% \"PCSK1\",] |&gt; \n  tidybulk() |&gt; \n  ggplot(aes(\n    x=condition,\n    y=vst,\n    fill=condition,\n    color=condition\n  ))+\n  geom_violin(alpha=.7)+\n  geom_jitter(alpha=.3)+\n  geom_pwc()+\n  stat_summary(fun = \"median\",size=1)+\n  theme_pubr(legend = \"right\")+\n  labs(\n    x=\"\",\n    y=\"Vst Exp.\",\n    color=\"\",\n    fill=\"\"\n\n  )\n</code></pre> <p></p> <p>You want:</p> <ul> <li>clean separation between conditions  </li> <li>no single sample driving the difference  </li> <li>replicate consistency  </li> </ul> <p>Let's try the another candidate!</p> <pre><code>#| fig-width: 5\n#| fig-height: 5\n\nse[rowData(se)$Symbol %in% \"VGF\",] |&gt; \n  tidybulk() |&gt; \n  ggplot(aes(\n    x=condition,\n    y=vst,\n    fill=condition,\n    color=condition\n  ))+\n  geom_violin(alpha=.7)+\n  geom_jitter(alpha=.3)+\n  geom_pwc()+\n  stat_summary(fun = \"median\",size=1)+\n  theme_pubr(legend = \"right\")+\n  labs(\n    x=\"\",\n    y=\"Vst Exp.\",\n    color=\"\",\n    fill=\"\"\n\n  )\n</code></pre> <p></p> <p>This one has much cleaner separation between control and AD!</p>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#5-summary","title":"5. Summary","text":"<p>Here is our final checklist for selecting PCR candidats from RNA-seq results.</p> <p>Final checklist:</p> <ul> <li>padj &lt; 0.05  </li> <li>|log2FC| &gt; log2(1.5)  </li> <li>baseMean &gt; 20  </li> <li>in enriched pathways  </li> <li>protein coding</li> <li>adequate exons and transcript length</li> <li>not outlier\u2011driven effects  </li> </ul>"},{"location":"omics/transcriptomics/pcr_validation/pcr/#references","title":"References","text":"<ol> <li>Practical: Organizing data with SummarizedExperiment</li> <li>tidybulk: an R tidy framework for modular transcriptomic data analysis</li> <li>Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2</li> <li>clusterProfiler: an R Package for Comparing Biological Themes Among Gene Clusters</li> <li>Genome wide annotation for Human</li> </ol>"},{"location":"omics/transcriptomics/rna_seq/deg/","title":"Differential Expression Tutorial","text":""},{"location":"omics/transcriptomics/rna_seq/deg/#setup","title":"Setup","text":"<p>First thing is first we need a folder to work with! Open RStudio and create a folder by clicking the folder button:</p> <p>Where can I find the folder button?</p> <p></p> <p>Now call this folder <code>rna_seq_tutorial</code>! Click inside this folder and create a few more folders:</p> <ol> <li><code>data</code></li> <li><code>scripts</code></li> <li><code>results</code></li> </ol> <p>Now let's make sure all the packages we need are installed:</p> <pre><code># Install CRAN packages\ninstall.packages(c(\"tidyverse\", \"R.utils\", \"EnhancedVolcano\", \"ggpubr\"))\n\n# Install Bioconductor manager if needed\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\n\n# Install Bioconductor packages\nBiocManager::install(c(\"DESeq2\", \"clusterProfiler\", \"org.Hs.eg.db\"))\n</code></pre> <p>Great now in console we will download our count data and meta data!</p> <p><pre><code>download.file(\"https://www.ncbi.nlm.nih.gov/geo/download/?type=rnaseq_counts&amp;acc=GSE125583&amp;format=file&amp;file=GSE125583_raw_counts_GRCh38.p13_NCBI.tsv.gz\",destfile = \"data/count_data.tsv.gz\")\n</code></pre> Now we will unzip this file using the <code>gunzip</code> function from <code>R.utils</code>. If you don't have <code>R.utils</code> go ahead and install using <code>install.packages(\"R.utils\")</code>.</p> <p><pre><code>R.utils::gunzip(\"data/count_data.tsv.gz\")\n</code></pre> Great, now let's get our meta data:</p> <pre><code>download.file(\"https://raw.githubusercontent.com/comp-tox-jhu/CompToxLab/refs/heads/main/docs/omics/transcriptomics/rna_seq/data/meta.csv\",destfile = \"data/meta.csv\")\n</code></pre> <p>Now our annotation data:</p> <pre><code>download.file(\"https://www.ncbi.nlm.nih.gov/geo/download/?format=file&amp;type=rnaseq_counts&amp;file=Human.GRCh38.p13.annot.tsv.gz\",destfile = \"data/annot.tsv.gz\")\n</code></pre> <p>And we will uncompress it:</p> <pre><code>R.utils::gunzip(\"data/annot.tsv.gz\")\n</code></pre> <p>Wonderful, now that we have both we can get to work! Let's create a script by going to create a new script by going to <code>File</code> &gt; <code>New File</code> &gt; <code>R Notebook</code>:</p> <p>How can I create a new script?</p> <p></p> <p>When we create a new R markdown there is some helpful text to get you going, let's delete that until all you see is:</p> <pre><code>---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n</code></pre> <p>Now let's change that title to \"RNA-seq Tutorial\". To start we need to make some headers to guide what we will do. We make headers by using hashtags <code>#</code> and the more hashtags the smaller the header. Let's make the following:</p> <pre><code>## Loading/Cleaning Data\n## Pre-Processing\n## Principal Component Analysis \n## Differential Expression\n## Volcano Plots\n## Functional Enrichment\n</code></pre> <p>Great! now under <code>## Setup</code> we will make a code chunk and we can do this using the short cut <code>Cntrl+Alt+I</code> or <code>Cmd+Alt+I</code> on a Mac. in that chunk we will load our libraries:</p> <p><pre><code>library(tidyverse)         # data manipulation/plotting\nlibrary(DESeq2)            # differential expression\nlibrary(EnhancedVolcano)   # creating volcano plots\nlibrary(ggpubr)            # publication ready plotting\nlibrary(clusterProfiler)   # enrichment analysis \n</code></pre> Wonderful with this we can start bringing in data to play with!</p>"},{"location":"omics/transcriptomics/rna_seq/deg/#loadingcleaning-data","title":"Loading/Cleaning Data","text":"<p>To load our data we will be pulling from the <code>readr</code> package from the <code>tidyverse</code>. So make another code chunk under <code>## Setup</code>:</p> <pre><code>counts &lt;- readr::read_tsv(\"../data/count_data.tsv\")\nmeta &lt;- readr::read_csv(\"../data/meta.csv\")\nannot &lt;- readr::read_tsv(\"../data/annot.tsv\")\n</code></pre> <p>If you'll notice, we don't have gene names in our first column of our counts data frame. So we need to map it to our annotation file:</p> <pre><code># remove the first column\ncounts_clean &lt;- counts |&gt; \n  column_to_rownames(\"GeneID\") \n</code></pre> <p>So what we did here is we took the counts data frame, and made the <code>GeneID</code> column the row names. Now let's clean up our meta data:</p> <p><pre><code># remove the first column\nmeta_clean &lt;- meta |&gt; \n  column_to_rownames(\"geo_accession\")\n</code></pre> Now we have our GEO accession number as our rownames, this ensures that the row names of our meta data match the column names of our counts data! But we still need to make sure everything is in the same order:</p> <pre><code># reoder meta data to match the order of the counts data\nmeta_clean &lt;- meta_clean[match(colnames(counts_clean), rownames(meta_clean)), ]\n\n# ensure the order is the same\nall(rownames(meta_clean) == colnames(counts_clean))\n</code></pre> <p>Great! if this says <code>TRUE</code> that means the order of our samples is the same for both the count and meta data! Now let's make a column in our meta data to define our groups:</p> <pre><code># add in a variable for condition\nmeta_clean &lt;- meta_clean |&gt; \n  mutate(condition = case_when(\n    grepl(\"AD\", title) ~ \"AD\",\n    grepl(\"CON \", title) ~ \"Control\"\n  )) |&gt; \n  mutate(condition = factor(condition,levels=c(\"Control\", \"AD\")))\n</code></pre> <p>Here we say we are creating a new column <code>condition</code>, and when another column, <code>title</code> has the pattern <code>AD</code> we put <code>AD</code> as the value, and if the pattern <code>CON</code> is found, we put <code>Control</code>.</p>"},{"location":"omics/transcriptomics/rna_seq/deg/#principal-component-analysis","title":"Principal Component Analysis","text":"<p>Now we are going to store our data in a DESeqDataSet object, this is essentially a fancy list object where we can conveniently hold our data. </p> <pre><code># create DESeq2 object\ndds &lt;- DESeqDataSetFromMatrix(countData=counts_clean, \n                              colData=meta_clean, \n                              design=~condition)\n</code></pre> <p>Nice what we have done is taken count data and a meta data file, defined our formula (~ condition, which is testing for differences between control samples and Alzheimer's disease samples). Before we move forward, let's check the grouping in our dataset. To do this we will use principal component analysis where we generate several linearly uncorrelated components that help us to visualize the variation in our data:</p> <pre><code># perform variance stabilization for pca\nvst &lt;- vst(dds, blind=FALSE)\n\n# plot pca plot \nplotPCA(object = vst,               # variance stabilized counts data\n        intgroup=c(\"condition\")) +  # variable to color by\n  ggpubr::theme_pubr(               # ggplot theme to use\n    legend = \"right\"                # where to put the legend\n  )+\n  labs(\n    color=\"Diagnosis\"               # legend color title\n  )\n</code></pre> <p>PCA Plot by Condition</p> <p></p> <p>Hmmm, this is concerning, there are two very obvious groups in our data, that seem to have nothing to do with diagnosis! Any thoughts what this could be? Well this next PCA plot may shed some light on that:</p> <pre><code># plot pca plot \nplotPCA(object = vst,                           # variance stabilized counts data\n        intgroup=c(\"characteristics_ch1.4\")) +  # variable to color by\n  ggpubr::theme_pubr(                           # ggplot theme to use\n    legend = \"right\"                            # where to put the legend\n  )+\n  labs(\n    color=\"Diagnosis\"                           # legend color title\n  )\n</code></pre> <p>PCA Plot by Sex</p> <p></p> <p>This tells us that sex seems to drive alot of the variation in our dataset. So we should add this as a covariate in our model!</p>"},{"location":"omics/transcriptomics/rna_seq/deg/#differential-expression","title":"Differential Expression","text":"<p>Ok onto the fun part, let's do differential expression! We will be using DESeq2 to perform differential expression and find genes that are differentially expressed between Alzheimer's disease and controls, while accounting for variation due to sex.</p> <pre><code># create DESeq2 object\ndds &lt;- DESeqDataSetFromMatrix(countData=counts_clean, \n                              colData=meta_clean, \n                              design=~condition+characteristics_ch1.4)\n\n# run DESeq2 on data\ndds &lt;- DESeq(dds)\n\n# convert results to a data frame\nres &lt;- data.frame(results(dds))\n</code></pre> <p>Nice what we have done is taken count data and a meta data file, defined our formula (~ condition + characteristics_ch1.4), performed differential expression, and collected our results as a data.frame! However, if you click on the results, you'll notice that <code>GeneID</code> isn't super informative! They are just numbers. To get gene names we will need to map it back to our annotation file:</p> <pre><code># add gene names to the results\nres_mapped &lt;- res |&gt;\n  mutate(GeneID=rownames(res)) |&gt;\n  mutate(GeneID=as.character(GeneID)) |&gt;\n  inner_join(annot |&gt; \n               mutate(GeneID=as.character(GeneID)),\n             by=\"GeneID\")\n</code></pre> <p>Here we took our results made a column named <code>GeneID</code> converted it to a character for mapping,  joined this data frame with the annotation data frame where we also converted the <code>GeneID</code> to a character value, and mapped on the <code>GeneID</code> column. </p>"},{"location":"omics/transcriptomics/rna_seq/deg/#volcano-plots","title":"Volcano Plots","text":"<p>Now that we have mapped our <code>GeneID</code> to the annotation data frame, we can visualize our top differentially expressed genes or DEGs:</p> <pre><code># use the EnhancedVolcano library to plot our differentially expressed genes\nEnhancedVolcano::EnhancedVolcano(\n  toptable = res_mapped,                                   # name of results df\n  x = \"log2FoldChange\",                                    # name of log2FC column\n  y=\"padj\",                                                # name of p-value column\n  pCutoff = 0.05,                                          # p-value cutoff\n  FCcutoff = log2(2),                                      # fold change cutoff\n  lab=res_mapped$Symbol,                                   # gene names                     \n  title = \"Volcano Plot\",                                  # title\n  col = c('black', 'pink', 'purple', 'red3'),              # pallete colors\n  drawConnectors = TRUE,                                   # box the labels\n  subtitle = \"Differential Expression of AD RNA-Seq Data\") # subtitle\n</code></pre> <p>Volcano Plot of DEGs</p> <p></p> <p>Nice! Here we see that GABA receptors, growth factors and integrins appear differentially expressed in Alzheimer's disease samples. </p>"},{"location":"omics/transcriptomics/rna_seq/deg/#functional-enrichment-analysis","title":"Functional Enrichment Analysis","text":"<p>Now individual genes are great but what do these genes represent? For this, we need functional enrichment analysis where we determine if our set of DEGs overlap with gene sets with known functions. We will use <code>clusterProfiler</code> to run this analysis!</p> <pre><code># enrich significant degs\nenrich &lt;- enrichGO(\n  gene = res_mapped |&gt;             # significant degs that pass threshold\n    filter(padj &lt; 0.05) |&gt; \n    filter(abs(log2FoldChange) &gt; log(2)) |&gt;\n    pull(Symbol),\n  OrgDb = 'org.Hs.eg.db',          # organism your samples belong to\n  keyType = \"SYMBOL\",              # type of gene name\n  ont = \"ALL\",                     # enrichment ontology\n  pvalueCutoff = 0.1,              # pvalue cutoff\n  pAdjustMethod = \"fdr\",           # pvalue adjustment method\n  universe = res_mapped$Symbol,    # what other genes did you test?\n  qvalueCutoff = 0.2,              # qvalue cutoff\n  minGSSize = 2,                   # minimum number of genes per term\n  maxGSSize = 800                  # maxiumum number of genes per term\n)\n</code></pre> <p>Now we can visualize these results with a dotplot where the x axis is the number of genes that overlap the y axis is the description and the size of the dots represents the number of genes that overlap and the color represents the p-value:</p> <pre><code>dotplot(\n    object = enrich,             # enrichment object\n    x='Count',                   # x-axis \n    showCategory = 10,           # how many terms to sho\n    size='Count',                # what to size by\n    color =\"p.adjust\")+          # what to color by\n    scale_fill_gradient(\n        low=\"midnightblue\",      # change colors of dotplot\n        high=\"thistle\"\n    )+\n    labs(                         \n        size=\"Count\",            # change size legend title\n        fill=\"Adjusted P-Value\"  # change color legend title\n    )\n</code></pre> <p>Dotplot of Enriched Terms</p> <p></p> <p>Here we see that terms related to synaptic signaling and neurotransmitter mechanics are dysregulated in Alzheimer's disease. </p>"},{"location":"omics/transcriptomics/rna_seq/deg/#references","title":"References","text":"<ol> <li>HBC DGE Workshop</li> <li>Differential expression analysis for sequence count data</li> </ol>"},{"location":"omics/transcriptomics/single_cell/01_introduction/","title":"01 introduction","text":""},{"location":"omics/transcriptomics/single_cell/01_introduction/#introduction","title":"Introduction","text":""},{"location":"omics/transcriptomics/single_cell/01_pipeline/","title":"01 pipeline","text":""},{"location":"omics/transcriptomics/single_cell/01_pipeline/#sinlge-cell-rna-seq-data-analysis","title":"Sinlge Cell RNA-Seq Data Analysis","text":"<pre><code># --- Single-Cell Differential Expression Analysis Workflow --------------------\n# \n# --- Load Libraries -----------------------------------------------------------\nlibrary(Seurat)               # single cell processing\nlibrary(celldex)              # single cell references\nlibrary(SingleR)              # single cell classification\nlibrary(clusterProfiler)      # enrichment analysis\nlibrary(enrichplot)           # enrichment plots\nlibrary(enrichR)              # enrichment + cell classification\nlibrary(tidyverse)            # data manipulation/plotting\nlibrary(ggfortify)            # plotting\nlibrary(patchwork)            # plotting\nlibrary(reshape2)             # manipulating data\nlibrary(ggrepel)              # plotting labels\nlibrary(gridExtra)            # plot arrangement\n\n# --- Load Data ----------------------------------------------------------------\n\n# read in single cell counts data\nsc_counts &lt;- readRDS(\n  \"./consults/wrappers/cleaned_data/sc_counts.rds\"\n) \n\n# read in single cell meta data\nsc_meta &lt;- readRDS(\n  \"./consults/wrappers/cleaned_data/sc_meta.rds\"\n)\ncolnames(sc_counts) &lt;- gsub(\"X\",\"\",colnames(sc_counts))\n\n# --- Set Single Cell Variables ------------------------------------------------\n\n# set single cell object creation variables\nproject = \"asd_organoids\"\nmin.cells = 3\nmin.features = 200\n\n# set initial qc variables\nsplit.by = \"treat\"\nsplit.plot = TRUE\nnFeature_RNA_min = 200\nnFeature_RNA_max = 2500\npercent_mt_thresh = 5\n\n# set highly variable features variables\nselection.method = \"vst\"\nnfeatures = 2000\n\n# clustering variables\nn_dimensions = 10\nchosen_res = \"1.1\"\n\n# cell marker variables\nonly.pos = TRUE\nmin.pct = 0.25\nlogfc.threshold = 0.25\nn_markers = 2\ncustom_markers = c(\"MKI67\",\"TOP2A\",\"BIRC5\",\"HES1\",\"HOPX\",\"TNC\",\"NEUROG1\",\"NHLH1\",\"EOMES\",\"NEUROD2\",\"SYT4\",\"PCP4\",\"BCL11B\",\"FEZF2\",\"SATB2\",\"DLX5\",\"GAD2\",\"SCGN\")\nhpca_label_type = \"label.main\"\ncellmarker_filt = \"Brain|Cortex|Undefined|Embryonic Prefrontal Cortex\"\nchosen_idents = \"cellmarker\"\n\n# set differential expression variables\ntest.use = \"MAST\"\ncondition = \"treat\"\n\n# --- Create Single Cell Object ------------------------------------------------\n\n# create Seurat single cell object\nso &lt;- CreateSeuratObject(\n  counts = sc_counts, \n  project = project, \n  meta.data = sc_meta,\n  min.cells = min.cells, \n  min.features = min.features)\n\n# remove counts/meta data not in the seurat object\nrm(sc_counts,sc_meta)\ngc()\n\n# --- Initial Quality Control --------------------------------------------------\n\n# define a column for mitochondrial content\nso[[\"percent.mt\"]] &lt;- PercentageFeatureSet(so, pattern = \"^MT-\")\n\n# Visualize QC metrics as a violin plot\ninit_qc_plots &lt;- VlnPlot(\n  so, \n  features = c(\"nFeature_RNA\", \n               \"nCount_RNA\",\n               \"percent.mt\"),\n  split.by = split.by,\n  split.plot = split.plot,\n  ncol = 3)\ninit_qc_plots\n\n# filter data based on qc metrics\nso &lt;- subset(so,\n             subset = nFeature_RNA &gt; nFeature_RNA_min &amp; nFeature_RNA &lt; nFeature_RNA_max &amp; percent.mt &lt; percent_mt_thresh)\n\n# --- Normalize the Data -------------------------------------------------------\n\n# log normalize our data\nso &lt;- NormalizeData(so)\n\n# --- Identify Highly Variable Features ----------------------------------------\n\nso &lt;- FindVariableFeatures(so,\n                           selection.method = selection.method,\n                           nfeatures = nfeatures)\n\n# Identify the 10 most highly variable genes\ntop10 &lt;- head(VariableFeatures(so), 10)\n\n# plot variable features with labels\nvariable_feat_plot &lt;- VariableFeaturePlot(so)\nvariable_feat_plot &lt;- variable_feat_plot + \n  ggrepel::geom_label_repel(\n    aes(label=ifelse(rownames(variable_feat_plot$data)  %in% top10,\n                     rownames(variable_feat_plot$data),\n                     \"\")\n    ),\n    max.overlaps=20)\nvariable_feat_plot\n\n\n# --- Scale The Data -----------------------------------------------------------\n\n# isolate gene names\nall.genes &lt;- rownames(so)\n\n# scale data using gene names\nso &lt;- ScaleData(so, features = all.genes)\n\n# --- Perform Dimension Reduction ----------------------------------------------\n\n# run pca on data\nso &lt;- RunPCA(so,\n             features = VariableFeatures(object = so))\n\n# use the elbow/pca heatmaps plots to identify the number of pcs to use\n# pca heatmap\npca_heatmap &lt;- DimHeatmap(so, \n                          dims = 1:15,\n                          cells = 500,\n                          balanced = TRUE)\npca_heatmap\n\n# elbow plot\nelbow_plot &lt;- ElbowPlot(so)\nelbow_plot\n\n# --- Cluster Cells ------------------------------------------------------------\n\n# find nearest neighbors using KNN\nso &lt;- FindNeighbors(so, dims = 1:n_dimensions)\n\n# cluster using the Louvain algorithm\nso &lt;- FindClusters(so, resolution = seq(.4, 1.5,by=.1))\n\n# run umap \nso &lt;- RunUMAP(so, dims = 1:n_dimensions)\n\n# identify single cell resolution columns\nres_cols &lt;- names(so@meta.data)[grepl(\"RNA_snn_res\",names(so@meta.data))]\ninit_umaps &lt;- list()\nfor(res in res_cols){\n  Idents(so) &lt;- so[[res]]\n  init_umap &lt;- DimPlot(so, reduction = \"umap\")+\n    labs(\n      title = res\n    )\n  init_umaps[[res]] &lt;- init_umap\n}\n\n# examine clusters \nn &lt;- length(init_umaps)\nnCol &lt;- floor(sqrt(n))\ndo.call(\"grid.arrange\", c(init_umaps, ncol=nCol))\n\n# choose a resolution\nIdents(so) &lt;- so[[paste(\"RNA_snn_res.\",chosen_res,sep = \"\")]]\nchosen_clusters_umap &lt;- DimPlot(so, reduction = \"umap\")\nchosen_clusters_umap\n\n# set cluster column\nso$cluster &lt;- so[[paste(\"RNA_snn_res.\",chosen_res,sep = \"\")]]\n\n# --- Find Cluster Markers -----------------------------------------------------\n\n# find markers for every cluster compared to all remaining cells, report only the positive\n# ones\nso.markers &lt;- FindAllMarkers(so, \n                             only.pos = only.pos, \n                             min.pct = min.pct, \n                             logfc.threshold = logfc.threshold)\nso.sig.markers &lt;- so.markers %&gt;%\n  group_by(cluster) %&gt;%\n  slice_max(n = n_markers, \n            order_by = avg_log2FC) \n\n# plot marker genes\nmarker_dotplot &lt;- DotPlot(so, \n                          features = unique(so.sig.markers$gene), \n                          dot.scale = 8) +\n  RotatedAxis()\n\nmarker_dotplot\n\n## custom marker dot plot\ncustom_marker_dotplot &lt;- DotPlot(so, \n                                 features = custom_markers, \n                                 dot.scale = 8) +\n  RotatedAxis()\ncustom_marker_dotplot\n\n# ---  Assign the cell types ---------------------------------------------------\n\n# use singler to assign cell types\nhpca.se &lt;- HumanPrimaryCellAtlasData()\nso_counts &lt;- log2(as.matrix(GetAssayData(so))+1)\npred &lt;- SingleR(\n  test = so_counts,\n  ref = hpca.se, \n  assay.type.test=1,\n  labels = hpca.se[[hpca_label_type]])\n\n# add in new labels\nso$singler &lt;- pred$labels\n\n# use cellmarker+enrichr to get cell classification\ncluster_gene_list &lt;- so.markers %&gt;%\n  group_by(cluster) %&gt;%\n  slice_max(n = 100, \n            order_by = avg_log2FC)\ncluster_gene_list &lt;- split(\n  cluster_gene_list$gene,\n  cluster_gene_list$cluster\n)\n\n# run through enrichr\nenriched_cluster_list &lt;- lapply(\n  cluster_gene_list,\n  function(x){\n    enriched=enrichR::enrichr(x,databases = c(\n      enrichR::listEnrichrDbs()$libraryName[grepl(\"CellMarker\",enrichR::listEnrichrDbs()$libraryName)]\n    ));\n    return(enriched)\n  }\n)\n\n# unlist the results and take the top result as the assigned cell_type\ncellmarker &lt;- unlist(\n  lapply(\n    enriched_cluster_list,\n    function(x){\n      unlist(x[[1]][grepl(cellmarker_filt,x[[1]]$Term),]$Term[1])}\n    ))\n\n# convert cell types to identities\nnames(cellmarker) &lt;- levels(so)\nso &lt;- RenameIdents(so, cellmarker)\n\n# add in a column for cellmarker labels and change identity back to \n# chosen resolution\nso$cellmarker &lt;- Idents(so)\nIdents(so) &lt;- so[[paste(\"RNA_snn_res.\",chosen_res,sep = \"\")]]\n\n\n# choose cell identities\nif(chosen_idents == \"singler\"){\n  Idents(so) &lt;- so$singler\n}else if (chosen_idents == \"cellmarker\"){\n  Idents(so) &lt;- so$cellmarker\n}else if (chosen_idents == \"cluster_numbers\"){\n  Idents(so) &lt;- so[[paste(\"RNA_snn_res.\",chosen_res,sep = \"\")]]\n}\n\n# plot chosen cell identities\nchosen_cell_type_umap &lt;- DimPlot(so, reduction = \"umap\", pt.size = 0.5) \nchosen_cell_type_umap\n\n# --- Differential Expression --------------------------------------------------\n\n# pairwise differential expression between individual cell types\n\n# isolate combinations of cell types\ncell_combinations &lt;- expand.grid(levels(so),levels(so))\ncell_combinations &lt;- cell_combinations %&gt;%\n  filter(Var1!=Var2)\ncell_combinations &lt;- unique(apply(cell_combinations, 1, function(x) paste0(sort(x), collapse = \"_\")))\n\n# loop through cell type combinations\n# perform differential expression\nbetween_cell_types &lt;- list()\nfor (cell_comparison in cell_combinations){\n  comp1 = strsplit(cell_comparison,\"_\")[[1]][1]\n  comp2 = strsplit(cell_comparison,\"_\")[[1]][2]\n  de &lt;-FindMarkers(so,  \n                   ident.1 = comp1,\n                   ident.2 = comp2,\n                   min.pct = min.pct, \n                   logfc.threshold = logfc.threshold,\n                   test.use = test.use)\n  between_cell_types[[cell_comparison]] = de\n}\n\n# differential expression between a cell type and everything else\nde_all &lt;-FindAllMarkers(so,  \n                        min.pct = min.pct, \n                        logfc.threshold = logfc.threshold)\n\n# pairwise differential expression between cell types in different conditions\ncondition_cell &lt;- paste(unlist(so[[condition]]),unlist(so[[chosen_idents]]),sep = \"-\")\nIdents(so) &lt;- condition_cell\ncond_cell_combinations &lt;- expand.grid(levels(so),levels(so))\ncond_cell_combinations &lt;- cond_cell_combinations %&gt;%\n  filter(Var1 != Var2)\ncond_cell_combinations &lt;- unique(apply(cond_cell_combinations, 1, function(x) paste0(sort(x), collapse = \"_\")))\n\n# loop through cell type combinations\n# perform differential expression\ncond_between_cell_types &lt;- list()\nfor (cond_cell_comparison in cond_cell_combinations){\n  comp1 = strsplit(cond_cell_comparison,\"_\")[[1]][1]\n  comp2 = strsplit(cond_cell_comparison,\"_\")[[1]][2]\n  de &lt;-FindMarkers(so,  \n                   ident.1 = comp1,\n                   ident.2 = comp2,\n                   min.pct = min.pct, \n                   logfc.threshold = logfc.threshold,\n                   test.use = test.use)\n  cond_between_cell_types[[cond_cell_comparison]] = de\n}\n\n# change identities back\nIdents(so) &lt;- so[[chosen_idents]]\n\n# general bulk condition differential expression\nIdents(so) &lt;- so[[condition]]\ngeneral_bulk_de &lt;-FindMarkers(so,  \n                 ident.1 = levels(so)[1],\n                 ident.2 = levels(so)[2],\n                 min.pct = min.pct, \n                 logfc.threshold = logfc.threshold,\n                 test.use = test.use)\n\n# change identities back\nIdents(so) &lt;- so[[chosen_idents]]\n</code></pre>"},{"location":"omics/transcriptomics/single_cell/02_setup/","title":"02 setup","text":""},{"location":"omics/transcriptomics/single_cell/02_setup/#setting-up-the-analysis-directory","title":"Setting Up The Analysis Directory","text":"<p>To begin we will need a space to work, let's create a directory to house all of our input data, scripts and results:</p> <pre><code>mkdir single_cell_pipeline\ncd single_cell_pipeline\n</code></pre> <p>Now let's make subfolders for our data, scripts and results:</p> <pre><code>mkdir data\nmkdir tools\nmkdir scripts\nmkdir results\n</code></pre>"},{"location":"omics/transcriptomics/single_cell/02_setup/#downloading-data","title":"Downloading Data","text":"<pre><code>cd data\nwget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE175nnn/GSE175814/suppl/GSE175814_RAW.tar\ntar -xvf GSE175814_RAW.tar\nrm GSE175814_RAW.tar\n</code></pre> <p>Moving data to it's own folder and removing the prefix:</p> <pre><code>for i in *_barcodes.tsv.gz\ndo \n    base=$(basename $i _barcodes.tsv.gz)\n    mkdir ${base}\n    mv ${base}_* ${base}/\n    mv ${base}/${base}_matrix.mtx.gz ${base}/matrix.mtx.gz\n    mv ${base}/${base}_barcodes.tsv.gz ${base}/barcodes.tsv.gz\n    mv ${base}/${base}_features.tsv.gz ${base}/features.tsv.gz  \ndone\n</code></pre>"},{"location":"omics/transcriptomics/single_cell/02_setup/#cleaning-da","title":"Cleaning Da","text":""},{"location":"omics/transcriptomics/wgcna/wgcna/","title":"Wgcna","text":""},{"location":"omics/transcriptomics/wgcna/wgcna/#wgcna-workflow","title":"WGCNA Workflow","text":"<pre><code># --- Load Libraries -----------------------------------------------------------\nlibrary(recount3)         # manipulating recount data\nlibrary(tidyverse)        # data manipulation/plotting\nlibrary(factoextra)       # clustering\nlibrary(DGCA)             # differential correlation\nlibrary(WGCNA)            # weighted gene correlation network analysis\nlibrary(patchwork)        # plot arrangement\n\n# --- Load Data ----------------------------------------------------------------\ngtex_brain  &lt;- readRDS(\"./consults/ad_concord/results/gtex_brain.rds\")\n\ncounts &lt;- assay(\n  gtex_brain[,gtex_brain$gtex.smtsd==\"Brain - Amygdala\"],\n  \"counts\")\n\nmeta &lt;- data.frame(\n  colData(gtex_brain[,gtex_brain$gtex.smtsd==\"Brain - Amygdala\"]))\n\nfeatures &lt;- data.frame(\n  rowRanges(gtex_brain[,gtex_brain$gtex.smtsd==\"Brain - Amygdala\"]))\n\nrm(gtex_brain)\ngc()\n\n# --- Set Variables ------------------------------------------------------------\n\n# Filtering variables\nfilterTypes = \"central\"\nfilterCentralType = \"median\"\nfilterCentralPercentile = 0.3\n\n# sample outlier variables\ndist_method=\"euclidean\"\nhclust_method=\"average\"\n\n# removing sample outlier variables\ncutHeight = 50\nminSize = 10\n\n# chosing power variables\npowerVector=c(c(1:10), seq(from = 12, to=20, by=2))\n\n# module building variables\npower = 5\ndeepSplit = 2\nminClusterSize = 30\nhclustMethod=\"ward.D2\"\ncutHeight=0.25\n\n# trait correlation variables\nnumeric_vars = \"numeric.age\"\n\n# module statistics variables\nnumeric_var = \"numeric_age\"\ngene_sig_thresh = 0.2\nmm_thresh = 0.8\n\n# --- Sample Outlier Removal ---------------------------------------------------\n\n# filter out low variance genes\nfilt &lt;- filterGenes(counts,\n                    filterTypes = filterTypes,\n                    filterCentralType = filterCentralType,\n                    filterCentralPercentile = filterCentralPercentile)\n\n# transpose so that the samples are rows and \n# the genes are columns\nfilt &lt;- t(filt)\n\n# apply ward's clustering\nhc &lt;- hclust(d = dist(filt,method = dist_method),\n                 method = hclust_method)\n\n# sample dendrogram\nhc_dendro &lt;- fviz_dend(hc)\n\n# cut tree\nclust = WGCNA::cutreeStatic(hc,\n                            cutHeight = cutHeight,\n                            minSize = minSize)\n\n# filter out outliers\nkeepSamples &lt;- (clust==1)\nfilt &lt;- filt[keepSamples,]\nmeta &lt;- meta[keepSamples,]\n\n# --- Soft-Thresholding Power --------------------------------------------------\n\n# pick soft thresholding power\nsft = WGCNA::pickSoftThreshold(\n  filt, \n  powerVector = powerVector,\n  verbose = 5)\n# plot power v. fit\npower_v_fit &lt;- ggplot(sft$fitIndices,\n                      aes(\n                        x=sft$fitIndices[,1],\n                        y=-sign(sft$fitIndices[,3])*sft$fitIndices[,2],\n                        label=sft$fitIndices[,1]\n                      ))+\n  geom_point(size=0)+\n  theme_bw()+\n  labs(\n    x=\"Soft Thresholding Power\",\n    y=\"Scale Free Topology Model Fit (R^2)\"\n  )+\n  geom_hline(yintercept = 0.9,color=\"red\")+\n  geom_text(hjust=0, vjust=0)\n\n# plot power v. connectivity\npower_v_connectivity &lt;- ggplot(sft$fitIndices,\n                               aes(\n                                 x=sft$fitIndices[,1],\n                                 y=sft$fitIndices[,5],\n                                 label=sft$fitIndices[,1]\n                               ))+\n  geom_point(size=0)+\n  theme_bw()+\n  labs(\n    x=\"Soft Thresholding Power\",\n    y=\"Mean Connectivity\"\n  )+\n  geom_text(hjust=0, vjust=0)\n\ncombined &lt;- power_v_fit|power_v_connectivity\n\n\n# --- Build Modules ------------------------------------------------------------\n\n# calculate adjacency matrix\nadjacency &lt;- adjacency(\n  filt, \n  power = power)\n\n# calculate dissimilarity matrix\nTOM &lt;- TOMsimilarity(adjacency)\nrownames(TOM) &lt;- rownames(adjacency)\ncolnames(TOM) &lt;- colnames(adjacency)\ndissTOM &lt;- 1-TOM\n\n# Call the hierarchical clustering function\ngeneTree &lt;- hclust(\n  as.dist(dissTOM), \n  method = hclustMethod)\n\n# Module identification using dynamic tree cut:\ndynamicMods &lt;- cutreeDynamic(\n  dendro = geneTree, \n  distM = dissTOM,\n  deepSplit = deepSplit, \n  pamRespectsDendro = FALSE,\n  minClusterSize = minClusterSize)\n\n# Convert numeric lables into colors\ndynamicColors &lt;- labels2colors(dynamicMods)\n\n# Calculate eigengenes\nMEList = moduleEigengenes(filt, colors = dynamicColors)\nMEs = MEList$eigengenes\n\n# Calculate dissimilarity of module eigengenes\nMEDiss = 1-cor(MEs)\n\n# Cluster module eigengenes\nMETree = hclust(\n  as.dist(MEDiss),\n  method = hclustMethod)\n\n# Plot the result\n# Plot the cut line into the dendrogram\nplot(METree, \n     main = \"Clustering of module eigengenes\",\n     xlab = \"\",\n     sub = \"\")+\n  abline(h=cutHeight, \n         col = \"red\")\nmodule_eigengene_dendrogram&lt;- grDevices::recordPlot()\nplot.new()\n\n# Call an automatic merging function\nmerge = mergeCloseModules(\n  filt, \n  dynamicColors, \n  cutHeight = cutHeight,\n  verbose = 3)\n\n# The merged module colors\nmergedColors = merge$colors\n\n# Eigengenes of the new merged modules:\nmergedMEs = merge$newMEs\n\n# module dendrogram plot with dynamic and merged clusters\nplotDendroAndColors(\n  geneTree,\n  cbind(dynamicColors, mergedColors),\n  c(\"Dynamic Tree Cut\", \"Merged dynamic\"),\n  dendroLabels = FALSE,\n  hang = 0.03,\n  addGuide = TRUE, \n  guideHang = 0.05)\n\nmodule_dendrogram&lt;- grDevices::recordPlot()\nplot.new()\n\n# Rename to moduleColors\nmoduleColors &lt;- mergedColors\n\n# Construct numerical labels corresponding to the colors\ncolorOrder &lt;- c(\"grey\", standardColors(50))\nmoduleLabels &lt;- match(moduleColors, colorOrder)-1\n\n# --- Trait Information --------------------------------------------------------\n\n# Define numbers of genes and samples\nnGenes &lt;- ncol(filt)\nnSamples &lt;- nrow(filt)\n\n# Recalculate MEs with color labels\nMEs0 &lt;- WGCNA::moduleEigengenes(\n  filt,\n  moduleColors)$eigengenes\n\nMEs &lt;- WGCNA::orderMEs(MEs0)\n\nmoduleTraitCor &lt;- WGCNA::cor(\n  MEs,\n  meta %&gt;% select(numeric_vars),\n  use = \"p\")\n\nmoduleTraitPvalue &lt;- WGCNA::corPvalueStudent(\n  moduleTraitCor, \n  nSamples)\n\nmodule_trait_df &lt;- merge(\n  moduleTraitCor,\n  moduleTraitPvalue,\n  by=\"row.names\")\n\ncolnames(module_trait_df) &lt;- c(\n  paste0(numeric_vars,\".correlation\"),\n  paste0(numeric_vars,\".p.value\")\n)\n\n# Will display correlations and their p-values\ntextMatrix &lt;- paste(signif(moduleTraitCor, 2), \"\\n(\",\n                    signif(moduleTraitPvalue, 1), \")\", sep = \"\");\ndim(textMatrix) &lt;- dim(moduleTraitCor)\n\n# Display the correlation values within a heatmap plot\ndev.off()\npar(mar = c(6, 8.5, 3, 3))\nWGCNA::labeledHeatmap(Matrix = moduleTraitCor,\n                      xLabels = colnames(meta %&gt;% \n                                           dplyr::select(numeric_vars)),\n                      yLabels = names(MEs),\n                      ySymbols = names(MEs),\n                      colorLabels = FALSE,\n                      colors = blueWhiteRed(50),\n                      textMatrix = textMatrix,\n                      setStdMargins = FALSE,\n                      cex.text = .6,\n                      zlim = c(-1,1),\n                      main = paste(\"Module-trait relationships\"))\ntrait_heatmap&lt;- grDevices::recordPlot()\nplot.new()\n\n\n# --- Gene/Connectivity/Module Membership Significance -------------------------\n\nADJ1=abs(cor(filt,use=\"p\"))^6\nAlldegrees1=intramodularConnectivity(ADJ1, moduleColors)\n\nGS1=as.numeric(cor(meta[numeric_var],\n                   filt, \n                   use=\"p\"))\nGeneSignificance=abs(GS1)\n\nconn_gs_plot &lt;- do.call(\n  patchwork::wrap_plots, \n  lapply(unique(moduleColors), function(module) {\n    restrict1 = (moduleColors==module)\n    df &lt;- data.frame(\n      connectivity=Alldegrees1$kWithin[restrict1],\n      gene_significance=GeneSignificance[restrict1]\n    )\n    mm_gs_corr &lt;- round(\n      cor(df$connectivity,\n          df$gene_significance),\n      3)\n    mm_gs_corr_pval &lt;- cor.test(\n      df$connectivity,\n      df$gene_significance)$p.value\n\n    ggplot(df,\n           aes(\n             x=connectivity,\n             y=gene_significance\n           ))+\n      geom_point(color=module)+\n      theme_bw()+\n      geom_smooth(method = \"lm\",\n                  color=module)+\n      labs(\n        x=\"Intramodular Connectivity\",\n        y=\"Gene Significance\"\n      ) +\n      labs(\n        title=paste0(stringr::str_to_title(module),\n                     \" Module\"),\n        subtitle = paste0(\n          \"correlation: \",\n          as.character(mm_gs_corr),\n          \"\\n\",\n          \"p-value: \",\n          as.character(mm_gs_corr_pval))\n      )\n  })) \n\ndatME=moduleEigengenes(filt,\n                       moduleColors)$eigengenes\ndatKME=signedKME(filt,\n                 datME, \n                 outputColumnName=\"MM.\")\n\nconn_mm_plot &lt;- do.call(\n  patchwork::wrap_plots, \n  lapply(unique(moduleColors), function(module) {\n    restrict1 = (moduleColors==module)\n    df &lt;- data.frame(\n      connectivity=Alldegrees1$kWithin[restrict1],\n      module_membership=(datKME[restrict1, paste(\"MM.\", module, sep=\"\")])^6\n    )\n    mm_gs_corr &lt;- round(\n      cor(df$connectivity,\n          df$module_membership),\n      3)\n    mm_gs_corr_pval &lt;- cor.test(\n      df$connectivity,\n      df$module_membership)$p.value\n\n    ggplot(df,\n           aes(\n             x=connectivity,\n             y=module_membership\n           ))+\n      geom_point(color=module)+\n      theme_bw()+\n      geom_smooth(method = \"lm\",\n                  color=module)+\n      labs(\n        x=\"Intramodular Connectivity\",\n        y=\"Module Membership \"\n      ) +\n      labs(\n        title=paste0(stringr::str_to_title(module),\n                     \" Module\"),\n        subtitle = paste0(\n          \"correlation: \",\n          as.character(mm_gs_corr),\n          \"\\n\",\n          \"p-value: \",\n          as.character(mm_gs_corr_pval))\n      )\n  })\n)\n\nsigGenes &lt;- lapply(\n  unique(moduleColors),\n  function(module){\n    FilterGenes= abs(GS1)&gt; gene_sig_thresh &amp; abs(datKME[paste0(\"MM.\",module)])&gt;mm_thresh\n    return(colnames(filt)[FilterGenes])\n  })\nnames(sigGenes) &lt;- unique(moduleColors)\n</code></pre>"},{"location":"programming_languages_tools/programming_languages_tools/","title":"Programming Languages &amp; Tools","text":""},{"location":"programming_languages_tools/programming_languages_tools/#unix","title":"Unix","text":"<ul> <li>Introduction To Unix/Shell</li> <li>Unix Cheatsheet</li> </ul>"},{"location":"programming_languages_tools/programming_languages_tools/#r","title":"R","text":"<ul> <li>Introduction To R</li> <li>Data Manipulation/Visualizaiton in R</li> <li>R Cheatsheet</li> </ul>"},{"location":"programming_languages_tools/programming_languages_tools/#python","title":"Python","text":"<ul> <li>Introduction To Python</li> </ul>"},{"location":"programming_languages_tools/programming_languages_tools/#misc","title":"Misc","text":"<ul> <li>Github</li> <li>API</li> </ul>"},{"location":"programming_languages_tools/r_cheatsheet/","title":"R Cheatsheet","text":"<p>This Cheatsheet is taken Directly From Base R Cheat Sheet</p>"},{"location":"programming_languages_tools/r_cheatsheet/#getting-help","title":"Getting Help","text":"<p>Accessing the help files</p> <pre><code># Get help of a particular function.\n?mean\n# Search the help files for a word or phrase.\nhelp.search(\u2018weighted mean\u2019)\n# Find help for a package. \nhelp(package = \u2018dplyr\u2019)\n</code></pre> <p>More about an object</p> <pre><code># Get a summary of an object\u2019s structure.\nstr(iris)\n# Find the class an object belongs to.\nclass(iris)\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#using-libraries","title":"Using Libraries","text":"<pre><code>install.packages(\u2018dplyr\u2019)\n# Download and install a package from CRAN.\nlibrary(dplyr)\n# Load the package into the session, making all its functions available to use.\ndplyr::select\n# Use a particular function from a package.\ndata(iris)\n# Load a built-in dataset into the environment. \n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#working-directory","title":"Working Directory","text":"<pre><code>getwd()\n# Find the current working directory (where inputs are found and outputs are sent).\nsetwd(\u2018C://file/path\u2019)\n# Change the current working directory. Use projects in RStudio to set the working directory to the folder you are working in. \n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#vectors","title":"Vectors","text":""},{"location":"programming_languages_tools/r_cheatsheet/#creating-vectors","title":"Creating Vectors","text":"<code>c(2, 4, 6)</code> 2 4 6 Join elements into a vector <code>2:6</code> 2 3 4 5 6 An integer sequence <code>seq(2, 3, by=0.5)</code> 2.0 2.5 3.0 A complex sequence <code>rep(1:2, times=3)</code> 1 2 1 2 1 2 Repeat a vector <code>rep(1:2, each=3)</code> 1 1 1 2 2 2 Repeat elements of a vector"},{"location":"programming_languages_tools/r_cheatsheet/#vector-functions","title":"Vector Functions","text":"<pre><code>sort(x)\n# Return x sorted.\nrev(x)\n# Return x reversed.\ntable(x)\n# See counts of values.\nunique(x)\n# See unique values.\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#selecting-vector-elements","title":"Selecting Vector Elements","text":"<p>By Position </p> <pre><code>x[4] # The fourth element.\nx[-4] # All but the fourth.\nx[2:4] # Elements two to four.\nx[-(2:4)] # All elements except two to four.\nx[c(1, 5)] # Elements one and five.\n</code></pre> <p>By Value</p> <p><pre><code>x[x == 10] # Elements which are equal to 10.\nx[x &lt; 0] # All elements less than zero.\nx[x %in% c(1, 2, 5)] # Elements in the set 1, 2, 5.\n</code></pre> Named Vectors</p> <pre><code>x[\u2018apple\u2019] Element with name \u2018apple\u2019.\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#programming","title":"Programming","text":""},{"location":"programming_languages_tools/r_cheatsheet/#for-loop","title":"For Loop","text":"<pre><code>for (variable in sequence){\nDo something\n}\n</code></pre> <p>Example</p> <pre><code>for (i in 1:4){\nj &lt;- i + 10\nprint(j)\n}\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#while-loop","title":"While Loop","text":"<pre><code>while (condition){\nDo something\n}\n</code></pre> <p>Example</p> <pre><code>while (i &lt; 5){\nprint(i)\ni &lt;- i + 1\n}\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#if-statements","title":"If Statements","text":"<pre><code>if (condition){\nDo something\n} else {\nDo something different\n}\n</code></pre> <p>Example</p> <pre><code>if (i &gt; 3){\nprint(\u2018Yes\u2019)\n} else {\nprint(\u2018No\u2019)\n}\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#functions","title":"Functions","text":"<pre><code>function_name &lt;- function(var){\nDo something\nreturn(new_variable)\n}\n</code></pre> <p>Example</p> <pre><code>square &lt;- function(x){\nsquared &lt;- x*x\nreturn(squared)\n}\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#reading-and-writing-data","title":"Reading and Writing Data","text":"Input Ouput Description <code>df &lt;- read.table(\u2018file.txt\u2019)</code> <code>write.table(df, \u2018file.txt\u2019)</code> Read and write a delimited text file. <code>df &lt;- read.csv(\u2018file.csv\u2019)</code> <code>write.csv(df, \u2018file.csv\u2019)</code> Read and write a comma separated value file. This is a special case of read.table and write.table. <code>load(\u2018file.RData\u2019</code>) <code>save(df, file = \u2019file.Rdata\u2019)</code> Read and write an R data file, a file type special for R."},{"location":"programming_languages_tools/r_cheatsheet/#conditions","title":"Conditions","text":"<code>a == b</code> Are equal <code>a &gt; b</code> Greater than <code>a &gt;= b</code> Greater than or equal to <code>is.na(a)</code> Is missing <code>a != b</code> Not equal <code>a &lt; b</code> Less than <code>a &lt;= b</code> Less than or equal to <code>is.null(a)</code> Is null"},{"location":"programming_languages_tools/r_cheatsheet/#data-types","title":"Data Types","text":"<code>as.logical</code> TRUE, FALSE, TRUE Boolean values (TRUE or FALSE). <code>as.numeric</code> 1, 0, 1 Integers or floating point numbers. <code>as.character</code> '1', '0', '1' Character strings. Generally preferred to factors. <code>as.factor</code> '1', '0', '1', levels: '1', '0' Character strings with preset levels. Needed for some statistical models"},{"location":"programming_languages_tools/r_cheatsheet/#maths-functions","title":"Maths Functions","text":"<pre><code>log(x) # Natural log. sum(x) Sum.\nexp(x) # Exponential. mean(x) Mean.\nmax(x) # Largest element. median(x) Median.\nmin(x) # Smallest element. quantile(x) Percentage quantiles.\nround(x, n) # Round to n decimal places.\nrank(x) # Rank of elements.\nsignif(x, n) # Round to n significant figures.\nvar(x) # The variance.\ncor(x, y) # Correlation. \nsd(x) # The standard deviation\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#variable-assignment","title":"Variable Assignment","text":"<pre><code>a &lt;- 'apple'\na\n</code></pre> <p>output</p> <pre><code>[1] 'apple'\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#the-environment","title":"The Environment","text":"<pre><code>ls() # List all variables in the environment.\nrm(x) # Remove x from the environment.\nrm(list = ls()) # Remove all variables from the environment.\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#matrixes","title":"Matrixes","text":"<pre><code>m &lt;- matrix(x, nrow = 3, ncol = 3) # Create a matrix from x.\nm[2, ] #  Select a row\nm[ , 1] #  Select a column\nm[2, 3] #  Select an element\nt(m) # Transpose\nm %*% n#  Matrix Multiplication\nsolve(m, n) # Find x in: m * x = n\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#lists","title":"Lists","text":"<pre><code>l &lt;- list(x = 1:5, y = c('a', 'b')) #  A list is collection of elements which can be of different types.\nl[[2]] # Second element of l.\nl[1] # New list with only the first element. \nl$x  # Element named x. \nl['y'] # New list with only element named y.\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#data-frames","title":"Data Frames","text":"<pre><code>df &lt;- data.frame(x = 1:3, y = c('a', 'b', 'c')) # A special case of a list where all elements are the same length.\ndf$x # grab column of name x\ndf[[2]] # grab second column values\ndf[ , 2] # grab second column\ndf[2, ] # grab second row\ndf[2, 2] # grab the value in the second column and second row\nnrow(df) # number of rows\nncol(df) # number of columns\ndim(df) # number of rows and number of columns\ncbind # bind by columns\nrbind # bind by rows\n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#strings","title":"Strings","text":"<pre><code>paste(x, y, sep = ' ')     # Join multiple vectors together.\npaste(x, collapse = ' ')   # Join elements of a vector together.\ngrep(pattern, x)           #  Find regular expression matches in x.\ngsub(pattern, replace, x). #  Replace matches in x with a string.\ntoupper(x)                 # Convert to uppercase.\ntolower(x)                 # Convert to lowercase.\nnchar(x)                   #  Number of characters in a string. \n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#factors","title":"Factors","text":"<pre><code>factor(x) # Turn a vector into a factor. Can set the levels of the factor and the order.\ncut(x, breaks = 4) # Turn a numeric vector into a factor but \u2018cutting\u2019 into sections. \n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#statistics","title":"Statistics","text":"<pre><code>lm(x ~ y, data=df) # Linear model.\nglm(x ~ y, data=df) # Generalised linear model.\nsummary # Get more detailed information out a model.\nt.test(x, y) # Preform a t-test for difference between means.\npairwise.t.test # Preform a t-test for paired data.\nprop.test # Test for a difference between proportions.\naov # Analysis of variance. \n</code></pre>"},{"location":"programming_languages_tools/r_cheatsheet/#distributions","title":"Distributions","text":"Random Variates Density Function Cumulative Distribution Quantile Normal <code>rnorm</code> <code>dnorm</code> <code>pnorm</code> <code>qnorm</code> Poison <code>rpois</code> <code>dpois</code> <code>ppois</code> <code>qpois</code> Binomial <code>rbinom</code> <code>dbinom</code> <code>pbinom</code> <code>qbinom</code> Uniform <code>runif</code> <code>dunif</code> <code>punif</code> <code>qunif</code>"},{"location":"programming_languages_tools/unix_cheatsheet/","title":"Unix Cheatsheet","text":""},{"location":"programming_languages_tools/unix_cheatsheet/#unix-cheatsheet","title":"Unix Cheatsheet","text":""},{"location":"programming_languages_tools/unix_cheatsheet/#file-commands","title":"File Commands","text":"Command Description ls directory listing ls -al formatted listing with hidden files cd dir change directory to dir cd change to home pwd show current directory mkdir dir create a directory dir rm file delete file rm -r dir delete directory dir rm -f file force remove file rm -rf dir force remove directory dir * cp file1 file2 copy file1 to file2 cp -r dir1 dir2 copy dir1 to dir2; create dir2 if it doesn't exist mv file1 file2 rename or move file1 to file2. if file2 is an existing directory, moves file1 into directory file2 ln -s file link create symbolic link link to file touch file create or update file cat &gt; file places standard input into file more file output the contents of file head file output the first 10 lines of file tail file output the last 10 lines of file tail -f file output the contents of file as it grows, starting with the last 10 lines Process Management"},{"location":"programming_languages_tools/unix_cheatsheet/#process-management","title":"Process Management","text":"Command Description ps display your currently active processes top display all running processes kill pid kill process id pid killall proc kill all processes named proc * bg lists stopped or background jobs; resume a stopped job in the background fg brings the most recent job to foreground fg n brings job n to the foreground"},{"location":"programming_languages_tools/unix_cheatsheet/#file-permissions","title":"File Permissions","text":"<p>chmod guo+ file \u2013 change the permissions of file/folder for the group (g), user (u), other (o)</p> <ul> <li>read (r)</li> <li>write (w)</li> <li>execute (x)</li> </ul> <p>Examples:</p> <ul> <li>chmod gu+rwx file \u2013 read, write, execute for everyone in the group and user (file)</li> <li>chmod -R gu+rwx folder \u2013 read, write, execute for everyone in the group and user (folder)</li> <li>chmod g-w - remove write for everyone in the group</li> </ul>"},{"location":"programming_languages_tools/unix_cheatsheet/#ssh","title":"SSH","text":"Command Description ssh user@host connect to host as user ssh -p port user@host connect to host on port, port as user ssh-copy-id user@host add your key to host for user to enable a keyed or passwordless login"},{"location":"programming_languages_tools/unix_cheatsheet/#searching","title":"Searching","text":"Command Description grep pattern files search for pattern in files grep -r pattern dir search recursively for pattern in dir command grep pattern locate file find all instances of file"},{"location":"programming_languages_tools/unix_cheatsheet/#system-information","title":"System Information","text":"Command Description date cal show this month's calendar uptime show current uptime w display who is online whoami who you are logged in as finger user \u2013 display information about user uname -a show kernel information cat /proc/cpuinfo cpu information cat /proc/meminfo memory information man command show the manual for command df show disk usage du show directory space usage free show memory and swap usage whereis app show possible locations of app which app show which app will be run by default"},{"location":"programming_languages_tools/unix_cheatsheet/#compression","title":"Compression","text":"Command Description tar cf file.tar files create a tar named file.tar containing files tar xf file.tar extract the files from file.tar tar czf file.tar.gz files create a tar with Gzip compression tar xzf file.tar.gz extract a tar using Gzip tar cjf file.tar.bz2 create a tar with Bzip2 compression tar xjf file.tar.bz2 extract a tar using Bzip2 gzip file compresses file and renames it to file.gz gzip -d file.gz decompresses file.gz back to file"},{"location":"programming_languages_tools/unix_cheatsheet/#network","title":"Network","text":"Command Description ping host ping host and output results whois domain get whois information for domain dig domain get DNS information for domain dig -x host reverse lookup host wget file download file wget -c file continue a stopped download"},{"location":"programming_languages_tools/unix_cheatsheet/#shortcuts","title":"Shortcuts","text":"Command Description Ctrl+C halts the current command Ctrl+Z stops the current command, resume with fg in the foreground or bg in the background Ctrl+D log out of current session, similar to exit Ctrl+W erases one word in the current line Ctrl+U erases the whole line Ctrl+R type to bring up a recent command !! repeats the last command exit log out of current session"},{"location":"programming_languages_tools/unix_cheatsheet/#references","title":"References","text":"<ol> <li>Unix/Linux Command Reference</li> </ol>"},{"location":"programming_languages_tools/conda/conda_environment/","title":"Conda Environments","text":""},{"location":"programming_languages_tools/conda/conda_environment/#conda-environments","title":"Conda Environments","text":"<p>For reproducible research it is advisable to keep the software versions you use consistent. An easy way of ensuring this is by creating a Conda environment, where you can explicitly say which versions are necessary to run your  pipeline:</p> <p>Conda Environments</p> <p></p>"},{"location":"programming_languages_tools/conda/conda_environment/#downloading-anaconda","title":"Downloading Anaconda","text":"<p>To build a conda environment, you will need the Anaconda Python distribution. You can download this at the following link:</p> <p>Download Anaconda</p>"},{"location":"programming_languages_tools/conda/conda_environment/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>Once you have downloaded Anaconda, go to your terminal (Terminal for Macs and Mobaxterm is recommended for PC). Now in the terminal create your environment with the desired version of python:</p> <pre><code>conda create -n yourenvname python=3.8\n</code></pre> <p>Once created you can activate your environment like so:</p> <pre><code>conda acvitate yourenvname\n</code></pre>"},{"location":"programming_languages_tools/conda/conda_environment/#installing-tools","title":"Installing Tools","text":"<p>To install tools in your conda environment use the following syntax:</p> <pre><code>conda install yourpackage\n</code></pre> <p>Check out a list of available tools here:</p> <p>!!! info \"Anaconda Packages</p> <p>Or if there is no available conda package you can also try installing these tools with pip:</p> <pre><code>pip install yourpackage\n</code></pre> <p>To see what's installed in your conda environment use:</p> <pre><code>conda list\n</code></pre>"},{"location":"programming_languages_tools/conda/conda_environment/#deactivating-an-environment","title":"Deactivating An Environment","text":"<p>When you are finished using your environment, deactivate the environment with:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"programming_languages_tools/conda/conda_environment/#installing-a-conda-environment-from-a-file","title":"Installing A Conda Environment From a File","text":"<p>When you run an analysis using a conda environment, you can pass that environment onto others by exporting the tool versions to a yml file with the following command:</p> <pre><code>conda env export -n yourenvname -f yourenvname.yml --no-builds\n</code></pre> <p>Here we add no <code>--no-builds</code> because the environment you build is often specific to the machine you built it on. If you build the environment on Mac and want others to use it on a PC or linux platform it is advisable you export it without the \"build\" information your specific machine used.</p>"},{"location":"programming_languages_tools/conda/conda_environment/#references","title":"References","text":"<ol> <li>Manage Environments</li> <li>Conda Instructions</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/","title":"Introduction","text":""},{"location":"programming_languages_tools/intro_to_python/01_introduction/#introduction-to-jupyterlab","title":"Introduction to JupyterLab","text":"<p>Jupyterlab is a web-based user interface to run Python code and is not a traditional Integrated Development Environment (IDE) where you create scripts via some text editor and then submit directly to command line. JupyterLab has several advantages, including being able to run code in chunks, annotating code with links, and displaying figures right next to code! For this reason, JupyterLab is a robust tool for script development/data analysis. </p>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/#downloading-anacondajupyterlab","title":"Downloading Anaconda/JupyterLab","text":"<p>To download JupyterLab you will need to download the Anaconda distrubution of conda:</p> <p>Download Anaconda/JupyterLab</p> <p>Follow the instructions to download Anaconda. Once downloaded, open the Anaconda-Navigator and click <code>Install</code> in the JupyterLab tab. When it is ready you will see a launch button which you will click:</p> <p>Open JupyterLab</p> <p></p> <p>When you open JupyterLab you will notice:</p> <ul> <li>Left Sidebar: containing your file browser, list of running kernels/terminals, table of contents, extension manager</li> <li>Main Work Area: containing options for file/window types to open (ipykernels, terminal environments, text files, markdown files, and python files)</li> </ul> <p>JupyterLab Layout</p> <p></p> <p>We are going to start by opening up a <code>.ipynb</code> file by clicking <code>Notebook Python 3 (ipykernel)</code>. These are not python scripts, but notebook files that contain code but also text, links and images. These files can easily be converted to a python script (file ending in <code>.py</code>) by going to:</p> <ul> <li><code>File</code></li> <li><code>Download as</code></li> <li><code>Python (.py)</code></li> </ul> <p>For now let's work in the Jupyter notebook (<code>.ipynb</code> file)!</p>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/#code-vs-markdown","title":"Code Vs. Markdown","text":"<p>You will notice when you open up your notebook that you are working in blocks:</p> <p>Code/Markdown Blocks</p> <p></p> <p>These blocks can either be:</p> <ul> <li>raw blocks: raw data that can be converted into HTML/Latex formats</li> <li>code blocks: python code that can be run in chunks</li> <li>markdown blocks: a plain text format that can render links, lists, and images like what you might find on a website</li> </ul> <p>Here we will focus on code blocks to run chunks of python code, and markdown blocks which can add in images, links, etc. to annotate our code.</p>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/#markdown-basics","title":"Markdown Basics","text":"<p>markdown code:</p> <p><pre><code>- list item 1\n- list item 2\n</code></pre> output: - list item 1 - list item 2</p> <p>markdown code:</p> <p><pre><code>1. numbered list item 1\n2. numbered list item 2\n</code></pre> output: 1. numbered list item 1 2. numbered list item 2</p> <p>markdown code:</p> <p><pre><code># Level 1 Heading\n## Level 2 Heading\n</code></pre> output:</p>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/#level-1-heading","title":"Level 1 Heading","text":""},{"location":"programming_languages_tools/intro_to_python/01_introduction/#level-2-heading","title":"Level 2 Heading","text":"<p>markdown code:</p> <p><pre><code>[google link](https://www.google.com/)\n</code></pre> output: google link</p> <p>Now that we have a basic understanding of markdown, let's create some annotations. In your first code block change the type to markdown and enter:</p> <pre><code># Introduction to Python \n\nHere are a few helpful links to get started:\n\n- [Python Cheatsheet](https://www.pythoncheatsheet.org/cheatsheet/basics)\n- [JupyterLab Documentation](https://jupyterlab.readthedocs.io/en/stable/)\n</code></pre> <p>Now hit either the play button at the top of the screen or hit <code>Shift + Enter</code> to run the block:</p> <p>Running A Markdown Block</p> <p></p>"},{"location":"programming_languages_tools/intro_to_python/01_introduction/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/","title":"Variables/Data Types","text":""},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#variables","title":"Variables","text":"<p>In Python, we store values using names called variables. We can assign a variable with an <code>=</code> sign:</p> <p><pre><code>max_coverage = 6000\nminCoverage = 35\nantibiotic = 'Streptomycin'\nantibiotic2 = 'Penicillin'\n</code></pre> You will note a few things about variables:</p> <ul> <li>can incorporate letters, digits and underscores</li> <li>cannot start with a digit</li> <li>these are case sensitive</li> </ul> <p>Variables, once we assign them to some value, can be passed into functions to accomplish certain tasks. Functions, generally speaking, take in some input and spit out some output. Let's use the simplist use case, the <code>print()</code> function:</p> <pre><code>print('The maximum coverage is ', max_coverage)\n</code></pre> <p>output</p> <pre><code>The maximum coverage is  6000\n</code></pre> <p>Here the function <code>print()</code> took in two character values and printed a combined string of words.</p> <p>Note</p> <p>variables are available to use between blocks. However, the order in which you run blocks matters so make sure to run your code blocks in order!</p>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#data-types","title":"Data Types","text":"<ul> <li><code>integer</code>: a positive/negative whole number (34, -675)</li> <li><code>float</code>: a floating point number (4.67, -2034.67)</li> <li><code>string</code>: a character string written with either single or double quotes ('Streptomycin', \"antibiotic\")</li> <li><code>bool</code>: a TRUE/FALSE value</li> </ul> <p>So you have a variable, how do you determine the type? Well we can use the <code>type()</code> function:</p> <pre><code>type(max_coverage)\n</code></pre> <p>output</p> <pre><code>int\n</code></pre> <p>If you want to convert between data types you can specify with the following functions:</p> <ul> <li><code>int()</code>: to convert to an integer</li> <li><code>float()</code>: to convert to a floating point number</li> <li><code>str()</code>: to convert to a string</li> </ul>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#calculations","title":"Calculations","text":"<p>You can use Python like a calculator using the following symbols:</p> Operator Name Example + Addition x + y - Subtraction x - y * Multiplication x * y / Division x / y % Modulus x % y ** Exponentiation x ** y // Floor division x // y <p>Let's try an few example:</p> <pre><code>35 / 7 - 5 + 4 * 4 + 2**2\n</code></pre> <p>output</p> <pre><code>20.0\n</code></pre> <p>We note that Python calculations follow the order of operations when performing a calculation. We should also bring up two non-standard operations that you may or may not be familiar with: Modulus and Floor division. Modulus is the remainder after division so:</p> <pre><code>7 % 2\n</code></pre> <p>output</p> <pre><code>1\n</code></pre> <p>Floor division is a division operation for which you round the result down to a whole number:</p> <pre><code>7 // 2\n</code></pre> <p>output</p> <pre><code>3\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#strings-operators","title":"Strings &amp; Operators","text":"<p>You can use <code>+</code> and <code>*</code> with string data as well to add and multiply, take for instance:</p> <pre><code>antibiotic + antibiotic\n</code></pre> <p>output</p> <pre><code>'StreptomycinStreptomycin'\n</code></pre> <pre><code>antibiotic * 4\n</code></pre> <p>output</p> <pre><code>'StreptomycinStreptomycinStreptomycinStreptomycin'\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#indexing","title":"Indexing","text":"<p>Unlike the other data types, strings have lengths. We can use the <code>len()</code> function to  check how long  a string is:</p> <pre><code>print(antibiotic)\n</code></pre> <p>output</p> <pre><code>'Streptomycin'\n</code></pre> <pre><code>len(antibiotic)\n</code></pre> <p>output</p> <pre><code>12\n</code></pre> <p>We can slice strings if needed to! However, the letters you are grabbing are zero-indexed meaning that the first letter is letter 0, the second letter is letter 1, and so on:</p> <pre><code>antibiotic[0]\n</code></pre> <p>output</p> <pre><code>'S'\n</code></pre> <pre><code>antibiotic[1]\n</code></pre> <p>output</p> <pre><code>'t'\n</code></pre> <p>We can grab more letters using the format <code>[start:stop]</code>:</p> <pre><code>antibiotic[1:5]\n</code></pre> <p>output</p> <pre><code>'trep'\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#comments","title":"Comments","text":"<p>When assigning variables we can add descriptions to our code to give our code context. We do this by writing our description after a <code>#</code> symbol:</p> <pre><code># creating a variable for time of day\ntime_of_day = 'Morning'\n</code></pre> <p>Everything after the <code>#</code> is not processed as Python code even within a code block in a Jupyter notebook.</p>"},{"location":"programming_languages_tools/intro_to_python/02_variables-data-types/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/03_libraries-data-frames/","title":"Libraries","text":"<p>Libraries are collections of functions called modules that can be imported and used in your script. Let's use the <code>math</code> library to grab constants:</p> <pre><code>import math\nmath.e\n</code></pre> <p>output</p> <pre><code>2.718281828459045\n</code></pre> <p>Now how about functions:</p> <pre><code>math.log2(25)\n</code></pre> <p>output</p> <pre><code>4.643856189774724\n</code></pre> <p>We want to point out that here we call the value <code>e</code> after <code>math</code>. This is called a member value. On the contrary we see that <code>.log2()</code> comes after <code>math</code>. The parentheses make this a method or function in a given package. </p> <p>Tip</p> <p>If you ever need assistance with a library, try using the <code>help()</code> function to grab more information (e.g. <code>help(math)</code>).</p>"},{"location":"programming_languages_tools/intro_to_python/03_libraries-data-frames/#importing-parts-of-libraries-using-aliases","title":"Importing Parts of Libraries &amp; Using Aliases","text":"<p>Sometimes you'll only need a few things from a library. To grab just those few things use the following approach:</p> <pre><code>from math import log2, e\nmath.log2(25)\n</code></pre> <p>output</p> <pre><code>4.643856189774724\n</code></pre> <p>Now sometimes the name of a library is just too long to continuously type out. For this we can use an alias</p> <pre><code>from math import log2 as l2\nmath.l2(25)\n</code></pre> <p>output</p> <pre><code>4.643856189774724\n</code></pre> <p>Here we abbreviate <code>log2</code> from the <code>math</code> package to <code>l2</code>.</p>"},{"location":"programming_languages_tools/intro_to_python/03_libraries-data-frames/#importing-and-inspecting-data-frames","title":"Importing and Inspecting Data Frames","text":"<p>In data analysis we often work with tabular data, or two dimensional data with columns and rows. Columns will typically contain the same type of data and rows will be one sample with different observations. We commonly read in tabular data using the <code>pandas</code> module:</p> <pre><code>import pandas as pd\nimport csv\ndf = pd.read_csv('/cluster/tufts/bio/tools/training/intro-to-r/metadata.tsv' , sep = '/t', engine = 'python')\nprint(df)\n</code></pre> <p>output</p> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452\n</code></pre> <p>If we want to inspect this data frame we can use a few useful commands. To get a quick summary of the data frame we can use:</p> <pre><code>df.info() # reveals that we have 6 columns, 9 rows, uses 504.0+ bytes of memory, and has one integer column\n</code></pre> <p>output</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 9 entries, 1 to 9\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   SampleID                 9 non-null      object\n 1   AntibioticUsage          9 non-null      object\n 2   DaySinceExperimentStart  9 non-null      object\n 3   Genotype                 9 non-null      object\n 4   Description              9 non-null      object\n 5   OtuCount                 9 non-null      int64 \ndtypes: int64(1), object(5)\nmemory usage: 504.0+ bytes\n</code></pre> <p>To get column names:</p> <pre><code>df.columns\n</code></pre> <p>output</p> <pre><code>Index(['SampleID', 'AntibioticUsage', 'DaySinceExperimentStart', 'Genotype',\n       'Description', 'OtuCount'],\n      dtype='object')\n</code></pre> <p>To get row names:</p> <pre><code>df.index\n</code></pre> <p>output</p> <pre><code>Int64Index([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n</code></pre> <p>To transpose (flip the columns and rows) the data frame:</p> <pre><code>df.T\n</code></pre> <p>output</p> <pre><code>                                                 1                          2  \nSampleID                                  WT.unt.1                   WT.unt.2   \nAntibioticUsage                               None                       None   \nDaySinceExperimentStart                       DAY0                       DAY0   \nGenotype                                        WT                         WT   \nDescription              16S_WT_unt_1_SRR2627457_1  16S_WT_unt_2_SRR2627461_1   \nOtuCount                                      1174                       1474   \n</code></pre> <p>If we wanted a numeric summary we can use:</p> <pre><code>df.describe()\n</code></pre> <p>output</p> <pre><code>          OtuCount\ncount     9.000000\nmean    777.777778\nstd     600.526806\nmin     175.000000\n25%     279.000000\n50%     452.000000\n75%    1451.000000\nmax    1492.000000\n</code></pre> <p>Note</p> <p>the <code>.describe()</code> method will only summarizes numeric data. So here we only have one column of numeric data that get's summarized.</p>"},{"location":"programming_languages_tools/intro_to_python/03_libraries-data-frames/#data-manipulation","title":"Data Manipulation","text":"<p>Say you want to grab certain values in a data frame using the number location. So the value in the second row and third column:</p> <pre><code>df.iloc[2,3]\n</code></pre> <p>output</p> <pre><code>'WT'\n</code></pre> <p>Here we see that the formula to grab values is <code>[row, column]</code>. If we wanted to use row/column names to specify the value:</p> <pre><code>df.loc[1,\"OtuCount\"]\n</code></pre> <p>output</p> <pre><code>1174\n</code></pre> <p>To grab all values in a row or column we use <code>:</code> to specify every value:</p> <pre><code>df.loc[:,\"OtuCount\"]\n</code></pre> <p>output</p> <pre><code>1    1174\n2    1474\n3    1492\n4    1451\n5     314\n6     189\n7     279\n8     175\n9     452\nName: OtuCount, dtype: int64\n</code></pre> <p>We can also subset our data with a few operators:</p> Operator Description &gt; greater than &gt;= greater than or equal &lt; less than &lt;= less than or equal == equals != not equal &amp; and | or <p>Let's go through a few of these:</p> <pre><code>df[df[\"AntibioticUsage\"] == \"None\"]    # select samples with no antibiotic useage\n</code></pre> <p>output</p> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n</code></pre> <pre><code>df[df[\"OtuCount\"] &gt; 400]   # select samples with an otu count over 400\n</code></pre> <p>output</p> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n9   WT.day3.9   Streptomycin    DAY3    WT  16S_WT_day3_9_SRR2628504_1  452\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/03_libraries-data-frames/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/","title":"Plotting Data","text":""},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#plotting-with-plotly","title":"Plotting with Plotly","text":"<p>While there are other plotting libraries, we will focus on <code>plotly</code> for the following reasons:</p> <ul> <li>has the ability to zoom </li> <li>images can be downloaded as <code>png</code> files</li> <li>select features can highlight features of the plot</li> </ul>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#basic-plot","title":"Basic Plot","text":"<p>Let's make a scatterplot:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,                      # the data we are using\n                 x=\"Day\",                 # x axis data\n                 y=\"OtuCount\",            # y axis data\n                 color='Day',             # how to color our data\n                 template=\"simple_white\") # what theme we would like\nfig.show()\n</code></pre> <p>Scatterplot</p> <p></p>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#adding-a-trendline","title":"Adding A TrendLine","text":"<p>We can add a trend line as well:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,\n                 x=\"Day\",\n                 y=\"OtuCount\",\n                 color='Day',\n                 template=\"simple_white\",\n                 trendline=\"ols\")         # add in a trend line\nfig.show()\n</code></pre> <p>Adding A Trend Line</p> <p></p>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#scaling","title":"Scaling","text":"<p>Now if one of your axes spans multiple magnitudes you can scale your data using the <code>log_x</code> or <code>log_y</code> arguements:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 trendline=\"ols\",\n                 log_y = True)             # scale y axis\nfig.show()\n</code></pre> <p>Scaling</p> <p></p>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#panels","title":"Panels","text":"<p>Sometimes it is useful to separate data by some variable and create panels. We can easily do this by specifying the <code>facet_row</code> or <code>facet_col</code> arguements - where plots are stacked one on top of the other or side-by-side, respectively:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 facet_col = \"DaySinceExperimentStart\") # split plots by variable\nfig.show()\n</code></pre> <p>Adding Panels</p> <p></p>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#modifying-text","title":"Modifying Text","text":"<p>To modify text we can use the <code>labels</code> and <code>title</code> option:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 labels={                        \n                     \"OtuCount\": \"OTU count\"     # add in a space and capitalize\n                 },\n                 title = \"Figure 1\")             # add in figure title\nfig.show()\n</code></pre> <p>Modifying Text</p> <p></p> <p>Tip</p> <p>For more plots and plot customization options, checkout the Plotly Graphing Library Page for more information</p>"},{"location":"programming_languages_tools/intro_to_python/04_plotting-plotly/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/05_lists/","title":"Lists","text":"<p>A data frame is not the only way to store data, we can also create lists of values which can be the same data type or different data types. Here is an example:</p> <pre><code>coverage = [200, 34, 900, 423, 98, 789]\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/05_lists/#grabbing-list-values","title":"Grabbing List Values","text":"<p>We can also grab these values by their index, which again are zero-indexed (meaning they start at zero). Here is an example of grabbing the 3rd item in the list:</p> <pre><code>coverage[2]\n</code></pre> <p>output</p> <pre><code>900\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/05_lists/#addingdeleting-values","title":"Adding/Deleting Values","text":"<p>To add values we can use the <code>.append()</code> method to add items to the end of a list:</p> <pre><code>coverage.append(542)\ncoverage\n</code></pre> <p>output</p> <pre><code>[200, 34, 300, 423, 98, 789, 542]\n</code></pre> <p>Additionally, we can also remove items from a list as well with the <code>del</code> statement:</p> <pre><code>del coverage[3]\ncoverage\n</code></pre> <p>output</p> <pre><code>[200, 34, 300, 98, 789, 542]\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/05_lists/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/","title":"Loops/Conditionals","text":""},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#loops","title":"Loops","text":"<p>Loops perform some operation on a value in a set of values. Let's go through an example using our <code>coverage</code> list from the previous note:</p> <pre><code>for i in coverage:\n    print(i)\n</code></pre> <p>output</p> <pre><code>200\n34\n300\n98\n789\n542\n</code></pre> <p>Here we see that <code>i</code> is a substitute for some value in the sequence provided - in this case 200, <code>34, 300, 98, 789, 542</code>. </p>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#nested-loops","title":"Nested Loops","text":"<p>Loops can also nested where a loop is placed inside a loop:</p> <pre><code>for i in [1,2]:\n    for j in coverage:\n        print(j*2)\n</code></pre> <p>output</p> <pre><code>200\n34\n300\n98\n789\n542\n400\n68\n600\n196\n1578\n1084\n</code></pre> <p>Here we move through the loop and for every value in the first list (<code>[1,2]</code>), Then for each pass of the first loop we move through values the second list (<code>[200, 34, 300, 98, 789, 542]</code>). Finally for each value <code>i</code> we then multiply by each value <code>j</code>. </p>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#pass-statement","title":"Pass Statement","text":"<p>If you want a placeholder for your loop, meaning no operation is performed, use the <code>pass</code> statement:</p> <pre><code>for i in coverage:\n    pass\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#conditionals","title":"Conditionals","text":"<p>If we were interested in performing some operation on a value only if a condition is met, we can use an <code>if</code> statement:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    else:\n        pass\n</code></pre> <p>output</p> <pre><code>789\n542\n</code></pre> <p>Here we use the comparison operators we mentioned in the Libraries &amp; Data Frames Topic Note to only print values in <code>coverage</code> if they are larger than <code>500</code>.</p>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#multiple-conditionals","title":"Multiple Conditionals","text":"<p>To perform operations based on multiple conditions you can add in <code>elif</code> statements:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    elif i &lt; 500:\n        print('This value is less than 500')\n</code></pre> <p>output</p> <pre><code>This value is less than 500\nThis value is less than 500\nThis value is less than 500\nThis value is less than 500\n789\n542\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/06_loops-conditionals/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_python/07_functions-scope/","title":"Functions/Scope","text":""},{"location":"programming_languages_tools/intro_to_python/07_functions-scope/#functions","title":"Functions","text":"<p>So far we have used functions in base python or python modules. But what if we want to create our own? Well here is the general formula to do so:</p> <pre><code># load the module we need\nimport numpy as np \n\n# define a function to return geometric mean\ndef geometric_mean(values):\n    return np.exp(np.mean(np.log(values)))\n\n# call our function\ngeometric_mean(coverage)                 \n</code></pre> <p>output</p> <pre><code>209.88855396892262\n</code></pre> <p>Here we see that we use <code>def</code> to define the function and <code>return</code> to specify what value you'd like to return. We then call our function and use our <code>coverage</code> variable as our set of values. The geometric mean of the set of values in <code>coverage</code> are then returned.</p>"},{"location":"programming_languages_tools/intro_to_python/07_functions-scope/#function-documentation","title":"Function Documentation","text":"<p>To clarify the purpose of your function you can add a multiline string to your function using three quotes <code>'''</code>:</p> <pre><code># define a function to return geometric mean\ndef geometric_mean(values):\n    ''' This function takes a list of\n    values and returns the geometric mean \n    of those values'''\n    return np.exp(np.mean(np.log(values)))\n</code></pre> <p>This multiline string is also accessible when we run our function through the <code>help</code> function:</p> <pre><code>help(geometric_mean)\n</code></pre> <p>output</p> <pre><code>Help on function geometric_mean in module __main__:\n\ngeometric_mean(values)\n    This function takes a list of\n    values and returns the geometric mean \n    of those values\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/07_functions-scope/#variable-scope","title":"Variable Scope","text":"<p>Variable naming can be difficult and sometimes variable names might need to be reused. Normally, when we use a variable name over again, we change the value of that variable. However, if we assign the same variable in and outside a function the values do not get overwritten:</p> <pre><code>x = 45\n\ndef print_x():\n    x = 30\n    return x\n</code></pre> <pre><code>x\n</code></pre> <p>output</p> <pre><code>45\n</code></pre> <pre><code>print_x()\n</code></pre> <p>output</p> <pre><code>30\n</code></pre>"},{"location":"programming_languages_tools/intro_to_python/07_functions-scope/#references","title":"References","text":"<ol> <li>Data Analysis and Visualization in Python for Ecologists</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/","title":"Introduction","text":""},{"location":"programming_languages_tools/intro_to_r/01_introduction/#introduction-to-rstudio","title":"Introduction To RStudio","text":"<p>Author: Jason Laird, PhD Student, M.Sc.</p> <p>Learning Objectives</p> <ul> <li>Introduction To RStudio</li> <li>Data Types/Variables/Vectors</li> <li>Data Structures</li> <li>Functions/Flow</li> <li>Inspecting/Manipulating Data</li> <li>Data Vizualization</li> </ul> <p>RStudio is what is known as an Integrated Development Environment or IDE. Here you can write scripts, run R code, use R packages, view plots, and manage projects. To download RStudio on your own computer, use the link below:</p> <p>Download RStudio</p> <p>Once downloaded, open RStudio. You'll see the following three panels:</p> <ul> <li>The Interactive R console/Terminal (left)</li> <li>Environment/History/Connections (upper right)</li> <li>Files/Plots/Packages/Help/Viewer (lower right)</li> </ul> <p>RStudio Layout</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#project-management","title":"Project Management","text":"<p>Before we dive into R it is worth taking a moment to talk about project management. Often times data analysis is incremental and files build up over time resulting in messy directories:</p> <p>Example of a Messy Directory</p> <p></p> <p>Sifting through a non-organized file system can make it difficult to find files, share data/scripts, and identify different versions of scripts. To remedy this, It is reccomended to work within an R Project. Before we make this project, we should make sure you are in your home directory. To do this click on the three dots in the files tab:</p> <p>Navigating Folders</p> <p></p> <p>Then enter in a ~ symbol to go home!</p> <p>Getting Home</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#r-project","title":"R Project","text":"<p>For the following intro to R tutorial we will be using glioblastoma data from cBioPortal. When working within R it is useful to set up an R project. R projects will set your working directory relative to the project directory. This can help ensure you are only working with files within this project space. To create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>intro_to_r</code>)</li> <li><code>Create Project</code></li> </ol> <p>When analyzing data it is useful to create a folder to house your raw data, scripts and results. We can do this by clicking the <code>New Folder</code> icon to create these folders:</p> <ol> <li>Click <code>New Folder</code> &gt; Enter <code>data</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>scripts</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>results</code> &gt; Click OK</li> </ol> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>download.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/intro_to_r/data/metadata.csv\",destfile = \"data/metadata.csv\")\ndownload.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/intro_to_r/data/metadata.tsv\",destfile = \"data/metadata.tsv\")\ndownload.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/intro_to_r/data/test.xlsx\",destfile = \"data/test.xlsx\")\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#data-principles","title":"Data Principles","text":"<ul> <li>Treat data as read-only</li> <li>Store raw data separately from cleaned data if you do need to manipulate it</li> <li>Ensure scripts to clean data are kept in a separate <code>scripts</code> folder</li> <li>Treat reproducible results as disposable</li> </ul> <p>Tip</p> <p>Result files are good candidate files to cut if you are getting low on storage.</p>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#new-r-script","title":"New R script","text":"<p>Now we will create an R script. R commands can be entered into the console, but saving these commands in a script will allow us to rerun these commands at a later date. To create an R script we will need to either:</p> <ul> <li>Go to <code>File &gt; New File &gt; R script</code></li> <li>Click the <code>New File</code> icon and select R script</li> </ul> <p>Creating a New R Script</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#running-r-code","title":"Running R Code","text":"<p>When running R code you have a few options:</p> <p>Running One Line/Chunk:</p> <ul> <li> <p>Put your cursor at the beginning of the line of code and hit <code>Ctrl + Enter</code> on Windows or  \u2318 + <code>Enter</code> on MacOSX.</p> </li> <li> <p>Highlight the line/chunk of code and hit <code>Ctrl + Enter</code> or \u2318 + <code>Enter</code>.</p> </li> </ul> <p>Running The Entire Script:</p> <ul> <li>Clicking <code>Source</code> at the top of the script window.</li> </ul>"},{"location":"programming_languages_tools/intro_to_r/01_introduction/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/02_basics/","title":"R Basics","text":""},{"location":"programming_languages_tools/intro_to_r/02_basics/#new-r-script","title":"New R script","text":"<p>Now we will create an R script. R commands can be entered into the console, but saving these commands in a script will allow us to rerun these commands at a later date. To create an R script we will need to either:</p> <ul> <li>Go to <code>File &gt; New File &gt; R script</code></li> <li>Click the <code>New File</code> icon and select R script</li> </ul> <p></p>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#running-r-code","title":"Running R Code","text":"<p>When running R code you have a few options:</p> <p>Running One Line/Chunk:</p> <ul> <li> <p>Put your cursor at the beginning of the line of code and hit <code>Ctrl + Enter</code> on Windows or  \u2318 + <code>Enter</code> on MacOSX.</p> </li> <li> <p>Highlight the line/chunk of code and hit <code>Ctrl + Enter</code> or \u2318 + <code>Enter</code>.</p> </li> </ul> <p>Running The Entire Script:</p> <ul> <li>Clicking <code>Source</code> at the top of the script window.</li> </ul>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#calculations","title":"Calculations","text":"<p>Let's try running some R code! R can be used to run all sorts of calculations just like a calculator:</p> <p></p> <p>You will notice that we ran this code in the script window but you can see the output in the console.  When we work with calculations it is useful to remember the order of operations - here are their equivalents in R:</p> <ul> <li>Parentheses: <code>(</code>, <code>)</code></li> <li>Exponents: <code>^</code> or <code>**</code></li> <li>Multiply: <code>*</code></li> <li>Divide: <code>/</code></li> <li>Add: <code>+</code></li> <li>Subtract: <code>-</code></li> </ul> <p>Let's look at some examples:</p> <pre><code>10 * 3^3\n</code></pre> <p>output</p> <pre><code>[1] 270\n</code></pre> <pre><code>(400 / 10) * (4e2) # 4e2 is the same as 4^2\n</code></pre> <p>output</p> <pre><code>[1] 16000\n</code></pre> <p>You'll notice that in the last equation we added words after a <code>#</code> and the equation still ran. This is what is known as a comment, where everything after the <code>#</code> is not registered as R code. Commenting is immensely valuable for giving your code context so that you and whoever else reads it knows the purpose of a given chunk of code.</p> <p>Additionally there are functions built in R to perform mathematical calculations:</p> <pre><code>abs(10) # absolute value\n</code></pre> <p>output</p> <pre><code>[1] 10\n</code></pre> <pre><code>sqrt(25) # square root\n</code></pre> <p>output</p> <pre><code>[1] 5\n</code></pre> <pre><code>log(10) # natural logarithm\n</code></pre> <p>output</p> <pre><code>[1] 2.302585\n</code></pre> <pre><code>log10(10) # log base 10\n</code></pre> <p>output</p> <pre><code>[1] 1\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#comparisons","title":"Comparisons","text":"<p>R can also be used to make comparisons. Here we note the operators used to do so:</p> <ul> <li>Equals: <code>==</code></li> <li>Does Not Equal: <code>!=</code></li> <li>Less Than Or Equal <code>&lt;=</code></li> <li>Greater Than Or Equal <code>&gt;=</code></li> <li>Greater Than <code>&gt;</code></li> <li>Less Than <code>&lt;</code></li> </ul> <pre><code>2 == 2\n</code></pre> <p>output</p> <pre><code>[1] TRUE\n</code></pre> <pre><code>2 != 2\n</code></pre> <p>output</p> <pre><code>[1] FALSE\n</code></pre> <pre><code>3 &lt;= 10\n</code></pre> <p>output</p> <pre><code>[1] TRUE\n</code></pre> <p>Note</p> <p>Unless the number is an integer, do not use <code>==</code> to compare. This is due to the fact that the decimal value may appear the same in R but from a machine level the two values can be very different.</p>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#variables-vectors","title":"Variables &amp; Vectors","text":"<p>Dealing with values can be cumbersome. In R, values can be assigned to words using the <code>&lt;-</code> operator:</p> <pre><code>x &lt;- 35 # assigning a value of 35\nx\n</code></pre> <p>output</p> <pre><code>[1] 35\n</code></pre> <pre><code>x &lt;- 40 # changing value to 40\nx\n</code></pre> <p>output</p> <pre><code>[1] 40\n</code></pre> <p>You'll notice that we initially assigned <code>x</code> to a value of <code>35</code> and then updated value to <code>40</code>. This is important to keep in mind because the last value assigned to <code>x</code> will be kept. Variables can I have a combination lowercase letters, uppercase letters, underscores and periods:</p> <pre><code>value &lt;- 40\nbiggerValue &lt;- 45\neven_bigger_value &lt;- 50\nbiggest.value &lt;- 55\n</code></pre> <pre><code>value\nbiggerValue\neven_bigger_value\nbiggest.value\n</code></pre> <p>output</p> <pre><code>[1] 40\n[1] 45\n[1] 50\n[1] 55\n</code></pre> <p>Note</p> <p>Take note that the spelling needs to be consistent to call the variable correctly.</p> <p>We can also assign a series of values in a specific order to a variable to create what is called a vector:</p> <pre><code>someVector &lt;- 5:10\nsomeVector\n</code></pre> <p>output</p> <pre><code>[1]  5  6  7  8  9 10\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#selecting-vector-elements","title":"Selecting Vector Elements","text":"<p>When we assign values to a vector we can select certain elements</p>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#environment","title":"Environment","text":"<p>As you may have noticed we have been assigning variables and they have been added to your <code>Environment</code> window:</p> <p></p> <p>If you would like to declutter your environment, you have a few options:</p> <ul> <li>You can use the <code>rm()</code> function to remove which ever variables you'd like. To remove more than one just put a comma between variable names.</li> <li>You can clear all variables by clicking the broom icon:</li> </ul> <p></p> <p>Warning</p> <p>Be careful when removing variables, especially if these values took a long time to generate!</p>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#r-packages","title":"R Packages","text":"<p>Aside from the base functions there are thousands of custom fuctions which are bundled in R packages. We can access these functions by loading the package that contains them. To load a package you can use the <code>library()</code> function:</p> <pre><code>library(dplyr)\n</code></pre> <p>However, sometimes, a function has the same name as a function in another package. This can create conflicts if you load both packages. To resolve this it is often desirable to specify where your function comes from with the <code>::</code> symbol. For example, the <code>select()</code> function through <code>dplyr</code> is a common name for a function. We can specify that we want the <code>select()</code> function in <code>dplyr</code> with:</p> <pre><code>dplyr::select()\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/02_basics/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/","title":"Data Types, Variables &amp; Vectors","text":""},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#data-types","title":"Data Types","text":"<p>In R we can use a few different types of types of data:</p> <ul> <li>numeric - numeric values that can contain whole numbers and decimals</li> <li>character - text value that is made a text value by adding quotes. So for example 1 2 3 is a numeric data, but \"1\" \"2\" \"3\" is character data</li> <li>integer - limited to just whole numbers, but will take up less memory than numeric data. We can specify an integer by adding L to a number (e.g. 1L or 3L)</li> <li>logical - These are boolean values so TRUE/T or FALSE/F.</li> <li>complex - complex number such as 1+6i</li> </ul>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#variables","title":"Variables","text":"<p>We can store these data in terms called variables. In R, values can be assigned to words using the <code>&lt;-</code> operator:</p> <pre><code>x &lt;- 35 \nx\n</code></pre> <p>output</p> <pre><code>[1] 35\n</code></pre> <p>Here we see that we assigned the value of 35 to the <code>x</code>!</p> <pre><code>x &lt;- 40 # changing value to 40\nx\n</code></pre> <p>output</p> <pre><code>[1] 40\n</code></pre> <p>You'll notice that we initially assigned <code>x</code> to a value of <code>35</code> and then updated value to <code>40</code>. This is important to keep in mind because the last value assigned to <code>x</code> will be kept. You'll also notice that we add words after the <code>#</code> symobl. This is what is known as a comment, where everything after the <code>#</code> is not registered as R code. Commenting is immensely valuable for giving your code context so that you and whoever else reads it knows the purpose of a given chunk of code. </p> <p>Variables can also have a combination lowercase letters, uppercase letters, underscores and periods:</p> <pre><code>value &lt;- 40\nbiggerValue &lt;- 45\neven_bigger_value &lt;- 50\nbiggest.value &lt;- 55\n</code></pre> <pre><code>value\nbiggerValue\neven_bigger_value\nbiggest.value\n</code></pre> <p>output</p> <pre><code>[1] 40\n[1] 45\n[1] 50\n[1] 55\n</code></pre> <p>Note</p> <p>Take note that the spelling needs to be consistent to call the variable correctly.</p>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#environment","title":"Environment","text":"<p>As you may have noticed we have been assigning variables and they have been added to your <code>Environment</code> window:</p> <p>R Environment Variables</p> <p></p> <p>If you would like to declutter your environment, you have a few options:</p> <ul> <li>You can use the <code>rm()</code> function to remove which ever variables you'd like. To remove more than one just put a comma between variable names.</li> <li>You can clear all variables by clicking the broom icon:</li> </ul> <p>Removing Environment Variables</p> <p></p> <p>Warning</p> <p>Be careful when removing variables, especially if these values took a long time to generate!</p>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#vectors","title":"Vectors","text":"<p>We can also assign a series of values in a specific order to a variable to create what is called a vector:</p> <pre><code>someVector &lt;- 5:10\nsomeVector\n</code></pre> <p>output</p> <pre><code>[1]  5  6  7  8  9 10\n</code></pre> <p>Just a note on vectors - when we create vectors of other data types, we typically need to put our set in the following format:</p> <pre><code>character_vec &lt;- c(\"a\",\"b\",\"c\")\n</code></pre> <p>Here we see that values are separated by commaas and in between <code>c()</code>. We can select values in a vector we need to use a few different comparison operators:</p> <ul> <li>Equals: <code>==</code></li> <li>Does Not Equal: <code>!=</code></li> <li>Less Than Or Equal <code>&lt;=</code></li> <li>Greater Than Or Equal <code>&gt;=</code></li> <li>Greater Than <code>&gt;</code></li> <li>Less Than <code>&lt;</code></li> </ul> <p>Let's try using some!</p> <pre><code>someVector[someVector == 10] # Elements which are equal to 10\n</code></pre> <p>output</p> <pre><code>[1] 10\n</code></pre> <pre><code>someVector[someVector &lt; 7] # All elements less than seven\n</code></pre> <p>output</p> <pre><code>[1] 5 6\n</code></pre> <pre><code>someVector[-4] # All but the fourth value\n</code></pre> <p>output</p> <pre><code>[1]  5  6  7  9 10\n</code></pre> <pre><code>someVector[2:4] # Elements two to four\n</code></pre> <p>output</p> <pre><code>[1] 6 7 8\n</code></pre> <pre><code>someVector[c(1, 5)] # Elements one and five\n</code></pre> <p>output</p> <pre><code>[1] 5 9\n</code></pre> <pre><code>someVector[someVector %in% c(5,8,9)] # Elements in the set 5,8,9\n</code></pre> <p>output</p> <pre><code>[1] 5 8 9\n</code></pre> <p>We can also give our vector names per value:</p> <pre><code>names(someVector) &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\")\nsomeVector\n</code></pre> <p>output</p> <pre><code>a  b  c  d  e  f \n5  6  7  8  9 10 \n</code></pre> <p>We see that each value now has a name, <code>a-f</code>. We can refer to values by this name:</p> <pre><code>someVector[\"c\"]\n</code></pre> <p>output</p> <pre><code>c \n7 \n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#data-type-conversion","title":"Data Type Conversion","text":"<p>We can convert between data types with a few functions:</p> <ul> <li><code>as.logical</code> - converts to boolean values</li> <li><code>as.numeric</code> - converts to numeric values</li> <li><code>as.character</code>  - converts to character values</li> <li><code>as.factor</code> - converts to factor values</li> </ul> <p>Here we mentions factors which are used for data with levels. Think of small, medium and large. This order can be specified when you factor data with the <code>factor</code> function:</p> <pre><code>factor_var &lt;- factor(c(\"a\",\"b\",\"c\"), levels=c(\"a\",\"b\",\"c\"))\nfactor_var\n</code></pre> <p>output</p> <pre><code>[1] a b c\nLevels: a b c\n</code></pre> <p>In the output we see that our data now has <code>Levels</code> which specify the desired order. </p>"},{"location":"programming_languages_tools/intro_to_r/02_data_types_variables_vectors/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/","title":"Data Structures","text":""},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#data-structures","title":"Data Structures","text":"<p>So we have all this lovely data to play with and in R we typically organize in a few ways:</p>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#vectors","title":"Vectors","text":"<p>Vectors are collections of data, like a:</p> <p>collection of numbers - <code>c(1,2,3)</code>   collection of characters -  <code>c(\"1\",\"2\",\"3\")</code>   collection of logical values - <code>c(TRUE,FALSE,TRUE)</code></p> <p>Note</p> <p>It should be noted that a vector needs to be a collection of the same type of data. You will also note that each list is separated by commas and surrounded by <code>c()</code>. This is necessary to create vectors so make sure to remember the <code>c()</code>!</p> <p>On the previous page we mentioned that values in vectors can be selected by their value (e.g. <code>someVector[4]</code> to grab the forth element) or their name if they are named (e.g. <code>someVector[\"a\"]</code> to grab the element named <code>a</code>)</p>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#strings","title":"Strings","text":"<p>When dealing with character data there are a few functions that are helpful when manipulating them. Let's create a few character vectors to play with:</p> <pre><code>x=c(\"a\",\"b\",\"c\")\ny=c(\"d\",\"e\",\"f\")\n</code></pre> <pre><code>paste(x, y, sep = '_') # join two or more vectors together\n</code></pre> <p>output</p> <pre><code>[1] \"a_d\" \"b_e\" \"c_f\"\n</code></pre> <pre><code>paste(x, collapse = '_') # collapes values in a vector together\n</code></pre> <p>output</p> <pre><code>[1] \"a_b_c\"\n</code></pre> <pre><code>grep(\"a\", x) # Find regular expression matches in x\n</code></pre> <p>output</p> <pre><code>[1] 1\n</code></pre> <p>Here we note that the pattern <code>a</code> was found 1 time. We can also replace patterns as well</p> <pre><code>gsub(\"a\", \"A\", x) # Replace pattern matches in x \n</code></pre> <p>output</p> <pre><code>[1] \"A\" \"b\" \"c\"\n</code></pre> <pre><code>toupper(x) # Convert to uppercase\n</code></pre> <p>output</p> <pre><code>[1] \"A\" \"B\" \"C\"\n</code></pre> <pre><code>tolower(x) # Convert to lowercase.\n</code></pre> <p>output</p> <pre><code>[1] \"a\" \"b\" \"c\"\n</code></pre> <pre><code>nchar(x) # Number of characters in a string\n</code></pre> <p>output</p> <pre><code>[1] 3\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#matrices","title":"Matrices","text":"<p>A matrix can be created by combining vectors of the same length and same data type. They are used frequently when performing operations on numeric data but can include other data types. In R we can create a matrix with the <code>matrix()</code> function:</p> <pre><code>m &lt;- matrix(data=1:9,nrow = 3,ncol=3)\nm\n</code></pre> <p>output</p> <pre><code>     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n</code></pre> <p>Here we take a vector and specify how many columns and how many rows we'd like. To select elements in a matrix we can use the following:</p> <pre><code>m[2, ] # Select a row\n</code></pre> <p>output</p> <pre><code> [1] 2 5 8\n</code></pre> <pre><code>m[ , 1] # Select a column\n</code></pre> <p>output</p> <pre><code>[1] 1 2 3\n</code></pre> <pre><code>m[2, 3] # Select an element\n</code></pre> <p>output</p> <pre><code>[1] 8\n</code></pre> <pre><code>nrow(m) # number of rows \n</code></pre> <p>output</p> <pre><code>[1] 3\n</code></pre> <pre><code>ncol(m) # number of columns \n</code></pre> <p>output</p> <pre><code>[1] 3\n</code></pre> <pre><code>dim(m) # number of rows then columns \n</code></pre> <p>output</p> <pre><code>[1] 3 3\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#data-frames","title":"Data Frames","text":"<p>Data frames are also collections of vectors of the same length. However, they do not need to be the same data type. Here we create a data.frame with the <code>data.frame()</code> function:</p> <pre><code>df &lt;- data.frame(\n  characters=c(\"past\",\"present\",\"future\"),\n  numbers=c(1,2,3),\n  logical=c(TRUE,FALSE,TRUE),\n  integer=c(1L,2L,3L)\n)\n</code></pre> <p>output</p> <pre><code>  characters numbers logical integer\n1       past       1    TRUE       1\n2    present       2   FALSE       2\n3     future       3    TRUE       3\n</code></pre> <p>We can manipulate data frames the same way we manipulate matrices. However, we have a short hand option to grab a column:</p> <pre><code>df$characters # select the column with the title characters\n</code></pre> <p>output</p> <pre><code>[1] \"past\"    \"present\" \"future\" \n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#lists","title":"Lists","text":"<p>Lists are collections of data that do not need to be the same type or length. We can create lists with the <code>list()</code> function:</p> <pre><code>l &lt;- list(\n  data.frame=data.frame(numbers=1:3,characters=c(\"past\",\"present\",\"future\")),\n  numbers=1:5,\n  characters=c(\"past\",\"present\",\"future\")\n)\n</code></pre> <p>output</p> <pre><code>$data.frame\n  numbers characters\n1       1       past\n2       2    present\n3       3     future\n\n$numbers\n[1] 1 2 3 4 5\n\n$characters\n[1] \"past\"    \"present\" \"future\" \n</code></pre> <pre><code>l[2] # list with just the second thing in the list\n</code></pre> <p>output</p> <pre><code>$numbers\n[1] 1 2 3 4 5\n</code></pre> <pre><code>l[[2]] # select the second thing in the list\n</code></pre> <p>output</p> <pre><code>[1] 1 2 3 4 5\n</code></pre> <pre><code>l$characters # grab the element named characters\n</code></pre> <p>output</p> <pre><code>[1] \"past\"    \"present\" \"future\" \n</code></pre> <pre><code>l['numbers'] # list with just the item named numbers\n</code></pre> <p>output</p> <pre><code>$numbers\n[1] 1 2 3 4 5\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/03_data-structures/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/","title":"Functions and Flow","text":""},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#functions","title":"Functions","text":"<p>Functions are operations we can perform on our various data structures to get some result. We typically like to make functions modular so they perform one specific task and not whole pipelines. Here is the general format for a function:</p> <pre><code>functionName &lt;- function(x){\n  result &lt;- operation(x)\n  return(result)\n}\n</code></pre> <p>So here we see that we assign some operation to a name, here it we just call it <code>functionName</code>. Then the function takes an input, <code>x</code>. Inside the function our result is obtained by doing some operation on our input. Finally we then use <code>return()</code> to return that result. Let's try making a function that will square the input:</p> <pre><code>squareInput &lt;- function(x){\n  result &lt;- x * x\n  return(result)\n}\n\nsquareInput(5)\n</code></pre> <p>output</p> <pre><code>25\n</code></pre> <p>Note</p> <p>Without <code>return()</code> in the function, R will return the last variable in the function. So you can leave out <code>return()</code>, however it is best to specify what you are returning for clarity.</p>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#additional-arguements","title":"Additional Arguements","text":"<p>Functions can have as little or as many arguements as needed. So in the example above we used one arguement, <code>x</code>. Let's try using more than one arguement:</p> <pre><code>item_in_vector_func &lt;- function(vector, item){\n  item_in_vector &lt;- item %in% vector\n  return(item_in_vector)\n}\nitem_in_vector_func(vector = c(1,2,3), item=2)\n</code></pre> <p>output</p> <pre><code>[1] TRUE\n</code></pre> <p>Here we used a new operator, <code>%in%</code>, which does exactly what it sounds like - it checks whether some value is in another set of values. We should also note, you can specify values in functions:</p> <pre><code>item_in_vector_func &lt;- function(vector=1:10, item=NULL){\n  item_in_vector &lt;- item %in% vector\n  return(item_in_vector)\n}\n</code></pre> <p>Here we specify default values for the <code>item_in_vector()</code> function. Unless we change them when we call the function, these values will remain. So when you call a function from a package it's a good idea to check what the default values are.</p>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#control-flow","title":"Control Flow","text":"<p>Now what if we don't want to perform an operation until a condition is met? For this we need an if/else statement:</p> <pre><code>x &lt;- 3\n\nif (x == 10){\n  print(\"x equals 10\")\n} else{\n  print(\"x does not equal 10\")\n}\n</code></pre> <p>output</p> <pre><code>[1] \"x does not equal 10\"\n</code></pre> <p>Now what if we wanted to include multiple conditions?</p> <pre><code>x &lt;- 3\n\nif (x == 10){\n  print(\"x equals 10\")\n} else if (x &gt; 2){\n  print(\"x is greater than 2\")\n} else if (x &gt; 1){\n  print(\"x is greater than 1\")\n} else{\n  print(\"x does not equal  10\")\n}\n</code></pre> <p>output</p> <pre><code>[1] \"x is greater than 2\"\n</code></pre> <p>Here notice that, x meets 2 of the conditions:</p> <ul> <li><code>x &gt; 2</code></li> <li><code>x &gt; 1</code></li> </ul> <p>However, we note that the conditional statement is broken when <code>x</code> meets the first condition in the order above. </p>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#loops","title":"Loops","text":"<p>Operations can be repeated with a <code>for</code> loop:</p> <pre><code>for (i in 1:5){\n  print(i*i)\n}\n</code></pre> <p>output</p> <pre><code>[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n</code></pre> <p>Here we see that <code>i</code> is a substitute for some value in the sequence provided - in this case <code>1,2,3,4,5</code>. We can also nest a loop inside a loop like so:</p> <pre><code>for (i in 1:3){\n  for(j in 3:5){\n    print(i*j)\n  }\n}\n</code></pre> <p>output</p> <pre><code>[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 8\n[1] 10\n[1] 9\n[1] 12\n[1] 15\n</code></pre> <p>You'll notice that for each value i was multiplied by each value j. So:</p> <p>Explanation</p> <pre><code>1*3\n1*4\n1*5\n2*3\n2*5\n2*6\n3*3\n3*4\n3*5\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#avoiding-loops","title":"Avoiding Loops","text":"<p>While we can use a loop to perform repetitive operations, R also offers a suite of <code>apply()</code> functions to apply some operation to different data structures in a more concise manner. For instance consider the following loop:</p> <pre><code>for (i in 1:5){\n  print(log(i))\n}\n</code></pre> <p>output</p> <pre><code>[1] 0\n[1] 0.6931472\n[1] 1.098612\n[1] 1.386294\n[1] 1.609438\n</code></pre> <p>We can replace this loop with the <code>sapply()</code> function which applies some operation to a vector with the following one line of code:</p> <pre><code>sapply(1:5,log)\n</code></pre> <p>output</p> <pre><code>[1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379\n</code></pre> <p>Note, that we don't print the statement as the output is automatically printed to the console. If we wanted to perform an operation on a list we could use the <code>lapply()</code> function:</p> <pre><code>lapply(list(\"a\",\"b\",\"c\"),toupper)\n</code></pre> <p>output</p> <pre><code>[[1]]\n[1] \"A\"\n\n[[2]]\n[1] \"B\"\n\n[[3]]\n[1] \"C\"\n</code></pre> <p>The <code>apply</code> familiy of functions can also be applied to data frames. Data frames have two axes, rows and columns. When using <code>apply()</code> on a data frame you specify <code>1</code> for rows and <code>2</code> for columns. Here is an example for calculating sums:</p> <pre><code>df=data.frame(a=1:3,b=1:3,c=1:3,d=1:3)\ndf\n</code></pre> <p>output</p> <pre><code>  a b c d\n1 1 1 1 1\n2 2 2 2 2\n3 3 3 3 3\n</code></pre> <pre><code># applying sums to rows\napply(df,1,sum)\n</code></pre> <p>output</p> <pre><code>[1]  4  8 12\n</code></pre> <pre><code># applying sums to rows\napply(df,2,sum)\n</code></pre> <p>output</p> <pre><code>a b c d \n6 6 6 6 \n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/04_functions-flow/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/","title":"Inspecting/Manipulating Data","text":""},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#importing-data","title":"Importing Data","text":"<p>When importing data, R provides a few base functions:</p> <ul> <li><code>read.csv()</code> - to read in .csv files or files separated by commas</li> <li><code>read.table()</code> - to read files separated by delimiters other than commas - like spaces, tabs, semicolons, etc.</li> <li><code>openxlsx::read.xlsx()</code> - to read excel files</li> </ul> <p>You'll note that <code>read.xlsx()</code> has the prefix <code>openxlsx::</code>. This is because the <code>read. xlsx()</code> function is not avaiable with base R. To get this function you will need either need to:</p> <ul> <li>specify the package that the function comes from:</li> </ul> <pre><code>openxlsx::read.xlsx()\n</code></pre> <ul> <li>load the library with the package:</li> </ul> <pre><code>library(openxlsx)\nread.xlsx()\n</code></pre> <p>We will now practice inspecting data frames that we will copy over from a shared location. In the <code>Terminal</code> tab enter the following command:</p> <pre><code>cp /cluster/tufts/bio/tools/training/intro-to-r/data/* data/\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#reading-csv-files","title":"Reading CSV Files","text":"<p>When importing <code>.csv</code> files you'll need to specify the path to where you're file is located. So if your <code>.csv</code> file is in <code>data/test.csv</code>, you can download it like so:</p> <pre><code>read.csv(\"./data/metadata.csv\")\n</code></pre> <p>We can also extend this to URL's as well:</p> <pre><code>read.csv(url(\"https://zenodo.org/api/files/739025d8-5111-476a-9bb9-7f28a200ce8e/linked-ee-dataset-v20220524-QT_2022-07-13-sdev.csv\"))\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#reading-text-delimeted-files","title":"Reading Text Delimeted Files","text":"<p>Like <code>read.csv()</code>, <code>read.table()</code> can also import data. The latter function is very useful in that it can download files not delimted (a.k.a separated) by commas. So to open a \".tsv\" file (a.k.a a file delimeted by a tab <code>\"\\t\"</code>):</p> <pre><code>meta &lt;- read.table(\"data/metadata.tsv\",sep=\"\\t\",stringsAsFactors=FALSE)\n</code></pre> <p>You'll notice in the code above that we include the option, <code>stringsAsFactors=FALSE</code>. If this was set to <code>TRUE</code> it would coerce your character columns into factor columns and this isn't always desired. So here we explicitly say <code>stringsAsFactors=FALSE</code> to be safe.</p>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#reading-excel-files","title":"Reading Excel Files","text":"<p>While files like the ones mentioned above are popular, so are excel spreadsheets. So it is worth mentioning how to read in excel data as well:</p> <pre><code>library(openxlsx)      \nread.xlsx(\"data/test.xlsx\")\n</code></pre> <p>Now in excel spreadsheets you may only want to pull out one page or start from a row that isn't the first. To do so you can use:</p> <pre><code>library(openxlsx)\nread.xlsx(\"./data/test.xlsx\",sheet=1,startRow = 1,colNames = TRUE,rowNames = FALSE)\n</code></pre> <p>So here we are pulling: the document \"/Documents/test.xlsx\", the second sheet, starting from the fifth row, specifying we do have column names, specifying we do not have row names. </p>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#inspecting-data","title":"Inspecting Data","text":"<p>You might have noticed that the only data frame we saved to a variable was the <code>metadata.tsv</code> file. We are going to now examine this file:</p> <p>To get a summary of each column:</p> <pre><code>summary(meta)\n</code></pre> <p>output</p> <pre><code>   SampleID         AntibioticUsage    DaySinceExperimentStart   Genotype         Description           OtuCount     \n Length:9           Length:9           Length:9                Length:9           Length:9           Min.   : 175.0  \n Class :character   Class :character   Class :character        Class :character   Class :character   1st Qu.: 279.0  \n Mode  :character   Mode  :character   Mode  :character        Mode  :character   Mode  :character   Median : 452.0  \n                                                                                                     Mean   : 777.8  \n                                                                                                     3rd Qu.:1451.0  \n                                                                                                     Max.   :1492.0 \n</code></pre> <p>To get the data's class:</p> <pre><code>class(meta)\n</code></pre> <p>output</p> <pre><code>[1] data.frame\n</code></pre> <p>To get a display of the data's contents:</p> <pre><code>str(meta)\n</code></pre> <p>output</p> <pre><code>'data.frame':   9 obs. of  6 variables:\n $ SampleID               : chr  \"WT.unt.1\" \"WT.unt.2\" \"WT.unt.3\" \"WT.unt.7\" ...\n $ AntibioticUsage        : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ DaySinceExperimentStart: chr  \"DAY0\" \"DAY0\" \"DAY0\" \"DAY0\" ...\n $ Genotype               : chr  \"WT\" \"WT\" \"WT\" \"WT\" ...\n $ Description            : chr  \"16S_WT_unt_1_SRR2627457_1\" \"16S_WT_unt_2_SRR2627461_1\" \"16S_WT_unt_3_SRR2627463_1\" \"16S_WT_unt_7_SRR2627465_1\" ...\n $ OtuCount               : int  1174 1474 1492 1451 314 189 279 175 452\n</code></pre> <p>To get the first 6 rows:</p> <pre><code>head(meta)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n</code></pre> <p>To get the last 6 rows:</p> <pre><code>tail(meta)\n</code></pre> <p>output</p> <pre><code>SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452\n</code></pre> <p>To get the length of a vector:</p> <pre><code>length(meta$Genotype)\n</code></pre> <p>output</p> <pre><code>[1] 9\n</code></pre> <p>To get the dimensions of a matrix/data frame:</p> <pre><code>dim(meta) # answer is given in number of rows, then number of columns\n</code></pre> <p>output</p> <pre><code>[1] 9 6\n</code></pre> <p>To get the number of columns/rows:</p> <pre><code>ncol(meta)\n</code></pre> <p>output</p> <pre><code>[1] 6\n</code></pre> <pre><code>nrow(meta)\n</code></pre> <p>output</p> <pre><code>[1] 9\n</code></pre> <p>To get your column names:</p> <pre><code>colnames(meta)\n</code></pre> <p>output</p> <pre><code>[1] \"SampleID\"                \"AntibioticUsage\"        \n[3] \"DaySinceExperimentStart\" \"Genotype\"               \n[5] \"Description\"             \"OtuCount\" \n</code></pre> <p>To get your row names:</p> <pre><code>rownames(meta)\n</code></pre> <p>output</p> <pre><code>[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\"\n</code></pre> <p>Now that we know how to import our data and inspect it, we can go ahead and manipulate it!</p>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#manipulating-data","title":"Manipulating Data","text":"<p>So now that we have downloaded and inspected our data we can get to manipulating it! So to start, let's talk about accessing parts of your data. To grab the first column in a data frame/matrix you can do so like:</p> <pre><code>meta[,1]\n</code></pre> <p>output</p> <pre><code>[1] \"WT.unt.1\"   \"WT.unt.2\"   \"WT.unt.3\"   \"WT.unt.7\"   \"WT.day3.11\"\n[6] \"WT.day3.13\" \"WT.day3.15\" \"WT.day3.14\" \"WT.day3.9\" \n</code></pre> <p>To grab the first row:</p> <pre><code>meta[1,]\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype               Description OtuCount\n1 WT.unt.1            None                    DAY0       WT 16S_WT_unt_1_SRR2627457_1     1174\n</code></pre> <p>Now if your data is a data frame you have a special way of accessing coluns with the <code>$</code> operator:</p> <pre><code>meta$AntibioticUsage\n</code></pre> <p>output</p> <pre><code>[1] \"None\"         \"None\"         \"None\"         \"None\"         \"Streptomycin\"\n[6] \"Streptomycin\" \"Streptomycin\" \"Streptomycin\" \"Streptomycin\"\n</code></pre> <p>This comes in handy for readability. While you can grab your data by column number, it is much easier to read that you are grabbing Sepal Length. To grab mulitple columns/rows, you can do the following for both data frames and matrices:</p> <pre><code>meta[,c(2,4,6)] # grabbing the 2nd, 4th, and 6th columns\n</code></pre> <p>output</p> <pre><code>  AntibioticUsage Genotype OtuCount\n1            None       WT     1174\n2            None       WT     1474\n3            None       WT     1492\n4            None       WT     1451\n5    Streptomycin       WT      314\n6    Streptomycin       WT      189\n7    Streptomycin       WT      279\n8    Streptomycin       WT      175\n9    Streptomycin       WT      452\n</code></pre> <p>In a data frame, to access columns you can be more specific and specify by column name:</p> <pre><code>meta[,c(\"SampleID\",\"Genotype\",\"OtuCount\")]\n</code></pre> <p>output</p> <pre><code>    SampleID Genotype OtuCount\n1   WT.unt.1       WT     1174\n2   WT.unt.2       WT     1474\n3   WT.unt.3       WT     1492\n4   WT.unt.7       WT     1451\n5 WT.day3.11       WT      314\n6 WT.day3.13       WT      189\n7 WT.day3.15       WT      279\n8 WT.day3.14       WT      175\n9  WT.day3.9       WT      452\n</code></pre> <p>Now if we wanted to add a new column we could add one like so:</p> <pre><code>meta$Day &lt;- c(0,0,0,0,3,3,3,3,3) # name of new column comes after the $ sign\nmeta\n</code></pre> <p>output</p> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174   0\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474   0\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492   0\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451   0\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#subsetting-data","title":"Subsetting Data","text":"<p>To subset our data we need to know a little bit about the different logical operators:</p> Operator Description &gt; greater than &gt;= greater than or equal &lt; less than &lt;= less than or equal == equals != not equal &amp; and | or <p>Let's go through a few of these!</p> <p>Subsetting so that we only have rows where the <code>OtuCount</code> is greater than 1000:</p> <pre><code>meta[meta$OtuCount &gt; 1000,]\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype               Description OtuCount Day\n1 WT.unt.1            None                    DAY0       WT 16S_WT_unt_1_SRR2627457_1     1174   0\n2 WT.unt.2            None                    DAY0       WT 16S_WT_unt_2_SRR2627461_1     1474   0\n3 WT.unt.3            None                    DAY0       WT 16S_WT_unt_3_SRR2627463_1     1492   0\n4 WT.unt.7            None                    DAY0       WT 16S_WT_unt_7_SRR2627465_1     1451   0\n</code></pre> <p>Subsetting so that we only have rows where <code>OtuCount</code> is less than 400:</p> <pre><code>meta[meta$OtuCount &lt; 400,]\n</code></pre> <p>output</p> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> is equal to <code>Stretomycin</code>:</p> <pre><code> meta[meta$AntibioticUsage == \"Streptomycin\",]\n</code></pre> <p>output</p> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> is not equal to <code>Stretomycin</code>:</p> <pre><code>meta[meta$AntibioticUsage != \"Streptomycin\",]\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage DaySinceExperimentStart Genotype               Description OtuCount Day\n1 WT.unt.1            None                    DAY0       WT 16S_WT_unt_1_SRR2627457_1     1174   0\n2 WT.unt.2            None                    DAY0       WT 16S_WT_unt_2_SRR2627461_1     1474   0\n3 WT.unt.3            None                    DAY0       WT 16S_WT_unt_3_SRR2627463_1     1492   0\n4 WT.unt.7            None                    DAY0       WT 16S_WT_unt_7_SRR2627465_1     1451   0\n</code></pre> <p>Subsetting so that we only have rows where the <code>AntibioticUsage</code> equals <code>Steptomycin</code> or the <code>OtuCount</code> is less than <code>300</code>:</p> <pre><code>meta[meta$AntibioticUsage == \"Streptomycin\" | meta$OtuCount &lt; 300,]\n</code></pre> <p>output</p> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount Day\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314   3\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189   3\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279   3\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175   3\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452   3\n</code></pre>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#using-dplyr","title":"Using Dplyr","text":"<p>When subsetting data we should also mention the R package <code>dplyr</code>. This package has functionality to neatly modify data frames using the <code>%&gt;%</code> operator to separate your subsetting operations. Let's go through a quick example:</p> <pre><code>library(dplyr)\n\nmeta %&gt;%\n  filter(OtuCount &lt; 1400) %&gt;%    # filter rows with OtuCount less than 1400\n  select(c(SampleID,AntibioticUsage,Genotype,OtuCount)) %&gt;%   # Select certain rows\n  group_by(AntibioticUsage) %&gt;%   # group data by some column\n  mutate(HighOtuCount = OtuCount &gt; 1000)   # add a new column \n</code></pre> <p>output</p> <pre><code># A tibble: 6 \u00d7 5\n# Groups:   AntibioticUsage [2]\n  SampleID   AntibioticUsage Genotype OtuCount HighOtuCount\n  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;       &lt;int&gt; &lt;lgl&gt;       \n1 WT.unt.1   None            WT           1174 TRUE        \n2 WT.day3.11 Streptomycin    WT            314 FALSE       \n3 WT.day3.13 Streptomycin    WT            189 FALSE       \n4 WT.day3.15 Streptomycin    WT            279 FALSE       \n5 WT.day3.14 Streptomycin    WT            175 FALSE       \n6 WT.day3.9  Streptomycin    WT            452 FALSE  \n</code></pre> <p>Tip</p> <p>For more dplyr data wrangling tips check out the Data Wrangling with dplyr and tidyr Cheat Sheet</p>"},{"location":"programming_languages_tools/intro_to_r/05_inspecting-manipulating-data/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/","title":"Data Vizualization","text":""},{"location":"programming_languages_tools/intro_to_r/06_visualization/#plotting-with-ggplot2","title":"Plotting With ggplot2","text":"<p>While it is possible to use the base plotting system in R, we are going to focus on using the <code>ggplot2</code> library to create plots due to it's widespread use in scientific figure generation and the versitility of the package. The basic formula for creating a plot is as such:</p> <pre><code>library(ggplot2)\n\nggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +    # specify what data you are using and what your x and y columns are\n  geom_point()   # what type of plot do you want to make? here we make a scatterplot\n</code></pre> <p>Basic Scatterplot</p> <p></p> <p>While we will go through a few plot types in this topic note, we reccomend you check out the R Graph Gallery for a complete list of possible plots and how to make them using the <code>ggplot2</code> library.</p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#themes","title":"Themes","text":"<p>You are not just limited to a grey background theme when plotting with <code>ggplot2</code>. A poplular theme used in scientific figures is the dark-on-light theme:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\n  geom_point()+\n  theme_bw()\n</code></pre> <p>Basic Scatterplot - With Theme</p> <p></p> <p>Tip</p> <p>For a complete list of themes, visit the Complete ggplot2 Themes page</p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#scaling","title":"Scaling","text":"<p>Oftentimes your data will span mulitple magnitudes and this can result in an awkward distribution of data. We can scale either your x or y axes using a log scale to remedy this:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\n  geom_point()+\n  theme_bw()+\n  scale_y_log10()\n</code></pre> <p>Basic Scatterplot -  With Log Scaling</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#relationships","title":"Relationships","text":"<p>When plotting two numeric data columns against one another, it might be useful to have a representation of their relationship. Here we show how to add a best fit line:</p> <pre><code>ggplot(data = meta, mapping = aes(x = Day, y = OtuCount)) +\n  geom_point()+\n  theme_bw() +\n  scale_y_log10() +\n  geom_smooth(method=\"lm\")\n</code></pre> <p>Basic Scatterplot - With Regression Line</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#panels-and-colors","title":"Panels and Colors","text":"<p>Panels and colors are an important cue to highlight differences in your data:</p> <pre><code>ggplot(data = meta, \n       mapping = aes(x = Day, y = OtuCount,color = AntibioticUsage)) +    # color by antibiotic usage\n  geom_point()+\n  facet_wrap(~AntibioticUsage)+    # create different panels for different types of antibiotic usage\n  theme_bw()                   \n</code></pre> <p>Basic Scatterplot - With Pannels and Colors</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#modifying-text","title":"Modifying Text","text":"<p>To modify your text style you can leverage the <code>theme()</code> function:</p> <pre><code>ggplot(data = meta, mapping = aes(x = AntibioticUsage,fill = AntibioticUsage)) +\n  geom_bar()+\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) # angle the text by 45 degrees and move the text down by 1 point\n</code></pre> <p>Basic Barplot - With The Text Modified</p> <p></p> <p>You can also modify the x label, y label, title, and title of the legend:</p> <pre><code>ggplot(data = meta, mapping = aes(x = AntibioticUsage, y = OtuCount,fill= AntibioticUsage)) +\n  geom_boxplot()+\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +\n  labs(\n    x = \"Antibiotic Usage\",      # x axis title\n    y = \"OTU Count\",             # y axis title\n    title = \"Figure 1\",          # main title of figure\n    color = \"Antibiotic Usage\"   # title of legend\n  )\n</code></pre> <p>Basic Boxplot - With Plot Labels Changed</p> <p></p>"},{"location":"programming_languages_tools/intro_to_r/06_visualization/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> </ol>"},{"location":"programming_languages_tools/misc/api/","title":"API","text":""},{"location":"programming_languages_tools/misc/api/#api-queries","title":"API Queries","text":"<p>An API, or Application Programming Interface, is a way of accessing data directly from a website. In this way we can pull data from a website without having to deal with parsing HTML content. An API request occurs between a client and a server:</p> <p> </p> Image by DATAQUEST <p>Essentiall, we (the client) reach out to the server and request data. In return we get the data and a response code telling us how the request went. Sometimes we don't get the data and the response code can give us a hint as to why:</p> <p> </p> Image by WhiteHat <p>Each website (with an available API) should have more specific documentation on these codes and how to structure your request. Here we are going to cover how to use the STRINGDB API using Python and R. </p>"},{"location":"programming_languages_tools/misc/api/#api-request","title":"API Request","text":"RPython <ul> <li>We will first need to load the R packages necessary to handle API requests:</li> </ul> <pre><code>library(httr)\nlibrary(jsonlite)\n</code></pre> <ul> <li>Now we will need to take a look at the API documentation on the STRINGDB website. Typically, we have a base url that we pull from, and in this case is is:</li> </ul> <p>https://string-db.org/</p> <ul> <li>We then need to plug in the information we would like to pull by adding to the url. So to get an image of the  information we will add the following to the url:</li> </ul> <pre><code>api/json/interaction_partners?\n</code></pre> <ul> <li>To point to specific genes, say PTCH1, we will add the following:</li> </ul> <pre><code>identifiers=PTCH1\n</code></pre> <ul> <li>Now the full url will be:</li> </ul> <pre><code>https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\n</code></pre> <ul> <li>We can now plug this url into the <code>GET</code> function!</li> </ul> <pre><code>res &lt;- GET(\"https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\")\nres\n</code></pre> <p>output</p> <pre><code>Response [https://string-db.org/api/json/interaction_partners?identifiers=PTCH1]\n  Date: 2023-01-10 19:37\n  Status: 200\n  Content-Type: text/json; charset=utf-8\n  Size: 2.72 kB\n</code></pre> <ul> <li>Here we see that our request did come through. However, the data is in json format. We can convert this json data to tabular data with:</li> </ul> <pre><code>data = fromJSON(rawToChar(res$content))\nhead(data)\n</code></pre> <p>output</p> <pre><code>           stringId_A           stringId_B preferredName_A preferredName_B\n1 9606.ENSP00000332353 9606.ENSP00000295731           PTCH1             IHH\n2 9606.ENSP00000332353 9606.ENSP00000297261           PTCH1             SHH\n3 9606.ENSP00000332353 9606.ENSP00000266991           PTCH1             DHH\n4 9606.ENSP00000332353 9606.ENSP00000256442           PTCH1           CCNB1\n5 9606.ENSP00000332353 9606.ENSP00000249373           PTCH1             SMO\n6 9606.ENSP00000332353 9606.ENSP00000376458           PTCH1            CDON\n</code></pre> <ul> <li>Congratulations! You have pulled data using an API!</li> </ul>"},{"location":"programming_languages_tools/misc/api/#references","title":"References","text":"<ol> <li>https://www.dataquest.io/blog/r-api-tutorial/</li> <li>https://www.dataquest.io/blog/python-api-tutorial/</li> <li>https://apidocs.whitehatsec.com/whs/docs/error-handling</li> </ol>"},{"location":"programming_languages_tools/misc/github/","title":"Github","text":""},{"location":"programming_languages_tools/misc/github/#introduction-to-github","title":"Introduction to GitHub","text":"<p>The power in GitHub lies in version control. Code is often changed and in doing so previous versions of files can easily be lost. GitHub saves changes to files so that one can go back and restore previous versions if needed. Additionally, there is a plethora of functionality to: update code collaboratively, publish static website pages, report issues with code, etc..</p>"},{"location":"programming_languages_tools/misc/github/#creating-a-git-repository","title":"Creating a Git Repository","text":"<ul> <li>To create a Git Repository by:</li> </ul> <pre><code># change into your project directory\ncd /path/to/your/project\n\n# initialize the repository\ngit init\n</code></pre>"},{"location":"programming_languages_tools/misc/github/#addcommit-files","title":"Add/Commit Files","text":"<ul> <li>To add files to be tracked:</li> </ul> <pre><code># add files to be tracked\ngit add main.py input.txt \n</code></pre> <ul> <li>To commit these files to your Git Repository:</li> </ul> <pre><code># commit the files to the repository, creating the first snapshot\ngit commit -m \"Initial Commit\"\n</code></pre>"},{"location":"programming_languages_tools/misc/github/#configure-your-credentials","title":"Configure Your Credentials","text":"<ul> <li>To configure your credentials:</li> </ul> <pre><code># configure your user name/email\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"[email address]\"\n</code></pre>"},{"location":"programming_languages_tools/misc/github/#push-to-github","title":"Push To GitHub","text":"<ul> <li>Currently, these files are not on github. To add them to Github:</li> </ul> <pre><code>## push files to github\ngit remote add origin git@github.com:user_name/my_new_repo.git\ngit push -u origin master\n</code></pre>"},{"location":"programming_languages_tools/misc/github/#going-further","title":"Going Further","text":"<ul> <li>GitHub has much more functionality than what has been described here. To learn more, check out:</li> </ul> <p>GitHub Docs</p> <ul> <li>If you are looking for a brief but relatively comprehensive list check out:</li> </ul> <p>git - the simple guide</p>"},{"location":"programming_languages_tools/misc/github/#references","title":"References","text":"<ol> <li>GitHub Docs</li> </ol>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/","title":"Introduction","text":""},{"location":"programming_languages_tools/r_data_viz/01_introduction/#introduction-to-rstudio","title":"Introduction To RStudio","text":"<p>RStudio is what is known as an Integrated Development Environment or IDE. Here you can write scripts, run R code, use R packages, view plots, and manage projects. This pane is broken up into three panels:</p> <ul> <li>The Interactive R console/Terminal (left)</li> <li>Environment/History/Connections (upper right)</li> <li>Files/Plots/Packages/Help/Viewer (lower right)</li> </ul> <p>RStudio Layout</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#project-management","title":"Project Management","text":"<p>Before we dive into R it is worth taking a moment to talk about project management. Often times data analysis is incremental and files build up over time resulting in messy directories:</p> <p>Example of a Messy Directory</p> <p></p> <p>Sifting through a non-organized file system can make it difficult to find files, share data/scripts, and identify different versions of scripts. To remedy this, It is reccomended to work within an R Project. Before we make this project, we should make sure you are in your home directory. To do this click on the three dots in the files tab:</p> <p>Navigating Folders</p> <p></p> <p>Then enter in a ~ symbol to go home!</p> <p>Getting Home</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#r-project","title":"R Project","text":"<p>For the following intro to R tutorial we will be using Alzheimer's Disease gene expression data from Srinivasan et al. 2020. When working within R it is useful to set up an R project. R projects will set your working directory relative to the project directory. This can help ensure you are only working with files within this project space. To create a new project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>r_data_viz</code>)</li> <li><code>Create Project</code></li> </ol> <p>When analyzing data it is useful to create a folder to house your raw data, scripts and results. We can do this by clicking the <code>New Folder</code> icon to create these folders:</p> <ol> <li>Click <code>New Folder</code> &gt; Enter <code>data</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>scripts</code> &gt; Click OK</li> <li>Click <code>New Folder</code> &gt; Enter <code>results</code> &gt; Click OK</li> </ol> <p>Now that we have our project set up we will need to download our data. In the <code>data</code> folder we will download our data and decompress it:</p> <pre><code>download.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/r_data_viz/data/ad_overview.png\",destfile = \"./data/ad_overview.png\")\ndownload.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/r_data_viz/data/expression_data.tsv\",destfile = \"./data/expression_data.tsv\")\ndownload.file(\"https://raw.githubusercontent.com/BioNomad/omicsTrain/main/docs/programming_languages_tools/r_data_viz/data/meta_data.tsv\",destfile = \"./data/meta_data.tsv\")\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#data-principles","title":"Data Principles","text":"<ul> <li>Treat data as read-only</li> <li>Store raw data separately from cleaned data if you do need to manipulate it</li> <li>Ensure scripts to clean data are kept in a separate <code>scripts</code> folder</li> <li>Treat reproducible results as disposable</li> </ul> <p>Tip</p> <p>Result files are good candidate files to cut if you are getting low on storage.</p>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#new-r-script","title":"New R script","text":"<p>Now we will create an R script. R commands can be entered into the console, but saving these commands in a script will allow us to rerun these commands at a later date. To create an R script we will need to either:</p> <ul> <li>Go to <code>File &gt; New File &gt; R script</code></li> <li>Click the <code>New File</code> icon and select R script</li> </ul> <p>Creating a New R Script</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#running-r-code","title":"Running R Code","text":"<p>When running R code you have a few options:</p> <p>Running One Line/Chunk:</p> <ul> <li> <p>Put your cursor at the beginning of the line of code and hit <code>Ctrl + Enter</code> on Windows or  \u2318 + <code>Enter</code> on MacOSX.</p> </li> <li> <p>Highlight the line/chunk of code and hit <code>Ctrl + Enter</code> or \u2318 + <code>Enter</code>.</p> </li> </ul> <p>Running The Entire Script:</p> <ul> <li>Clicking <code>Source</code> at the top of the script window.</li> </ul>"},{"location":"programming_languages_tools/r_data_viz/01_introduction/#references","title":"References","text":"<ol> <li>R for Reproducible Scientific Analysis</li> <li>Base R Cheat Sheet</li> <li>Alzheimer\u2019s Patient Microglia Exhibit Enhanced Aging and Unique Transcriptional Activation</li> </ol>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/","title":"Data Manipulation","text":""},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#loading-libraries","title":"Loading Libraries","text":"<p>To manipulate our data and ultimately plot it, we often use libraries of functions to reduce the amount of coding we need to do. For the purposes of this tutorial we will load the following:</p> <pre><code>library(tidyverse)          # data manipulation/plotting\nlibrary(janitor)            # data tabulation\nlibrary(reshape2)           # reshape to plug into ggplot\nlibrary(ggfortify)          # pca plot\nlibrary(ggplotify)          # convert to ggplot\nlibrary(png)                # use png images\nlibrary(grid)               # modify png images\nlibrary(pheatmap)           # create heatmaps\nlibrary(patchwork)          # combine multiple plots\nlibrary(DESeq2)             # run differential expression\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#loading-data","title":"Loading Data","text":"<p>To load our data we will be using functions from the <code>readr</code> packages instead of the base functions provided through base R. The <code>readr</code> collection of functions are advantageous as they are typically more user friendly and are faster at importing data. Let's load our sample and meta data:</p> <pre><code># load sample data\nmeta &lt;- read_delim(file=\"../data/meta_data.tsv\",\n             delim=\"\\t\")\n\n# load mRNA data\ncounts &lt;- read_delim(file=\"../data/expression_data.tsv\",\n             delim=\"\\t\")\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#inspecting-data","title":"Inspecting Data","text":"<p>We can examine the structure of our data with the <code>str</code> function:</p> <pre><code>str(meta)\n</code></pre> <p>output</p> <pre><code>    spc_tbl_ [113 \u00d7 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patient_id     : chr [1:113] \"GSM3561843\" \"GSM3561844\" \"GSM3561845\" \"GSM3561846\" ...\n $ braak.score:ch1: chr [1:113] \"III\" \"IV\" \"II\" \"V\" ...\n $ cell type:ch1  : chr [1:113] \"myeloid\" \"endothelial\" \"neuron\" \"neuron\" ...\n $ diagnosis:ch1  : chr [1:113] \"Control\" \"Control\" \"Control\" \"AD\" ...\n $ expired_age:ch1: chr [1:113] \"&gt;90\" \"88\" \"79\" \"84\" ...\n $ Sex:ch1        : chr [1:113] \"Female\" \"Female\" \"Male\" \"Female\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patient_id = col_character(),\n  ..   `braak.score:ch1` = col_character(),\n  ..   `cell type:ch1` = col_character(),\n  ..   `diagnosis:ch1` = col_character(),\n  ..   `expired_age:ch1` = col_character(),\n  ..   `Sex:ch1` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n</code></pre> <p>Here we see the overall structure of our meta data. We have a data frame with 113 rows, 6 columns, the first few observations per column, and the data type of each column. Now if this is a bit much to look at we can also just look at the top 6 rows of our data:</p> <pre><code>head(counts[,1:5])\n</code></pre> <p>output</p> <pre><code>    # A tibble: 6 \u00d7 5\n  SYMBOL   GSM3561843 GSM3561844 GSM3561845 GSM3561846\n  &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 AKT3           1391       1918        472        773\n2 NR2E3             2         50          9         20\n3 NAALADL1         10         18          2          1\n4 SIGLEC14        512          0          1          1\n5 MIR708            4          0          0          0\n6 NAV2-AS6         24         44         47         35\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#convert-a-column-to-rownames","title":"Convert A Column To Rownames","text":"<p>Sometimes when we import our data, we may not get it in the form we need it in. In that case we will need to do some data munging (AKA data cleaning). A common operation is take a column of unique identifiers and turn that column into the rownames for the data frame:</p> <p>Column to Rownames</p> <p></p> <p>We will need to make the gene names our row names we can do this with the <code>column_to_rownames</code> funtion:</p> <pre><code># convert the SYMBOL column to rownames\ncounts &lt;- counts %&gt;%\n  column_to_rownames(\"SYMBOL\")\n\n# now let's inspect the new counts data frame\nhead(counts[,1:5])\n</code></pre> <p>output</p> <pre><code>             GSM3561843 GSM3561844 GSM3561845 GSM3561846 GSM3561847\nAKT3           1391       1918        472        773        140\nNR2E3             2         50          9         20          9\nNAALADL1         10         18          2          1          0\nSIGLEC14        512          0          1          1         31\nMIR708            4          0          0          0          0\nNAV2-AS6         24         44         47         35          0\n</code></pre> <p>Let's do the same for patient id in our meta data. This way we can better map our counts data frame to our meta data data frame:</p> <pre><code># make the patient_id column the rownames\nmeta &lt;- meta %&gt;%\n  column_to_rownames(\"patient_id\")\n\n# now let's inspect the new counts data frame\nhead(meta[,1:5])\n</code></pre> <p>output</p> <pre><code>               braak.score:ch1 cell type:ch1 diagnosis:ch1 expired_age:ch1 Sex:ch1\nGSM3561843             III       myeloid       Control             &gt;90  Female\nGSM3561844              IV   endothelial       Control              88  Female\nGSM3561845              II        neuron       Control              79    Male\nGSM3561846               V        neuron            AD              84  Female\nGSM3561847              II       myeloid       Control              73    &lt;NA&gt;\nGSM3561848               V       myeloid            AD              84  Female\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#changing-column-names","title":"Changing Column Names","text":"<p>Data won't always come in with user friendly column names. We can change column names in a data frame with the <code>rename</code>/<code>rename_with</code> functions:</p> <p>Renaming Column Names</p> <p></p> <p>In our meta data frame all our column names have extra characters on the end. We can change all these column names with the <code>rename_with</code> function:</p> <pre><code># rename columns that have the pattern :ch1\nmeta &lt;- meta %&gt;%\n  rename_with(~ gsub(\":ch1\",\"\",.))\n\n# we can check our data frame to see our changes\nhead(meta)\n</code></pre> <p>output</p> <pre><code>               braak.score   cell type diagnosis expired_age    Sex\nGSM3561843         III     myeloid   Control         &gt;90 Female\nGSM3561844          IV endothelial   Control          88 Female\nGSM3561845          II      neuron   Control          79   Male\nGSM3561846           V      neuron        AD          84 Female\nGSM3561847          II     myeloid   Control          73   &lt;NA&gt;\nGSM3561848           V     myeloid        AD          84 Female\n</code></pre> <p>If we were interested in changing the name of one column we could use the <code>rename</code> function to do so:</p> <pre><code># rename the cell type column\nmeta &lt;- meta %&gt;%\n  dplyr::rename(\"cell_type\"=\"cell type\")\n\n# let's check the names of our data frame now!\nhead(meta)\n</code></pre> <p>output</p> <pre><code>               braak.score   cell_type diagnosis expired_age    Sex\nGSM3561843         III     myeloid   Control         &gt;90 Female\nGSM3561844          IV endothelial   Control          88 Female\nGSM3561845          II      neuron   Control          79   Male\nGSM3561846           V      neuron        AD          84 Female\nGSM3561847          II     myeloid   Control          73   &lt;NA&gt;\nGSM3561848           V     myeloid        AD          84 Female\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#filtering-rows","title":"Filtering Rows","text":"<p>Not all rows in your data will be worth keeping we can remove rows with the <code>filter</code> function:</p> <p>Filtering Data Frames</p> <p></p> <p>We will be assessing differentially expressed genes between Alzheimer's disease patients and control patients. However, we will be filtering out any non-neuronal tissue samples:</p> <pre><code># only keep neuronal samples\nmeta_filt &lt;- meta %&gt;%\n  filter(cell_type==\"neuron\")\n\n# we can check how many rows were filtered out by comparing the filtered\n# data frame with the previous data frame\nnrow(meta)\nnrow(meta_filt)\n</code></pre> <p>output</p> <pre><code>[1] 113\n[1] 42\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#selecting-columns","title":"Selecting Columns","text":"<p>we have just removed 71 rows (so 71 samples) from our meta data. To run DESeq2 we will need to remove those samples from our counts data frame as well. We can remove columns using the <code>select</code> function:</p> <p>Selecting Columns</p> <p></p> <pre><code># select only columns whose names are present in the rownames of the meta data\ncounts_filt &lt;- counts %&gt;%\n  select(rownames(meta_filt))\n\n# let's check how many columns are present in the filtered and unfiltered data frames\nncol(counts)\nncol(counts_filt)\n</code></pre> <p>output</p> <pre><code>[1] 113\n[1] 42\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#add-a-column","title":"Add A Column","text":"<p>Right now our condition variable is <code>Control</code> v. <code>AD</code>. We may want to add a column that spells out that <code>AD</code> represents Alzheimer's Disease. We can add columns with the <code>mutate</code> function:</p> <p>Adding Columns To A Data Frame</p> <p></p> <pre><code># let's create a column for Alzheimer's Disease Status using the ifelse function to insert a value if a test is TRUE\nmeta_filt &lt;- meta_filt %&gt;%\n  mutate(diagnosis = gsub(\"AD\",\"Alzheimer's Disease\",diagnosis))\n# let's inspect this new column!\nmeta_filt$diagnosis[1:10]\n</code></pre> <p>output</p> <pre><code>[1] \"Control\"             \"Alzheimer's Disease\" \"Control\"             \"Alzheimer's Disease\"\n[5] \"Alzheimer's Disease\" \"Control\"             \"Control\"             \"Alzheimer's Disease\"\n[9] \"Control\"             \"Alzheimer's Disease\"\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#changing-a-column","title":"Changing A Column","text":"<p>We can also use mutate to update an existing column as well let's update the disease status column to be a factor setting this as a factor will help with DESeq2 later on:</p> <pre><code># factor the diagnosis column to set the `Control` value as the reference\nmeta_filt &lt;- meta_filt %&gt;%\n  mutate(diagnosis=factor(diagnosis,\n                               levels = c(\n                                 \"Control\",\n                                 \"Alzheimer's Disease\"\n                               )))\n\n# let's inspect our updated column!\nmeta_filt$diagnosis[1:10]\n</code></pre> <p>output</p> <pre><code>[1] Control             Alzheimer's Disease Control             Alzheimer's Disease\n[5] Alzheimer's Disease Control             Control             Alzheimer's Disease\n[9] Control             Alzheimer's Disease\nLevels: Control Alzheimer's Disease\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#merging-data-frames","title":"Merging Data Frames","text":"<p>Sometimes we need to take two data frames and merge them into one data frame. We can do this with the <code>join</code> functions. Here we will do an inner join (meaning we merge two data frames an only keep rows that match by some column):</p> <p>Merging Data Frames</p> <p></p> <pre><code># flip the counts data so patients are the columns\n# make the rownames a patient id column\ncounts_flip &lt;- counts_filt %&gt;%\n  t() %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(\"patient_id\")\n\n# make the rownames a patient id column\nmeta_patient_to_col &lt;- meta_filt %&gt;%\n  rownames_to_column(\"patient_id\")\n\n# inner join the meta data and the counts data\nmerged &lt;- meta_patient_to_col %&gt;%\n  inner_join(\n    .,\n    counts_flip,\n    by=\"patient_id\"\n  )%&gt;%\n  `rownames&lt;-`(.[[\"patient_id\"]])\n\nnrow(merged)\nncol(merged)\n</code></pre> <p>output</p> <pre><code>[1] 42\n[1] 3006\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/02_data_manipulatation/#creating-a-table-for-publication","title":"Creating a Table For Publication","text":"<p>When getting reading for a publication we may want to include a summary of our data. The <code>janitor</code> package contains many helpful functions for that purpose! Let's try to create a table of sex by disease status:</p> <pre><code># often times you may want to tabulate your data to prepare a \n# table 1 for your paper. An easy way of doing this is to\n# use the tabyl/adorn functions from the janitor package\ntable_1 &lt;- merged %&gt;%                     # data frame to work with\n  tabyl(Sex, diagnosis) %&gt;%               # tabulate sex by diagnosis\n  adorn_totals(\"row\") %&gt;%                 # adorn totals by row\n  adorn_percentages(\"all\") %&gt;%            # adorn percentages to all cells\n  adorn_pct_formatting(digits = 1) %&gt;%    # round the percentage to the first digit\n  adorn_ns                                # keep both count and percentage\n\n# let's take a look at our table 1!\ntable_1\n</code></pre> <p>output</p> <pre><code>        Sex    Control Alzheimer's Disease\n Female 14.3%  (6)          16.7%  (7)\n   Male 28.6% (12)          19.0%  (8)\n   &lt;NA&gt;  7.1%  (3)          14.3%  (6)\n  Total 50.0% (21)          50.0% (21)\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/","title":"Data Visualization","text":""},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#creating-a-barplot","title":"Creating a Barplot","text":"<p>Now do we know how to manipulate data frames we are going to use the <code>ggplot</code> package to plot our different meta data categories we will start by using the <code>tabyl</code> function to tabulate diagnosis by sex and then melt to ensure that categorical variables are in one separate columns while all values are in one column:</p> <pre><code># tabulate diagnosis by sex and ensure categorical variables are in separate columns:\ndiagnosis_sex &lt;- tabyl(merged,diagnosis,Sex) %&gt;%\n  reshape2::melt()\ndiagnosis_sex\n</code></pre> <p>output</p> <pre><code>                diagnosis variable value\n1             Control   Female     6\n2 Alzheimer's Disease   Female     7\n3             Control     Male    12\n4 Alzheimer's Disease     Male     8\n5             Control      NA_     3\n6 Alzheimer's Disease      NA_     6\n</code></pre> <p>Here we see that each variable has it's own column - this is necessary for plotting in ggplot:</p> <pre><code>ggplot(diagnosis_sex,         # data frame to use for plotting\n      aes(x=diagnosis,       # set the x axis\n          y=value,           # set the y axis\n          fill=variable))+   # what column do we fill the bars by\n geom_bar(stat=\"identity\")   # keep the count values as they are\n</code></pre> <p>Bar Plot</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#fixing-ggplot-colors","title":"Fixing ggplot Colors","text":"<p>Above we created a ggplot bar plot! however, it isn't publication worth yet for that we will need to play around with the formatting let's change the colors first!</p> <pre><code>ggplot(diagnosis_sex,            # data frame to use for plotting\n       aes(x=diagnosis,          # set the x axis\n           y=value,              # set the y axis\n           fill=variable))+      # what column do we fill the bars by\n  geom_bar(stat=\"identity\")+     # keep the count values as they are\n  scale_fill_manual(values = c(\n    c(                           # modify the colors manually\n      \"Female\"=\"mediumvioletred\",\n      \"Male\"=\"lightsteelblue4\",\n      \"NA_\"=\"grey\")\n  ))\n</code></pre> <p>Bar Plot With Colors</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#fixing-ggplot-themes","title":"Fixing ggplot themes","text":"<p>Now let's change the theme of the plot! A plot's theme is essentially the background styling. A common theme used in publications is the black and white theme:</p> <pre><code>ggplot(diagnosis_sex,            # data frame to use for plotting\n       aes(x=diagnosis,          # set the x axis\n           y=value,              # set the y axis\n           fill=variable))+      # what column do we fill the bars by\n  geom_bar(stat=\"identity\")+     # keep the count values as they are\n  scale_fill_manual(values = c(\n    c(                           # modify the colors manually\n      \"Female\"=\"mediumvioletred\",\n      \"Male\"=\"lightsteelblue4\",\n      \"NA_\"=\"grey\")\n  ))+\n  theme_bw()                     # set the black and white theme   \n</code></pre> <p>Bar Plot With Themes</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#fixing-ggplot-labels","title":"Fixing ggplot Labels","text":"<p>Now let's change the labels of the plot! We will rotate the x axis labels, change the font size, and change the label text!</p> <pre><code>barplot &lt;- ggplot(\n  diagnosis_sex,                        # data frame to use for plotting\n       aes(x=diagnosis,                 # set the x axis\n           y=value,                     # set the y axis\n           fill=variable))+             # what column do we fill the bars by\n  geom_bar(stat=\"identity\")+            # keep the count values as they are\n  scale_fill_manual(values = c(\n    c(                                  # modify the colors manually\n      \"Female\"=\"mediumvioletred\",\n      \"Male\"=\"lightsteelblue4\",\n      \"NA_\"=\"grey\")\n  ))+\n  theme_bw()+                           # set the black and white theme  \n  theme(\n    axis.text.x = element_text(         # reference just the axis x text\n      angle = 45,                       # rotate the text 45 degrees\n      hjust = 1                         # move the text horizontally down by one\n    ),\n    text =element_text(size = 14)       # increase base text size to 14\n  )+\n  labs(\n    x=\"\",                               # change x axis title to no text\n    y=\"Count\",                          # change y axis title \n    fill=\"Sex\",                         # change legend title\n    title = \"\"                          # add a figure title\n  )\nbarplot\n</code></pre> <p>Bar Plot With Themes</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#creating-a-pca-plot","title":"Creating A PCA Plot","text":"<p>PCA or Principal Component Analysis is a way of visualizing the variation in high dimensional data in just a few dimensions. For more information, check out our tutorial on PCA. Let's examine the variation in our gene expression data and color by disease status:</p> <pre><code># run pca on just our counts data (exclude meta data)\npca_res &lt;- prcomp(merged %&gt;%                  # data frame to use\n                    select(-names(.)[1:7]))   # unselecting meta data columns\n\n# create a pca plot colored by smoking status using the autoplot function\n# and modify it using our ggplot modifiers from above!\npca_plot &lt;- autoplot(pca_res,           # the pca object\n         data = merged,                 # the whole data frame (with meta data)\n         colour = \"diagnosis\")+         # what variable to color by\n  theme_bw()+                           # set the theme to black &amp; white\n  scale_color_manual(                   \n    values=c(                           # modify the colors manually\n      \"Alzheimer's Disease\"=\"mediumvioletred\",\n      \"Control\"=\"lightsteelblue4\"))+\n  labs(\n    color=\"Diagnosis\",                  # change the legend title\n    title=\"PCA Plot Of Expression Data\" # chage the figure title\n  )\npca_plot\n</code></pre> <p>PCA Plot</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#running-deseq2","title":"Running DESeq2","text":"<p>Here we want to cover how to plot differential expression results without going into too much detail. Here we provide a wrapper to run DESeq2 on our counts/meta data:</p> <pre><code>runDESeq2 &lt;- function(meta_data,\n                      counts_data,\n                      formula){\n  # take a counts data frame with the genes as rownames\n  # and the patients as column names and confirm: data is an integer\n  # and that the gene name is the first column\n  counts_data &lt;- data.frame(gene=rownames(counts_data)) %&gt;%\n    cbind.data.frame(sapply(counts_data,as.integer)) %&gt;%\n    mutate(tmp_col=gene) %&gt;%\n    column_to_rownames(\"tmp_col\")\n  # create the deseq object\n  dds &lt;- DESeqDataSetFromMatrix(countData=counts_data, \n                                colData=meta_data, \n                                design=formula,\n                                tidy = TRUE)\n  # run deseq\n  dds &lt;- DESeq(dds)\n  # shrink log fold changes\n  res &lt;- lfcShrink(dds, coef=2, type=\"apeglm\")\n  # get/return results data frame\n  res &lt;- data.frame(res)\n  return(list(\n    dds=dds,\n    res=res\n  ))\n}\n</code></pre> <p>Let's run DESeq2 on our data and check out the results:</p> <pre><code># now let's run DESeq2 on our data!\ndds_res &lt;- runDESeq2(\n  meta_data = meta_filt,\n  counts_data = counts_filt ,\n  formula = as.formula(\"~ diagnosis\"))\n\nres &lt;- dds_res$res\n# let's take a look at the results\nhead(res)\n</code></pre> <p>DESeq2 Results</p> <pre><code>           baseMean log2FoldChange     lfcSE      pvalue       padj\nAKT3     490.709649     -0.4670004 0.2361901 0.030085339 0.08116548\nNR2E3      9.598653      0.6037522 0.2356439 0.004889092 0.05707112\nNAALADL1   1.999027      0.7468250 0.5435370 0.042450489 0.09012843\nSIGLEC14   2.625559      0.3236496 0.5752871 0.110457222 0.13128830\nMIR708     1.504679      0.5881716 0.5340170 0.092049083 0.12021453\nNAV2-AS6  27.774062      0.2249761 0.1803118 0.184228395 0.19262298\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#cleaning-up-results","title":"Cleaning Up Results","text":"<p>Above we have the DESeq2 results for our data. However, it is nice to include columns for the change direction (i.e. up or down regulated) and whether or not the results are significant. Let's add that information in!</p> <pre><code># add columns for change direction, modify this column so that only genes\n# with a p-value below 0.05 are colored, drop any genes with na \n# fold changes/pvalues and add in a gene column\nres &lt;- res %&gt;%\n  mutate(change_direction=ifelse(\n    log2FoldChange&gt;0 ,\n    \"Upregulated\",\n    \"Downregulated\"\n  )) %&gt;%\n  mutate(change_direction=ifelse(\n    pvalue&lt;0.05 &amp;\n      abs(log2FoldChange) &gt; 0.5,\n    change_direction,\n    \"Not Significant\"\n  )) %&gt;%\n  drop_na()\n\n# isolate top genes\ntop_genes &lt;- res %&gt;%\n  filter(change_direction != \"Not Significant\") %&gt;%\n  arrange(pvalue) %&gt;%\n  slice_head(n=10) %&gt;%\n  rownames(.)\n\n# add in column for top genes\nres &lt;- res %&gt;%\n  mutate(top_genes=ifelse(\n    rownames(.) %in% top_genes,\n    rownames(.),\n    NA\n  ))\n\n\n# now let's take a look at the new columns!\nhead(res)\n</code></pre> <p>output</p> <pre><code>           baseMean log2FoldChange     lfcSE      pvalue       padj change_direction top_genes\nAKT3     490.709649     -0.4670004 0.2361901 0.030085339 0.08116548  Not Significant      &lt;NA&gt;\nNR2E3      9.598653      0.6037522 0.2356439 0.004889092 0.05707112      Upregulated      &lt;NA&gt;\nNAALADL1   1.999027      0.7468250 0.5435370 0.042450489 0.09012843      Upregulated      &lt;NA&gt;\nSIGLEC14   2.625559      0.3236496 0.5752871 0.110457222 0.13128830  Not Significant      &lt;NA&gt;\nMIR708     1.504679      0.5881716 0.5340170 0.092049083 0.12021453  Not Significant      &lt;NA&gt;\nNAV2-AS6  27.774062      0.2249761 0.1803118 0.184228395 0.19262298  Not Significant      &lt;NA&gt;\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#create-a-volcano-plot","title":"Create a Volcano Plot","text":"<p>A popular way to visualize differential expression results is a volcano plot or a plot of log2 fold change versus the -log10 of the p-value where genes towards the top are more significant:</p> <p>Volcano Plot</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#isolate-normalized-expression-data","title":"Isolate Normalized Expression Data","text":"<p>DESeq2 normalizes your expression data as such if we intend to plot expression data we will need to use this expression data instead:</p> <pre><code># extract normalized data from deseq results\ncounts_norm &lt;- counts(dds_res$dds, normalized=TRUE) %&gt;%\n  t() %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(\"patient_id\")\n\n# ensure patients match\nmeta_patient_to_col &lt;- meta_filt %&gt;%\n  rownames_to_column(\"patient_id\")\n\n# merge our data\nmerged_norm &lt;- meta_patient_to_col %&gt;%\n  inner_join(\n    .,\n    counts_norm,\n    by=\"patient_id\"\n  )\n</code></pre>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#create-a-expression-box-plots","title":"Create a Expression Box Plots","text":"<p>Often times we may want to visualize the expression of our top differentially expressed genes. We can do this using a box plot!</p> <pre><code>exp_plot &lt;- ggplot(merged_norm,         # data frame to use                     \n       aes(\n         x=diagnosis,                   # x axis\n         y=LINC02615,                   # y axis\n         fill=diagnosis                 # what to fill by\n       ))+\n  geom_jitter(alpha = .5,width = .2)+\n  geom_boxplot(alpha = 0.8)+\n  scale_y_log10()+\n  theme_bw()+\n  scale_fill_manual(values = c(\n    c(                                  # modify the colors manually\n      \"Alzheimer's Disease\"=\"mediumvioletred\",\n      \"Control\"=\"lightsteelblue4\")\n  )) +\n  theme(\n    axis.text.x = element_text(         # reference just the axis x text\n      angle = 45,                       # rotate the text 45 degrees\n      hjust = 1                         # move the text horizontally down by one\n    ),\n    text =element_text(size = 14)       # increase base text size to 14\n  )+\n  labs(\n    x=\"\",                               # change x axis title to no text\n    y=\"LINC02615 Expression\",           # change y axis title \n    fill=\"Diagnosis\",                   # change legend title\n  )\n\nexp_plot\n</code></pre> <p>Expression Plot</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#creating-a-heatmap","title":"Creating A Heatmap","text":"<p>Along with our gene expression box plots, it is often useful to create a heatmap of our top differentially expressed genes. We will create one using the pheatmap function!</p> <pre><code># isolate the top 15 degs\ntop_15 &lt;- res %&gt;% \n  filter(pvalue &lt;0.05)%&gt;%\n  arrange(desc(log2FoldChange)) %&gt;% \n  head(30) %&gt;%\n  rownames(.)\n\n# filter DESeq2 normalized counts to select top 15 genes\ntop_15_counts &lt;- counts(dds_res$dds, normalized=TRUE) %&gt;%\n  data.frame() %&gt;%\n  filter(rownames(.) %in% top_15) \n\n# set the colors for our heatmeap annotation\nanot_colors &lt;- list(Diagnosis=c(                               \n  \"Alzheimer's Disease\"=\"mediumvioletred\",\n  \"Control\"=\"lightsteelblue4\"))\n\n# select dianosis and make the patient id the rownames\nanot_col &lt;- merged_norm %&gt;%\n  column_to_rownames(\"patient_id\") %&gt;%\n  select(c(diagnosis)) %&gt;%\n  arrange(diagnosis) %&gt;%\n  dplyr::rename(\"Diagnosis\" = \"diagnosis\")\n\nheat &lt;- pheatmap::pheatmap(                     # heatmap function\n  top_15_counts[,rownames(anot_col)],           # data to input \n  color=colorRampPalette(                       # set specific color range\n    c(\"navy\", \"white\", \"red\"))(50),\n  scale = \"row\",                                # scale by row\n  fontsize = 12,                                # set the font size\n  annotation_col =  anot_col,                   # set the annotation variable\n  annotation_colors = anot_colors,              # set annotation variables\n  cluster_cols = F,                             # do not cluster by column\n  cluster_rows = F,                             # do not cluster by row\n  show_colnames = F,                            # do not show column names\n  main=\"\")                                      # set a main title\nheat &lt;- as.ggplot(heat)\n</code></pre> <p>Heatmap</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#using-images-in-ggplot","title":"Using Images in ggplot","text":"<p>Images can also be insterted into a ggplot let's add an image that describes the study design:</p> <pre><code># read in image\nimg &lt;- readPNG(\"../data/ad_overview.png\", native = TRUE)\n# convert to a grob\nimg &lt;- rasterGrob(img, interpolate=TRUE)\n# add grob to a ggplot \ngg_img &lt;- ggplot()+annotation_custom(img, xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf)+theme_void()\ngg_img\n</code></pre> <p>Study Design Overview</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#combining-plots","title":"Combining Plots","text":""},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#side-by-side","title":"Side By Side","text":"<p>We can now combine these into a combined figure with the patchwork package!</p> <pre><code># use \"|\" to set plot side by side\ntop_row &lt;- gg_img|exp_plot\ntop_row\n</code></pre> <p>Top Row Of Figure</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#bottomtop-rows","title":"Bottom/Top Rows","text":"<pre><code># define a top and bottom row using \"/\"\nleft_side &lt;- top_row/volcano_plot\nleft_side\n</code></pre> <p>Left Side Of Figure</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#putting-it-all-together","title":"Putting It All Together","text":"<pre><code># define right side of figure\nright_side &lt;- heat\n\n# combine left and right sides\ncombined &lt;- left_side | right_side\n\ncombined\n</code></pre> <p>Combined Figure</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#adding-labels","title":"Adding Labels","text":"<pre><code># now add at title, caption, and annotation levels starting at A\ncombined_with_labels &lt;- combined+\n  plot_annotation(\n    tag_levels = 'A',\n    title = \"Alzheimer's Disease Neuronal Differential Expression\",\n    caption = \"For Educational Purposes Only\"\n  ) &amp;\n  theme(plot.tag = element_text(size = 20),\n        plot.title = element_text(size = 25),\n        plot.caption = element_text(size = 20))\n\ncombined_with_labels\n</code></pre> <p>Combined Figure</p> <p></p>"},{"location":"programming_languages_tools/r_data_viz/03_data_visualization/#saving-figures","title":"Saving Figures","text":"<pre><code># we can save our new ggplot with the ggsave function:\nggsave(\n  filename = \"./results/combined_figures.png\",\n  plot = combined_with_labels\n)\n</code></pre>"},{"location":"programming_languages_tools/unix/01_introduction/","title":"Introduction To Shell/Unix","text":"<p>Author: Jason Laird, PhD Student, M.Sc. </p> <p>Many software don't have a graphical user interface (or GUI) that users can use to run a pipeline. Instead many of these tools are only available through the command line interface. To use the command line interface you will need to know shell commands - which is exactly what we will cover here!</p> <p>Learning Objectives</p> <ol> <li>Navigating Files &amp; Directories</li> <li>Managing Files &amp; Directories</li> <li>Redirection and Pipes</li> <li>Regular Expressions</li> <li>Shell Scripting</li> </ol>"},{"location":"programming_languages_tools/unix/01_introduction/#getting-to-the-terminal","title":"Getting To the Terminal","text":"<p>Getting to a shell terminal will be different based on whether or not you have a Mac or a Windows machine:</p> <ul> <li>Mac: Click the Terminal icon on your application bar at the bottom of your screen to open a shell terminal</li> <li>Windows: I recommend using a program like MobaXterm. MobaXterm is an enhanced terminal for Windows machines that allows you to use Unix commands. This is useful because many bioinformatics programs are written for Unix shells! </li> </ul> <p>MobaXterm Download Instructions</p>"},{"location":"programming_languages_tools/unix/01_introduction/#what-is-a-command","title":"What is a Command?","text":"<p>When working with command line you will need to use \"commands\" to perform tasks. You can think of a command as the same thing as a click using a mouse - you can open a file/folder, move between folders, edit a file, etc.. However, commands on command line are far more powerful and can automate all this clicking. A command has the following structure:</p> <pre><code>command [options] [arguments]\n</code></pre> <p>Explanation of Terms</p> <ul> <li><code>command</code> name of the command you'd like to use</li> <li><code>[options]</code> options you can input to modify the original command</li> <li><code>[arguments]</code> the inputs you'd like the command to work with</li> </ul>"},{"location":"programming_languages_tools/unix/01_introduction/#data-for-todays-tutorial","title":"Data For Today's Tutorial","text":"<p>To get started let's copy some data to get started! So in command line enter the following code to copy data to your computer:</p> <pre><code>wget https://github.com/BioNomad/omicsTrain/blob/main/docs/programming_languages_tools/unix/unix_tutorial.zip\n</code></pre> <p>The <code>wget</code> command can be used to pull most files from the internet! However this file is compressed (ends in either <code>.zip</code>,<code>.tar</code>, or <code>.tar.gz</code>), meaning we can't access the files yet. To uncompress our data let's use the unzip command:</p> <p><pre><code>unzip unix_tutorial.zip\n</code></pre> Great, now we are ready to get started!</p>"},{"location":"programming_languages_tools/unix/01_introduction/#references","title":"References","text":"<ol> <li>Introduction to the Command Line for Genomics</li> </ol>"},{"location":"programming_languages_tools/unix/02_navigating_files_directories/","title":"Navigating Files/Directories","text":""},{"location":"programming_languages_tools/unix/02_navigating_files_directories/#changing-and-checking-directories","title":"Changing and Checking Directories","text":"<p>File systems follow a heirarchy. Think of the files in your computer, you have folders which can contain folders or files. Now that have our tutorial data we will enter or change into our <code>unix_tutorial</code> folder or directory with the <code>cd</code> command (<code>cd</code> being short for change directory):</p> <pre><code>cd unix_tutorial\n</code></pre> <p>Now we have entered a directory! But how do we understand where we are? We can do this with the <code>pwd</code> command (print working directory):</p> <pre><code>pwd\n</code></pre> <p>output</p> <pre><code>~/Documents/unix_tutorial\n</code></pre> <p>This output will be a little different for every user based on your computer's file structure. However, you will note here that we are within the root folder (<code>~</code>) which contains all other folders, the <code>Documents</code> folder, and finally the <code>unix_tutorial</code> folder. This string of folders showing where we are is called our file path or just path for short. </p>"},{"location":"programming_languages_tools/unix/02_navigating_files_directories/#listing-directory-contents","title":"Listing Directory Contents","text":"<p>Now this <code>unix_tutorial</code> folder has other folders inside it. We can list these folders with the <code>ls</code> command:</p> <pre><code>ls\n</code></pre> <p>output</p> <pre><code>data_folder results_folder  scripts_folder\n</code></pre> <p>Here we see that we have three folders: <code>data_folder</code>,  <code>results_folder</code>, and   <code>scripts_folder</code>. If we want more information on our files we can add options to our command. Let's do this by adding the long option to our <code>ls</code> command:</p> <pre><code>ls -l\n</code></pre> <p>output</p> <pre><code>total 0\ndrwxr-xr-x  4 username  groupname  128 May 13 09:33 data_folder\ndrwxr-xr-x  2 username  groupname   64 May 13 09:26 results_folder\ndrwxr-xr-x  2 username  groupname   64 May 13 09:26 scripts_folder\n</code></pre> <p>The long format will usually have 8 columns:  - permissions of the file</p> <p>Permissions</p> <p>permissions are split into a line of 10 characters. The first character will be a <code>d</code> if it is a directory and a <code>-</code> if it is a file. Then the characters are read in groups of three - the first three characters are the user's permissions, the next three are the group permissions and the last three are other user permissions. <code>r</code> means someone can read the file/folder, <code>x</code> means someone can execute (like as in executing a script), and <code>w</code> means someone can write to the file/folder. </p> <ul> <li>number of files (folders count as 2, files count as 1, so for example data_folder has 2 files in it)</li> <li>the username</li> <li>the group name</li> <li>size of the file/folder</li> <li>day of modification</li> <li>time of modification</li> <li>file/folder name</li> </ul>"},{"location":"programming_languages_tools/unix/02_navigating_files_directories/#navigation-shortcuts","title":"Navigation Shortcuts","text":"<p>Now let's practice navigating directories by entering our <code>data_folder</code>:</p> <pre><code>cd data_folder\npwd\n</code></pre> <p>output</p> <pre><code>~/Documents/unix_tutorial/data_folder\n</code></pre> <p>To move around we have a few shortcuts. If we want to move to our root folder (where all other folders live) we can use the <code>cd ~</code> command. To move one folder up we can use the <code>cd ..</code> command:</p> <pre><code>cd ..\npwd\n</code></pre> <p>output</p> <pre><code>~/Documents/unix_tutorial\n</code></pre>"},{"location":"programming_languages_tools/unix/02_navigating_files_directories/#references","title":"References","text":"<ol> <li>Introduction to the Command Line for Genomics</li> </ol>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/","title":"Managing Files/Directories","text":""},{"location":"programming_languages_tools/unix/03_managing_files_directories/#making-filesdirectories","title":"Making Files/Directories","text":"<p>Just like your Folders or Finder app you can use the command line to manipulate your files/directories. Let's start by making a new directory with the <code>mkdir</code> command  (short for make directory):</p> <pre><code>mkdir new_dir\nls\n</code></pre> <p>output</p> <pre><code>data_folder new_dir     results_folder  scripts_folder\n</code></pre> <p>Great! We've made a new directory! Now how about files? We can make empty files with the <code>touch</code> command:</p> <pre><code>touch new_file\nls\n</code></pre> <p>output</p> <pre><code>data_folder new_dir     new_file    results_folder  scripts_folder\n</code></pre>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/#copyingrenaming-filesdirectories","title":"Copying/Renaming Files/Directories","text":"<p>Sometimes we may want to copy a file to another folder, which we can do with the cp command (short for copy)!</p> <pre><code>cp new_file data_folder\nls data_folder\n</code></pre> <p>output</p> <pre><code>accList.txt meta.txt    new_file\n</code></pre> <p>We can see that we have successfully copied over our file! If we wanted to copy a directory we can use the <code>-r</code> option:</p> <pre><code>cp new_dir data_folder\nls data_folder\n</code></pre> <p>output</p> <pre><code>accList.txt meta.txt    new_file    new_dir\n</code></pre> <p>If we just wanted to move our file or directory without copying, we can use the <code>mv</code> commmand (short for move). However, the <code>mv</code> command can also be used to rename a file/folder as well! Let's use it to rename our file!</p> <pre><code>mv new_file brand_new_file\nls\n</code></pre> <p>output</p> <pre><code>data_folder new_dir     brand_new_file  results_folder  scripts_folder\n</code></pre>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/#removing-filesfolders","title":"Removing Files/Folders","text":"<p>We can also delete/remove files/folders - However, do this with extreme caution as this is a permanent deletion! We will remove our empty files that we just created with the <code>rm</code> (short for remove) command!</p> <pre><code>rm brand_new_file\nls\n</code></pre> <p>output</p> <pre><code>data_folder new_dir results_folder  scripts_folder\n</code></pre> <p>To remove our directory we will need the <code>-r</code> option:</p> <pre><code>rm -r new_dir\nls\n</code></pre> <p>output</p> <pre><code>data_folder results_folder  scripts_folder\n</code></pre>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/#viewing-files","title":"Viewing Files","text":"<p>To view the contents of a file, you can use the <code>cat</code> command. So let's enter our <code>data_folder</code> to view our data!</p> <pre><code>cd data_folder\ncat accList.txt\n</code></pre> <p>output</p> <pre><code>SRR1219879  \nSRR1219880\n</code></pre> <p>Sometimes a file can be thousands of lines and you may only want to view  a certain portion of it. We can view the begnnning of a file with <code>more</code> and the end of a file with the <code>less</code> command. To exit out of this viewing mode, just hit <code>q</code>! To get a quick idea of what your file contains you can also use the <code>head</code> command to grab the first 6 lines!</p>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/#editing-file-content","title":"Editing File Content","text":"<p>To edit the content of a file we need to use a text editor like nano or vim to edit on the command line. The most user-friendly editor is typically nano. So let's edit <code>new_file</code> which we have copied over to the <code>data_folder</code>:</p> <pre><code>nano new_file\n</code></pre> <p>output</p> <pre><code>UW PICO 5.09                         File: new_file                           \n\n\n\n^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Pg   ^K Cut Text  ^C Cur Pos   \n^X Exit      ^J Justify   ^W Where is  ^V Next Pg   ^U UnCut Text^T To Spell  \n</code></pre> <p>Here we will write the words <code>Hello World</code> then hit <code>Control</code> and then <code>X</code> where you will be prompted to save:</p> <p>output</p> <pre><code>UW PICO 5.09                         File: new_file \nHello World\n\n\n\nSave modified buffer (ANSWERING \"No\" WILL DESTROY CHANGES) ?                    \n             Y Yes                                                            \n^C Cancel    N No  \n</code></pre> <p>Hit <code>Y</code> to save then <code>Enter</code> to go back to the command line! Let's check the contents of our file:</p> <pre><code>cat new_file\n</code></pre> <p>output</p> <pre><code>Hello World\n</code></pre>"},{"location":"programming_languages_tools/unix/03_managing_files_directories/#references","title":"References","text":"<ol> <li>Introduction to the Command Line for Genomics</li> </ol>"},{"location":"programming_languages_tools/unix/04_redirection_pipes/","title":"Redirection and Pipes","text":""},{"location":"programming_languages_tools/unix/04_redirection_pipes/#redirection","title":"Redirection","text":"<p>We can move around the output of our commands to be the input of another command. For instance we can take the output of one file and add it to another with the <code>&gt;</code> symbol:</p> <pre><code>cat new_file &gt; super_new_file\ncat super_new_file\n</code></pre> <p>output</p> <pre><code>Hello World\n</code></pre> <p>Here we looked at the contents of <code>new_file</code>  and put those contents in a new file which we called <code>super_new_file</code>. However, be careful with the <code>&gt;</code> symbol as it will overwrite the contents of files. If we just wanted to append the contents we can use <code>&gt;&gt;</code> symbol (much safer option):</p> <pre><code>cat new_file &gt;&gt; super_new_file\ncat super_new_file\n</code></pre> <p>output</p> <pre><code>Hello World\nHello World\n</code></pre>"},{"location":"programming_languages_tools/unix/04_redirection_pipes/#piping","title":"Piping","text":"<p>So far we have entered commands separately, but we can actually take the output of one command and use it as the input for another with the <code>|</code> symbol! Here we will use the <code>cat</code> command to look at the contents of <code>meta.txt</code> then use the command <code>wc</code>  (word count) to get the number of lines, words and characters</p> <pre><code>cat new_file | wc \n</code></pre> <p>output</p> <pre><code>1       2      12\n</code></pre> <p>Just to note, that you can use <code>wc new_file</code> to get the same result, this is just an example of how to pipe the output of one command into another. </p>"},{"location":"programming_languages_tools/unix/04_redirection_pipes/#references","title":"References","text":"<ol> <li>Introduction to the Command Line for Genomics</li> </ol>"},{"location":"programming_languages_tools/unix/05_regular_expressions/","title":"Regular Expressions","text":""},{"location":"programming_languages_tools/unix/05_regular_expressions/#syntax","title":"Syntax","text":"<p>Regular expressions are a great way to find/manipulate patterns in files. Regular expressions follow a specific syntax:</p> <ul> <li><code>.</code> used for matching any character</li> <li><code>*</code> used for mathcing zero or more times</li> <li><code>+</code>  used for matching one or more times</li> <li><code>?</code> used for matching zero or one times</li> <li><code>|</code> or operator</li> <li><code>^</code> used for matching the start of a line</li> <li><code>$</code> used for matching at the end of a line</li> </ul> <p>So if we wanted to list all files that end with <code>txt</code> we would use:</p> <pre><code>ls *txt\n</code></pre> <p>output</p> <pre><code>accList.txt meta.txt\n</code></pre> <p>However, to use the rest of the symbols we need to use special commands which we will describe below.</p>"},{"location":"programming_languages_tools/unix/05_regular_expressions/#search-files","title":"Search Files","text":"<p>Regular expressions are a great way to find/manipulate patterns in files. Let's learn how to search for files starting with <code>a</code> with <code>grep</code>:</p> <pre><code>ls | grep ^a\n</code></pre> <p>output</p> <pre><code>accList.txt\n</code></pre> <p>We can also search inside files for lines with certain patterns. Let's try to find the line that contains the word \"Run\" in our meta data file:</p> <pre><code>grep \"Run\" meta.txt \n</code></pre> <p>output</p> <pre><code>Run analyte_type    Assay.Type  body_site\n</code></pre>"},{"location":"programming_languages_tools/unix/05_regular_expressions/#replace-patterns","title":"Replace Patterns","text":"<p>Sometimes you may want to replace a pattern in a file and we can accomplish this with the <code>sed</code> command! Let's replace the word \"body_site\" with \"tissue\" in our <code>meta.txt</code> file:</p> <pre><code>sed s/body_site/tissue/g meta.txt\n</code></pre> <p>output</p> <pre><code>Run analyte_type    Assay.Type  tissue\nSRR1219879  DNA WGS Peripheral blood\nSRR1219880  DNA WGS Peripheral blood\n</code></pre> <p>The <code>sed</code> command follows the pattern <code>s/pattern/replacement/g</code>.</p>"},{"location":"programming_languages_tools/unix/05_regular_expressions/#manipulating-structured-data","title":"Manipulating Structured Data","text":"<p>If we have structured data, like a data frame (think csv, tsv, excel file), then we can manipulate the data with <code>awk</code> which follows the following pattern:</p> <pre><code>awk '/pattern/ { action }' file\n</code></pre> <p>Let's print out the first column of our <code>meta.txt</code> file with <code>awk</code> as an example without a pattern:</p> <pre><code>awk '{print $1}' meta.txt\n</code></pre> <p>output</p> <pre><code>Run\nSRR1219879\nSRR1219880\n</code></pre> <p>You can pick other columns with the <code>$</code> and the number of the column. If you choose <code>$0</code> the entire file will be printed. For example if we wanted to print any line that had the pattern <code>SRR</code> we would use:</p> <pre><code>awk '/SRR/ {print $0}' meta.txt \n</code></pre> <p>output</p> <pre><code>Run\nSRR1219879  DNA WGS Peripheral blood\nSRR1219880  DNA WGS Peripheral blood\n</code></pre>"},{"location":"programming_languages_tools/unix/05_regular_expressions/#references","title":"References","text":"<ol> <li>Regular Expressions Tutorial</li> <li>AWK Quick Guide</li> </ol>"},{"location":"programming_languages_tools/unix/06_shell_scripts/","title":"Writing Shell Scripts","text":""},{"location":"programming_languages_tools/unix/06_shell_scripts/#creating-a-script","title":"Creating a Script","text":"<p>Shell scripts are convenient ways of running multiple commands. These scripts usually end with <code>.sh</code>. Let's use nano to create a shell script that says the date. First move to our <code>scripts_folder</code>:</p> <pre><code>cd ../scripts_folder/\n</code></pre> <p>Now create a script called \"first_script.sh\" with the following contents:</p> <p>first_script.sh</p> <pre><code>#!/bin/bash\n\n# run the date command\ndate\n</code></pre> <p>You'll note we use <code>#!/bin/bash</code> to make sure this is a bash shell script. When we start using other programming languages (like python) this will be swapped out with <code>#!/bin/ptyhon</code>. We also have a line that starts with a <code>#</code>. This is called a comment. Comments aren't read as code, they are just plain text we can add to give our commands/code context. Comments are very useful in making your code readable. We should also note that we can't run our script just yet. To run our script we will need to change the permissions on it with the <code>chmod</code> command:</p> <pre><code>chmod +x first_script.sh \n</code></pre> <p>This command allows us to execute our script. To execute our script we can enter the following:</p> <pre><code>./first_script.sh \n</code></pre> <p>first_script.sh</p> <pre><code>Sat May 13 13:00:45 EDT 2023\n</code></pre>"},{"location":"programming_languages_tools/unix/06_shell_scripts/#variables","title":"Variables","text":"<p>Now let's make our script more complicated with a variable (basically a word that represents some value):</p> <p>first_script.sh</p> <pre><code>#!/bin/bash\n\n# define a variable\ninitial=\"Today is:\"\n\n# echo the variable and date\necho $initial \ndate\n</code></pre> <p>Here we see that we have text (\"Today is:\") that is assigned to <code>initial</code>, which we reference with a <code>$</code> sign. To run this script we will enter the following:</p> <pre><code>./first_script.sh \n</code></pre> <p>first_script.sh</p> <pre><code>Today is:\nSat May 13 13:00:45 EDT 2023\n</code></pre>"},{"location":"programming_languages_tools/unix/06_shell_scripts/#functions","title":"Functions","text":"<p>We can create a function using shell scripting with the following syntax:</p> <pre><code>function_name (){\n    command\n}\n</code></pre> <p>To call that function we write out function_name(). Let's modify our script to add a function!</p> <p>first_script.sh</p> <pre><code>#!/bin/bash\n\n# define a variable\ninitial=\"Today is:\"\n\n# echo the variable and date\necho $initial \ndate\n\n# define a function to print the first column of the first argument you give the script\nfirst_col() {\n    awk '{print $1}' $1\n}\n\n# call this function\nfirst_col $1\n</code></pre> <p>In our function, we ask it to print the first column of the first argument passed to the function. When we call the function, we ask it to apply this function to the first argument we give the script itself. So let's run this script with the meta.txt file as our argument:</p> <pre><code>./first_script.sh\n</code></pre> <p>output</p> <pre><code>Today is:\nSat May 13 13:30:30 EDT 2023\nRun\nSRR1219879\nSRR1219880\n</code></pre>"},{"location":"programming_languages_tools/unix/06_shell_scripts/#loops","title":"Loops","text":"<p>We can use loops to perform repetitive tasks. Loops follow the following syntax:</p> <pre><code>for variable in list\ndo\n    command $variable\ndone\n</code></pre> <p>Let's modify our script to use a loop to examine the first line of all files in working directory:</p> <p>first_script.sh</p> <pre><code>#!/bin/bash\n\n# define a variable\ninitial=\"Today is:\"\n\n# echo the variable and date\necho $initial \ndate\n\n# define a function to print the first column of the first argument you give the script\nfirst_col() {\n    awk '{print $1}' $1\n}\n\n# call this function\nfirst_col $1\n\n# make a loop to examine the first line of all files in working directory\nfor file in *\ndo \n    head -n 1 $file\ndone\n</code></pre> <p>To run the script we will enter the following:</p> <pre><code>./first_script.sh\n</code></pre> <p>output</p> <pre><code>Today is:\nSat May 13 13:30:30 EDT 2023\nRun\nSRR1219879\nSRR1219880\n#!/bin/bash\n</code></pre>"},{"location":"programming_languages_tools/unix/06_shell_scripts/#references","title":"References","text":"<ol> <li>Introduction to the Command Line for Genomics</li> </ol>"}]}